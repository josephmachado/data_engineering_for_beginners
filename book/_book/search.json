[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data Engineering For Beginners",
    "section": "",
    "text": "Preface\nThis is a Quarto book.\nTo learn more about Quarto books visit https://quarto.org/docs/books.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "sql_basics.html",
    "href": "sql_basics.html",
    "title": "1  Read data, Combine tables, & aggregate numbers to understand business performance",
    "section": "",
    "text": "1.1 Setup\n%%capture\n%%bash\npython ./generate_data.py\npython ./run_ddl.py\n%%sql --show\nuse prod.db\nIn this chapter, we will go over SQL basics.",
    "crumbs": [
      "Use SQL to transform data",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Read data, Combine tables, & aggregate numbers to understand business performance</span>"
    ]
  },
  {
    "objectID": "sql_basics.html#a-spark-catalog-can-have-multiple-schemas-schemas-can-have-multiple-tables",
    "href": "sql_basics.html#a-spark-catalog-can-have-multiple-schemas-schemas-can-have-multiple-tables",
    "title": "1  Read data, Combine tables, & aggregate numbers to understand business performance",
    "section": "1.2 A Spark catalog can have multiple schemas, & schemas can have multiple tables",
    "text": "1.2 A Spark catalog can have multiple schemas, & schemas can have multiple tables\nTypically database servers can have multiple databases; each database can have multiple schemas. Each schema can have multiple tables, and each table can have multiple columns.\nNote: We use Trino, which has catalogs that allow it to connect with the different underlying systems. (e.g., Postgres, Redis, Hive, etc.)\nIn our lab, we use Trino, and we can check the available catalogs, their schemas, the tables in a schema, & the columns in a table, as shown below.\n\n%%sql \nshow catalogs;\n\n\n%%sql\nshow schemas IN demo;\n\n-- Catalog -&gt; schema\n\n\n%%sql\nshow schemas IN prod;\n\n-- schema -&gt; namespace\n\n\n%%sql\nshow tables IN prod.db -- namespace -&gt; Table\n\nNote how, when referencing the table name, we use the full path, i.e., database.schema.table_name. We can skip using the full path of the table if we let Trino know which schema to use by default, as shown below.\n\n%%sql\nDESCRIBE lineitem\n\n\n%%sql\nDESCRIBE extended lineitem",
    "crumbs": [
      "Use SQL to transform data",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Read data, Combine tables, & aggregate numbers to understand business performance</span>"
    ]
  },
  {
    "objectID": "sql_basics.html#use-selectfrom-limit-where-order-by-to-read-the-required-data",
    "href": "sql_basics.html#use-selectfrom-limit-where-order-by-to-read-the-required-data",
    "title": "1  Read data, Combine tables, & aggregate numbers to understand business performance",
    "section": "1.3 Use SELECT…FROM, LIMIT, WHERE, & ORDER BY to read the required data",
    "text": "1.3 Use SELECT…FROM, LIMIT, WHERE, & ORDER BY to read the required data\nThe most common use for querying is to read data in our tables. We can do this using a SELECT ... FROM statement, as shown below.\n\n%%sql\n-- use * to specify all columns\nSELECT\n  *\nFROM\n  orders\nLIMIT\n  4\n\n\n%%sql\n-- use column names to only read data from those columns\nSELECT\n  o_orderkey,\n  o_totalprice\nFROM\n  orders\nLIMIT\n  4\n\nHowever, running a SELECT ... FROM statement can cause issues when the data set is extensive. If you want to look at the data, use LIMIT n to tell Trino only to get n number of rows.\nWe can use the’ WHERE’ clause if we want to get the rows that match specific criteria. We can specify one or more filters within the’ WHERE’ clause. The WHERE clause with more than one filter can use combinations of AND and OR criteria to combine the filter criteria, as shown below.\n\n%%sql\n-- all customer rows that have c_nationkey = 20\nSELECT\n  *\nFROM\n  customer\nWHERE\n  c_nationkey = 20\nLIMIT\n  10;\n\n\n%%sql\n-- all customer rows that have c_nationkey = 20 and c_acctbal &gt; 1000\nSELECT\n  *\nFROM\n  customer\nWHERE\n  c_nationkey = 20\n  AND c_acctbal &gt; 1000\nLIMIT\n  10;\n\n\n%%sql\n-- all customer rows that have c_nationkey = 20 or c_acctbal &gt; 1000\nSELECT\n  *\nFROM\n  customer\nWHERE\n  c_nationkey = 20\n  OR c_acctbal &gt; 1000\nLIMIT\n  10;\n\n\n%%sql\n-- all customer rows that have (c_nationkey = 20 and c_acctbal &gt; 1000) or rows that have c_nationkey = 11\nSELECT\n  *\nFROM\n  customer\nWHERE\n  (\n    c_nationkey = 20\n    AND c_acctbal &gt; 1000\n  )\n  OR c_nationkey = 11\nLIMIT\n  10;\n\nWe can combine multiple filter clauses, as seen above. We have seen examples of equals (=) and greater than (&gt;) conditional operators. There are 6 conditional operators, they are\n\n&lt; Less than\n&gt; Greater than\n&lt;= Less than or equal to\n&gt;= Greater than or equal to\n= Equal\n&lt;&gt; and != both represent Not equal (some DBs only support one of these)\n\nAdditionally, for string types, we can make pattern matching with like condition. In a like condition, a _ means any single character, and % means zero or more characters, for example.\n\n%%sql\n-- all customer rows where the name has a 381 in it\nSELECT\n  *\nFROM\n  customer\nWHERE\n  c_name LIKE '%381%';\n\n\n%%sql\n-- all customer rows where the name ends with a 381\nSELECT\n  *\nFROM\n  customer\nWHERE\n  c_name LIKE '%381';\n\n\n%%sql\n-- all customer rows where the name starts with a 381\nSELECT\n  *\nFROM\n  customer\nWHERE\n  c_name LIKE '381%';\n\n\n%%sql\n-- all customer rows where the name has a combination of any character and 9 and 1\nSELECT\n  *\nFROM\n  customer\nWHERE\n  c_name LIKE '%_91%';\n\nWe can also filter for more than one value using IN and NOT IN.\n\n%%sql\n-- all customer rows which have nationkey = 10 or nationkey = 20\nSELECT\n  *\nFROM\n  customer\nWHERE\n  c_nationkey IN (10, 20);\n\n\n%%sql\n-- all customer rows which have do not have nationkey as 10 or 20\nSELECT\n  *\nFROM\n  customer\nWHERE\n  c_nationkey NOT IN (10, 20);\n\nWe can get the number of rows in a table using count(*) as shown below.\n\n%%sql\nSELECT\n  COUNT(*)\nFROM\n  customer;\n\n-- 1500\n\n\n%%sql\nSELECT\n  COUNT(*)\nFROM\n  lineitem;\n\n-- 60175\n\nIf we want to get the rows sorted by values in a specific column, we use ORDER BY, for example.\n\n%%sql\n-- Will show the first ten customer records with the lowest custkey\n-- rows are ordered in ASC order by default\nSELECT\n  *\nFROM\n  orders\nORDER BY\n  o_custkey\nLIMIT\n  10;\n\n\n%%sql\n-- Will show the first ten customer's records with the highest custkey\nSELECT\n  *\nFROM\n  orders\nORDER BY\n  o_custkey DESC\nLIMIT\n  10;",
    "crumbs": [
      "Use SQL to transform data",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Read data, Combine tables, & aggregate numbers to understand business performance</span>"
    ]
  },
  {
    "objectID": "sql_basics.html#combine-data-from-multiple-tables-using-joins",
    "href": "sql_basics.html#combine-data-from-multiple-tables-using-joins",
    "title": "1  Read data, Combine tables, & aggregate numbers to understand business performance",
    "section": "1.4 Combine data from multiple tables using JOINs",
    "text": "1.4 Combine data from multiple tables using JOINs\nWe can combine data from multiple tables using joins. When we write a join query, we have a format as shown below.\nSELECT\n    a.*\nFROM\n    table_a a -- LEFT table a\n    JOIN table_b b -- RIGHT table b\n    ON a.id = b.id\nThe table specified first (table_a) is the left table, whereas the table established second is the right table. When we have multiple tables joined, we consider the joined dataset from the first two tables as the left table and the third table as the right table (The DB optimizes our join for performance).\nSELECT\n    a.*\nFROM\n    table_a a -- LEFT table a\n    JOIN table_b b -- RIGHT table b\n    ON a.id = b.id\n    JOIN table_c c -- LEFT table is the joined data from table_a & table_b, right table is table_c\n    ON a.c_id = c.id\nThere are five main types of joins, they are:\n\n1.4.1 1. Inner join (default): Get rows with same join keys from both tables\n\n%%sql\nSELECT\n  o.o_orderkey,\n  l.l_orderkey\nFROM\n  orders o\n  JOIN lineitem l ON o.o_orderkey = l.l_orderkey\n  AND o.o_orderdate BETWEEN l.l_shipdate - INTERVAL '5' DAY AND l.l_shipdate  + INTERVAL '5' DAY\nLIMIT\n  10;\n\n\n%%sql\nSELECT\n  COUNT(o.o_orderkey) AS order_rows_count,\n  COUNT(l.l_orderkey) AS lineitem_rows_count\nFROM\n  orders o\n  JOIN lineitem l ON o.o_orderkey = l.l_orderkey\n  AND o.o_orderdate BETWEEN l.l_shipdate - INTERVAL '5' DAY AND l.l_shipdate  + INTERVAL '5' DAY;\n-- 2477, 2477\n\nNote: JOIN defaults to INNER JOIN`.\nThe output will have rows from orders and lineitem that found at least one matching row from the other table with the specified join condition (same orderkey and orderdate within ship date +/- 5 days).\nWe can also see that 2,477 rows from orders and lineitem tables matched.\n\n\n1.4.2 2. Left outer join (aka left join): Get all rows from the left table and only matching rows from the right table.\n\n%%sql\n\nSELECT\n  o.o_orderkey,\n  l.l_orderkey\nFROM\n  orders o\n  LEFT JOIN lineitem l ON o.o_orderkey = l.l_orderkey\n  AND o.o_orderdate BETWEEN l.l_shipdate - INTERVAL '5' DAY AND l.l_shipdate  + INTERVAL '5' DAY\nLIMIT\n  10;\n\n\n%%sql\nSELECT\n  COUNT(o.o_orderkey) AS order_rows_count,\n  COUNT(l.l_orderkey) AS lineitem_rows_count\nFROM\n  orders o\n  LEFT JOIN lineitem l ON o.o_orderkey = l.l_orderkey\n  AND o.o_orderdate BETWEEN l.l_shipdate - INTERVAL '5' DAY AND l.l_shipdate  + INTERVAL '5' DAY;\n-- 15197, 2477\n\nThe output will have all the rows from orders and the rows from lineitem that were able to find at least one matching row from the orders table with the specified join condition (same orderkey and orderdate within ship date +/- 5 days).\nWe can also see that the number of rows from the orders table is 15,197 & from the lineitem table is 2,477. The number of rows in orders is 15000, but the join condition produces 15197 since some orders match with multiple lineitems.\n\n\n1.4.3 3. Right outer join (aka right join): Get matching rows from the left and all rows from the right table.\n\n%%sql\nSELECT\n  o.o_orderkey,\n  l.l_orderkey\nFROM\n  orders o\n  RIGHT JOIN lineitem l ON o.o_orderkey = l.l_orderkey\n  AND o.o_orderdate BETWEEN l.l_shipdate - INTERVAL '5' DAY AND l.l_shipdate  + INTERVAL '5' DAY\nLIMIT\n  10;\n\n\n%%sql\nSELECT\n  COUNT(o.o_orderkey) AS order_rows_count,\n  COUNT(l.l_orderkey) AS lineitem_rows_count\nFROM\n  orders o\n  RIGHT JOIN lineitem l ON o.o_orderkey = l.l_orderkey\n  AND o.o_orderdate BETWEEN l.l_shipdate - INTERVAL '5' DAY AND l.l_shipdate  + INTERVAL '5' DAY;\n-- 2477, 60175\n\nThe output will have the rows from orders that found at least one matching row from the lineitem table with the specified join condition (same orderkey and orderdate within ship date +/- 5 days) and all the rows from the lineitem table.\nWe can also see that the number of rows from the orders table is 15,197 & from the lineitem table is 2,477.\n\n\n1.4.4 4. Full outer join: Get matched and un-matched rows from both the tables.\n\n%%sql\nSELECT\n  o.o_orderkey,\n  l.l_orderkey\nFROM\n  orders o\n  FULL OUTER JOIN lineitem l ON o.o_orderkey = l.l_orderkey\n  AND o.o_orderdate BETWEEN l.l_shipdate - INTERVAL '5' DAY AND l.l_shipdate  + INTERVAL '5' DAY\nLIMIT\n  10\n\n\n%%sql\nSELECT\n  COUNT(o.o_orderkey) AS order_rows_count,\n  COUNT(l.l_orderkey) AS lineitem_rows_count\nFROM\n  orders o\n  FULL OUTER JOIN lineitem l ON o.o_orderkey = l.l_orderkey\n  AND o.o_orderdate BETWEEN l.l_shipdate - INTERVAL '5' DAY AND l.l_shipdate  + INTERVAL '5' DAY;\n-- 15197, 60175\n\nThe output will have all the rows from orders that found at least one matching row from the lineitem table with the specified join condition (same orderkey and orderdate within ship date +/- 5 days) and all the rows from the lineitem table.\nWe can also see that the number of rows from the orders table is 15,197 & from the lineitem table is 2,477.\n\n\n1.4.5 5. Cross join: Join every row in left table with every row in the right table\n\n%%sql\nSELECT\n  n.n_name AS nation_c_name,\n  r.r_name AS region_c_name\nFROM\n  nation n\n  CROSS JOIN region r;\n\nThe output will have every row of the nation joined with every row of the region. There are 25 nations and five regions, leading to 125 rows in our result from the cross-join.\nThere are cases where we will need to join a table with itself, called a SELF-join. Lets consider an example.\n\nFor every customer order, get the order placed earlier in the same week (Sunday - Saturday, not the previous seven days). Only show customer orders that have at least one such order.\n\n\n%%sql    \nSELECT\n    o1.o_custkey as o1_custkey,\n    o1.o_totalprice as o1_totalprice,\n    o1.o_orderdate as o1_orderdate,\n    o2.o_totalprice as o2_totalprice,\n    o2.o_orderdate as o2_orderdate\nFROM\n    orders o1\n    JOIN orders o2 ON o1.o_custkey = o2.o_custkey\n    AND year(o1.o_orderdate) = year(o2.o_orderdate)\n    AND weekofyear(o1.o_orderdate) = weekofyear(o2.o_orderdate)\nWHERE\n    o1.o_orderkey != o2.o_orderkey\nLIMIT\n    10;",
    "crumbs": [
      "Use SQL to transform data",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Read data, Combine tables, & aggregate numbers to understand business performance</span>"
    ]
  },
  {
    "objectID": "sql_basics.html#combine-data-from-multiple-rows-into-one-using-group-by",
    "href": "sql_basics.html#combine-data-from-multiple-rows-into-one-using-group-by",
    "title": "1  Read data, Combine tables, & aggregate numbers to understand business performance",
    "section": "1.5 Combine data from multiple rows into one using GROUP BY",
    "text": "1.5 Combine data from multiple rows into one using GROUP BY\nMost analytical queries require calculating metrics that involve combining data from multiple rows. GROUP BY allows us to perform aggregate calculations on data from a set of rows recognized by values of specified column(s). For example:\n\nCreate a report that shows the number of orders per orderpriority segment.\n\n\n%%sql\nSELECT\n  o_orderpriority,\n  COUNT(*) AS num_orders\nFROM\n  orders\nGROUP BY\n  o_orderpriority;\n\nIn the above query, we group the data by orderpriority, and the calculation count(*) will be applied to the rows having a specific orderpriority value.\nThe calculations allowed are typically SUM/MIN/MAX/AVG/COUNT. However, some databases have more complex aggregate functions; check your DB documentation.\n\n1.5.1 Use HAVING to filter based on the aggregates created by GROUP BY",
    "crumbs": [
      "Use SQL to transform data",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Read data, Combine tables, & aggregate numbers to understand business performance</span>"
    ]
  },
  {
    "objectID": "sql_basics.html#replicate-if.else-logic-with-case-statements",
    "href": "sql_basics.html#replicate-if.else-logic-with-case-statements",
    "title": "1  Read data, Combine tables, & aggregate numbers to understand business performance",
    "section": "1.6 Replicate IF.ELSE logic with CASE statements",
    "text": "1.6 Replicate IF.ELSE logic with CASE statements\nWe can do conditional logic in the SELECT ... FROM part of our query, as shown below.\n\n%%sql\nSELECT\n    o_orderkey,\n    o_totalprice,\n    CASE\n        WHEN o_totalprice &gt; 100000 THEN 'high'\n        WHEN o_totalprice BETWEEN 25000\n        AND 100000 THEN 'medium'\n        ELSE 'low'\n    END AS order_price_bucket\nFROM\n    orders;\n\nWe can see how we display different values depending on the totalprice column. We can also use multiple criteria as our conditional criteria (e.g., totalprice &gt; 100000 AND orderpriority = ‘2-HIGH’).",
    "crumbs": [
      "Use SQL to transform data",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Read data, Combine tables, & aggregate numbers to understand business performance</span>"
    ]
  },
  {
    "objectID": "sql_basics.html#stack-tables-on-top-of-each-other-with-union-and-union-all-subtract-tables-with-except",
    "href": "sql_basics.html#stack-tables-on-top-of-each-other-with-union-and-union-all-subtract-tables-with-except",
    "title": "1  Read data, Combine tables, & aggregate numbers to understand business performance",
    "section": "1.7 Stack tables on top of each other with UNION and UNION ALL, subtract tables with EXCEPT",
    "text": "1.7 Stack tables on top of each other with UNION and UNION ALL, subtract tables with EXCEPT\nWhen we want to combine data from tables by stacking them on top of each other, we use UNION or UNION ALL. UNION removes duplicate rows, and UNION ALL does not remove duplicate rows. Let’s look at an example.\n\n%%sql\n\nSELECT c_custkey, c_name FROM customer WHERE c_name LIKE '%_91%' -- 25 rows\n\n\n%%sql\n-- UNION will remove duplicate rows; the below query will produce 25 rows\nSELECT c_custkey, c_name FROM customer WHERE c_name LIKE'%_91%'\nUNION\nSELECT c_custkey, c_name FROM customer WHERE c_name LIKE '%_91%'\nUNION\nSELECT c_custkey, c_name FROM customer WHERE c_name LIKE '%_91'\n\n\n%%sql\n-- UNION ALL will not remove duplicate rows; the below query will produce 75 rows\nSELECT c_custkey, c_name FROM customer WHERE c_name LIKE '%_91%'\nUNION ALL\nSELECT c_custkey, c_name FROM customer WHERE c_name LIKE '%_91%'\nUNION ALL\nSELECT c_custkey, c_name FROM customer WHERE c_name LIKE '%_91%';\n\nWhen we want to get all the rows from the first dataset that are not in the second dataset, we can use EXCEPT.\n\n%%sql\n-- EXCEPT will get the rows in the first query result that is not in the second query result, 0 rows\nSELECT c_custkey, c_name FROM customer WHERE c_name LIKE '%_91%'\nEXCEPT\nSELECT c_custkey, c_name FROM customer WHERE c_name LIKE '%_91%';\n\n\n%%sql\n-- The below query will result in 23 rows; the first query has 25 rows, and the second has two rows\nSELECT c_custkey, c_name FROM customer WHERE c_name LIKE'%_91%'\nEXCEPT\nSELECT c_custkey, c_name FROM customer WHERE c_name LIKE '%191%';",
    "crumbs": [
      "Use SQL to transform data",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Read data, Combine tables, & aggregate numbers to understand business performance</span>"
    ]
  },
  {
    "objectID": "sql_basics.html#sub-query-use-query-instead-of-a-table",
    "href": "sql_basics.html#sub-query-use-query-instead-of-a-table",
    "title": "1  Read data, Combine tables, & aggregate numbers to understand business performance",
    "section": "1.8 Sub-query: Use query instead of a table",
    "text": "1.8 Sub-query: Use query instead of a table\nWhen we want to use the result of a query as a table in another query, we use subqueries. Let’s consider an example:\n\nCreate a report that shows the nation, how many items it supplied (by suppliers in that nation), and how many items it purchased (by customers in that nation).\n\n\n%%sql\nSELECT\n  n.n_name AS nation_c_name,\n  s.quantity AS supplied_items_quantity,\n  c.quantity AS purchased_items_quantity\nFROM\n  nation n\n  LEFT JOIN (\n    SELECT\n      n.n_nationkey,\n      SUM(l.l_quantity) AS quantity\n    FROM\n      lineitem l\n      JOIN supplier s ON l.l_suppkey = s.s_suppkey\n      JOIN nation n ON s.s_nationkey = n.n_nationkey\n    GROUP BY\n      n.n_nationkey\n  ) s ON n.n_nationkey = s.n_nationkey\n  LEFT JOIN (\n    SELECT\n      n.n_nationkey,\n      SUM(l.l_quantity) AS quantity\n    FROM\n      lineitem l\n      JOIN orders o ON l.l_orderkey = o.o_orderkey\n      JOIN customer c ON o.o_custkey = c.c_custkey\n      JOIN nation n ON c.c_nationkey = n.n_nationkey\n    GROUP BY\n      n.n_nationkey\n  ) c ON n.n_nationkey = c.n_nationkey;\n\nIn the above query, we can see that there are two sub-queries, one to calculate the quantity supplied by a nation and the other to calculate the quantity purchased by the customers of a nation.",
    "crumbs": [
      "Use SQL to transform data",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Read data, Combine tables, & aggregate numbers to understand business performance</span>"
    ]
  },
  {
    "objectID": "sql_basics.html#change-data-types-cast-and-handle-nulls-coalesce",
    "href": "sql_basics.html#change-data-types-cast-and-handle-nulls-coalesce",
    "title": "1  Read data, Combine tables, & aggregate numbers to understand business performance",
    "section": "1.9 Change data types (CAST) and handle NULLS (COALESCE)",
    "text": "1.9 Change data types (CAST) and handle NULLS (COALESCE)\nEvery column in a table has a specific data type. The data types fall under one of the following categories.\n\nNumerical: Data types used to store numbers.\n\nInteger: Positive and negative numbers. Different types of Integer, such as tinyint, int, and bigint, allow storage of different ranges of values. Integers cannot have decimal digits.\nFloating: These can have decimal digits but stores an approximate value.\nDecimal: These can have decimal digits and store the exact value. The decimal type allows you to specify the scale and precision. Where scale denotes the count of numbers allowed as a whole & precision denotes the count of numbers allowed after the decimal point. E.g., DECIMAL(8,3) allows eight numbers in total, with three allowed after the decimal point.\n\nBoolean: Data types used to store True or False values.\nString: Data types used to store alphanumeric characters.\n\nVarchar(n): Data type allows storage of variable character string, with a permitted max length n.\nChar(n): Data type allows storage of fixed character string. A column of char(n) type adds (length(string) - n) empty spaces to a string that does not have n characters.\n\nDate & time: Data types used to store dates, time, & timestamps(date + time).\nObjects (JSON, ARRAY): Data types used to store JSON and ARRAY data.\n\nSome databases have data types that are unique to them as well. We should check the database documents to understand the data types offered.\nFunctions such as DATE_DIFF and ROUND are specific to a data type. It is best practice to use the appropriate data type for your columns. We can convert data types using the CAST function, as shown below.\nA NULL will be used for that field when a value is not present. In cases where we want to use the first non-NULL value from a list of columns, we use COALESCE as shown below.\nLet’s consider an example as shown below. We can see how when l.orderkey is NULL; the DB uses 999999 as the output.\n\n%%sql\nSELECT\n    o.o_orderkey,\n    o.o_orderdate,\n    COALESCE(l.l_orderkey, 9999999) AS lineitem_orderkey,\n    l.l_shipdate\nFROM\n    orders o\n    LEFT JOIN lineitem l ON o.o_orderkey = l.l_orderkey\n    AND o.o_orderdate BETWEEN l.l_shipdate - INTERVAL '5' DAY\n    AND l.l_shipdate + INTERVAL '5' DAY\nLIMIT\n    10;",
    "crumbs": [
      "Use SQL to transform data",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Read data, Combine tables, & aggregate numbers to understand business performance</span>"
    ]
  },
  {
    "objectID": "sql_basics.html#use-these-standard-inbuilt-db-functions-for-string-time-and-numeric-data-manipulation",
    "href": "sql_basics.html#use-these-standard-inbuilt-db-functions-for-string-time-and-numeric-data-manipulation",
    "title": "1  Read data, Combine tables, & aggregate numbers to understand business performance",
    "section": "1.10 Use these standard inbuilt DB functions for String, Time, and Numeric data manipulation",
    "text": "1.10 Use these standard inbuilt DB functions for String, Time, and Numeric data manipulation\nWhen processing data, more often than not, we will need to change values in columns; shown below are a few standard functions to be aware of:\n\nString functions\n\nLENGTH is used to calculate the length of a string. E.g., SELECT LENGTH('hi'); will output 2.\nCONCAT combines multiple string columns into one. E.g., SELECT CONCAT(clerk, '-', orderpriority) FROM ORDERS LIMIT 5; will concatenate clear and orderpriority columns with a dash in between them.\nSPLIT is used to split a value into an array based on a given delimiter. E.g., SELECT SPLIT(clerk, '#') FROM ORDERS LIMIT 5; will output a column with arrays formed by splitting clerk values on #.\nSUBSTRING is used to get a sub-string from a value, given the start and end character indices. E.g., SELECT clerk, SUBSTR(clerk, 1, 5) FROM orders LIMIT 5; will get the first five (1 - 5) characters of the clerk column. Note that the indexing starts from 1 in Trino.\nTRIM is used to remove empty spaces to the left and right of the value. E.g., SELECT TRIM(' hi '); will output hi without any spaces around it. LTRIM and RTRIM are similar but only remove spaces before and after the string, respectively.\n\nDate and Time functions\n\nAdding and subtracting dates: Is used to add and subtract periods; the format heavily depends on the DB. E.g., In Trino, the query\n  SELECT\n  date_diff('DAY', DATE '2022-10-01', DATE '2023-11-05') diff_in_days,\n  date_diff('MONTH', DATE '2022-10-01', DATE '2023-11-05') diff_in_months,\n  date_diff('YEAR', DATE '2022-10-01', DATE '2023-11-05') diff_in_years;\nIt will show the difference between the two dates in the specified period. We can also add/subtract an arbitrary period from a date/time column. E.g., SELECT DATE '2022-11-05' + INTERVAL '10' DAY; will show the output 2022-11-15.\nstring &lt;=&gt; date/time conversions: When we want to change the data type of a string to date/time, we can use the DATE 'YYYY-MM-DD' or TIMESTAMP 'YYYY-MM-DD HH:mm:SS functions. But when the data is in a different date/time format such as MM/DD/YYYY, we will need to specify the input structure; we do this using date_parse, E.g. SELECT date_parse('11-05-2023', '%m-%d-%Y');. We can convert a timestamp/date into a string with the required format using date_format. E.g., SELECT DATE_FORMAT(orderdate, '%Y-%m-01') AS first_month_date FROM orders LIMIT 5; will map every orderdate to the first of their month.\nTime frame functions (YEAR/MONTH/DAY): When we want to extract specific periods from a date/time column, we can use these functions. E.g., SELECT year(date '2023-11-05'); will return 2023. Similarly, we have month, day, hour, min, etc.\n\nNumeric\n\nROUND is used to specify the number of digits allowed after the decimal point. E.g. SELECT ROUND(100.102345, 2);",
    "crumbs": [
      "Use SQL to transform data",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Read data, Combine tables, & aggregate numbers to understand business performance</span>"
    ]
  },
  {
    "objectID": "sql_basics.html#save-queries-as-views-for-more-straightforward-reads",
    "href": "sql_basics.html#save-queries-as-views-for-more-straightforward-reads",
    "title": "1  Read data, Combine tables, & aggregate numbers to understand business performance",
    "section": "1.11 Save queries as views for more straightforward reads",
    "text": "1.11 Save queries as views for more straightforward reads\nWhen we have large/complex queries that we need to run often, we can save them as views. Views are DB objects that operate similarly to a table. The OLAP DB executes the underlying query when we query a view.\nUse views to hide query complexities and limit column access (by exposing only specific table columns) for end-users.\nFor example, we can create a view for the nation-level report from the above section, as shown below.\n\n%%sql\nDROP VIEW IF EXISTS nation_supplied_purchased_quantity\n\n\n%%sql\nCREATE VIEW nation_supplied_purchased_quantity AS\nSELECT\n    n.n_name AS nation_name,\n    s.quantity AS supplied_items_quantity,\n    c.quantity AS purchased_items_quantity\nFROM\n    nation n\n    LEFT JOIN (\n        SELECT\n            n_nationkey as nationkey,\n            sum(l_quantity) AS quantity\n        FROM\n            lineitem l\n            JOIN supplier s ON l.l_suppkey = s.s_suppkey\n            JOIN nation n ON s.s_nationkey = n.n_nationkey\n        GROUP BY\n            n.n_nationkey\n    ) s ON n.n_nationkey = s.nationkey\n    LEFT JOIN (\n        SELECT\n            n_nationkey as nationkey,\n            sum(l_quantity) AS quantity\n        FROM\n            lineitem l\n            JOIN orders o ON l.l_orderkey = o.o_orderkey\n            JOIN customer c ON o.o_custkey = c.c_custkey\n            JOIN nation n ON c.c_nationkey = n.n_nationkey\n        GROUP BY\n            n.n_nationkey\n    ) c ON n.n_nationkey = c.nationkey;\n\n\n%%sql\nSELECT\n    *\nFROM\n    nation_supplied_purchased_quantity;\n\nNow the view nation_supplied_purchased_quantity will run the underlying query when used. Note here we use the minio.tpch schema because catalog tpch does not allow the creation of views. The tpch catalog comes with Trino and only allows read operations. Read more about connectors here.",
    "crumbs": [
      "Use SQL to transform data",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Read data, Combine tables, & aggregate numbers to understand business performance</span>"
    ]
  },
  {
    "objectID": "sql_basics.html#exercises",
    "href": "sql_basics.html#exercises",
    "title": "1  Read data, Combine tables, & aggregate numbers to understand business performance",
    "section": "1.12 Exercises",
    "text": "1.12 Exercises\n\nCreate a report that shows the number of returns for each region name\nTop 10 most selling parts\nSellers who sell atleast one of the top 10 selling parts\nNumber of returns per order price bucket\n\nAssume the price bucket logic is\n CASE\n        WHEN totalprice &gt; 100000 THEN 'high'\n        WHEN totalprice BETWEEN 25000\n        AND 100000 THEN 'medium'\n        ELSE 'low'\n    END AS order_price_bucket\n\nAverage time (in days) between receiptdate and shipdate for each nation",
    "crumbs": [
      "Use SQL to transform data",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Read data, Combine tables, & aggregate numbers to understand business performance</span>"
    ]
  },
  {
    "objectID": "sql_basics.html#recommended-reading",
    "href": "sql_basics.html#recommended-reading",
    "title": "1  Read data, Combine tables, & aggregate numbers to understand business performance",
    "section": "1.13 Recommended reading",
    "text": "1.13 Recommended reading\n\nhttps://www.startdataengineering.com/post/improve-sql-skills-de/\nhttps://www.startdataengineering.com/post/n-sql-tips-de/\nhttps://www.startdataengineering.com/post/advanced-sql/",
    "crumbs": [
      "Use SQL to transform data",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Read data, Combine tables, & aggregate numbers to understand business performance</span>"
    ]
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "7  Summary",
    "section": "",
    "text": "In summary, this book has no content whatsoever.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Summary</span>"
    ]
  },
  {
    "objectID": "sql.html",
    "href": "sql.html",
    "title": "Use SQL to transform data",
    "section": "",
    "text": "SQL is the foundation on which data engineering works. Most data pipelines are SQL scripts strung together. While there are caveats for using Python v SQL (add:link) in data engineering, almost always SQL (or SQL-like dataframe) are the way you will code.\nIn data engineering context, SQL is used for\n\nAnalytical querying, which involves large amounts of data and aggeregating them to create metrics that define how well business has been performing (e.g. daily active users for a social media company) and how to predict the future.\nData processing, which involves transforming the data from multiple systems into well modelled datasets that can be used for analytics.\n\nIn this section we will see how to use SQL to transform data, and how to use window functions to enable complex computations in SQL.",
    "crumbs": [
      "Use SQL to transform data"
    ]
  },
  {
    "objectID": "cte.html",
    "href": "cte.html",
    "title": "2  CTE (Common Table Expression) improves code readability and reduces repetition",
    "section": "",
    "text": "2.1 Why to use a CTE\nA CTE is a named select statement that can be reused in a single query.\nComplex SQL queries often involve multiple sub-queries. Multiple sub-queries make the code hard to read.Use a Common Table Expression (CTE) to make your queries readable\nCTEs also make testing complex queries simpler",
    "crumbs": [
      "Use SQL to transform data",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>CTE (Common Table Expression) improves code readability and reduces repetition</span>"
    ]
  },
  {
    "objectID": "cte.html#how-to-define-a-cte",
    "href": "cte.html#how-to-define-a-cte",
    "title": "2  CTE (Common Table Expression) improves code readability and reduces repetition",
    "section": "2.2 How to define a CTE",
    "text": "2.2 How to define a CTE\nUse the WITH key word to start defining a CTE, the with key word is not necessary for consequetive CTE definitions.\n\n%%sql\nuse prod.db\n\n\n%%sql\n-- CTE definition\nWITH\n  supplier_nation_metrics AS ( -- CTE 1 defined using WITH keyword\n    SELECT\n      n.n_nationkey,\n      SUM(l.l_QUANTITY) AS num_supplied_parts\n    FROM\n      lineitem l\n      JOIN supplier s ON l.l_suppkey = s.s_suppkey\n      JOIN nation n ON s.s_nationkey = n.n_nationkey\n    GROUP BY\n      n.n_nationkey\n  ),\n  buyer_nation_metrics AS ( -- CTE 2 defined just as a name\n    SELECT\n      n.n_nationkey,\n      SUM(l.l_QUANTITY) AS num_purchased_parts\n    FROM\n      lineitem l\n      JOIN orders o ON l.l_orderkey = o.o_orderkey\n      JOIN customer c ON o.o_custkey = c.c_custkey\n      JOIN nation n ON c.c_nationkey = n.n_nationkey\n    GROUP BY\n      n.n_nationkey\n  )\nSELECT -- The final select will not have a comma before it\n  n.n_name AS nation_name,\n  s.num_supplied_parts,\n  b.num_purchased_parts\nFROM\n  nation n\n  LEFT JOIN supplier_nation_metrics s ON n.n_nationkey = s.n_nationkey\n  LEFT JOIN buyer_nation_metrics b ON n.n_nationkey = b.n_nationkey\nLIMIT 10;\n\nNote that the last CTE does not have a , after it.\nLet’s look another example: Calculate the money lost due to discounts. Use lineitem to get the price of items (without discounts) that are part of an order and compare it to the order.\nadd: details extn\nHint: Figure out the grain that the comparison need to be made in. Think in steps i.e. get the price of all the items in an order without discounts and then compare it to the orders data whose totalprice has been computed with discounts.\n\n%%sql\nWITH lineitem_agg AS (\n    SELECT \n        l_orderkey,\n        SUM(l_extendedprice) AS total_price_without_discount\n    FROM \n        lineitem\n    GROUP BY \n        l_orderkey\n)\nSELECT \n    o.o_orderkey,\n    o.o_totalprice, \n    l.total_price_without_discount - o.o_totalprice AS amount_lost_to_discount\nFROM \n    orders o\nJOIN \n    lineitem_agg l ON o.o_orderkey = l.l_orderkey\nORDER BY \n    o.o_orderkey;\n\nHere are the schemas of orders and lineitem tables. add: TPCH image",
    "crumbs": [
      "Use SQL to transform data",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>CTE (Common Table Expression) improves code readability and reduces repetition</span>"
    ]
  },
  {
    "objectID": "cte.html#recreating-similar-cte-is-a-sign-that-it-should-be-a-table",
    "href": "cte.html#recreating-similar-cte-is-a-sign-that-it-should-be-a-table",
    "title": "2  CTE (Common Table Expression) improves code readability and reduces repetition",
    "section": "2.3 Recreating similar CTE is a sign that it should be a table",
    "text": "2.3 Recreating similar CTE is a sign that it should be a table\nA sql query with multiple temporary tables is better than a 1000-line SQL query with numerous CTEs.\nKeep the number of CTE per query small (depends on the size of the query, but typically &lt; 5)\nCasestudy:\nRead the query below and answer the question\nwith orders as (\nselect\n        order_id,\n        customer_id,\n        order_status,\n        order_purchase_timestamp::TIMESTAMP AS order_purchase_timestamp,\n        order_approved_at::TIMESTAMP AS order_approved_at,\n        order_delivered_carrier_date::TIMESTAMP AS order_delivered_carrier_date,\n        order_delivered_customer_date::TIMESTAMP AS order_delivered_customer_date,\n        order_estimated_delivery_date::TIMESTAMP AS order_estimated_delivery_date\n    from raw_layer.orders\n    ),\n stg_customers as (\n    select\n        customer_id,\n        zipcode,\n        city,\n        state_code,\n        datetime_created::TIMESTAMP as datetime_created,\n        datetime_updated::TIMESTAMP as datetime_updated,\n        dbt_valid_from,\n        dbt_valid_to\n    from customer_snapshot\n),\nstate as (\nselect\n        state_id::INT as state_id,\n        state_code::VARCHAR(2) as state_code,\n        state_name::VARCHAR(30) as state_name\n    from raw_layer.state\n    ),\ndim_customers as (\nselect\n    c.customer_id,\n    c.zipcode,\n    c.city,\n    c.state_code,\n    s.state_name,\n    c.datetime_created,\n    c.datetime_updated,\n    c.dbt_valid_from::TIMESTAMP as valid_from,\n    case\n        when c.dbt_valid_to is NULL then '9999-12-31'::TIMESTAMP\n        else c.dbt_valid_to::TIMESTAMP\n    end as valid_to\nfrom stg_customers as c\ninner join state as s on c.state_code = s.state_code\n)\nselect\n    o.order_id,\n    o.customer_id,\n    o.order_status,\n    o.order_purchase_timestamp,\n    o.order_approved_at,\n    o.order_delivered_carrier_date,\n    o.order_delivered_customer_date,\n    o.order_estimated_delivery_date,\n    c.zipcode as customer_zipcode,\n    c.city as customer_city,\n    c.state_code as customer_state_code,\n    c.state_name as customer_state_name\nfrom orders as o\ninner join dim_customers as c on\n    o.customer_id = c.customer_id\n    and o.order_purchase_timestamp &gt;= c.valid_from\n    and o.order_purchase_timestamp &lt;= c.valid_to;",
    "crumbs": [
      "Use SQL to transform data",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>CTE (Common Table Expression) improves code readability and reduces repetition</span>"
    ]
  },
  {
    "objectID": "cte.html#exercises",
    "href": "cte.html#exercises",
    "title": "2  CTE (Common Table Expression) improves code readability and reduces repetition",
    "section": "2.4 Exercises",
    "text": "2.4 Exercises\n\nSellers who sell atleast one of the top 10 selling parts.",
    "crumbs": [
      "Use SQL to transform data",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>CTE (Common Table Expression) improves code readability and reduces repetition</span>"
    ]
  },
  {
    "objectID": "cte.html#recommended-reading",
    "href": "cte.html#recommended-reading",
    "title": "2  CTE (Common Table Expression) improves code readability and reduces repetition",
    "section": "2.5 Recommended reading",
    "text": "2.5 Recommended reading\n\nhttps://www.startdataengineering.com/post/using-common-table-expression-in-redshift/",
    "crumbs": [
      "Use SQL to transform data",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>CTE (Common Table Expression) improves code readability and reduces repetition</span>"
    ]
  },
  {
    "objectID": "windows.html",
    "href": "windows.html",
    "title": "3  Use window function when you need to use values from other rows to compute a value for the current row",
    "section": "",
    "text": "3.1 Window functions have four parts\nWindow functions allow you to operate on a set of rows at a time and produce output which has the same grain as the input (vs GROUP BY which operates on a set of rows, but also changes the meaning of an output row).\nLets see why we may need window functions as opposed to a GROUP BY.\nNOTE Notice how GROUP BY changes granularity, i.e. the input data had one row per order (aka order grain or order level) but the output had one row per date (aka date grain or date level).\nWhen you perform some operation that requires data from multiple rows to produce the data for one row without changing the grain Window functions are almost always a good fit.\nCommon scenarios when you want to use window functions:\n%%sql\nuse prod.db\n%%sql\nSELECT\n  o_custkey,\n  o_orderdate,\n  o_totalprice,\n  SUM(o_totalprice) -- FUNCTION \n  OVER (\n    PARTITION BY\n      o_custkey -- PARTITION\n    ORDER BY\n      o_orderdate -- ORDER BY; ASCENDING ORDER unless specified as DESC\n  ) AS running_sum\nFROM\n  orders\nWHERE\n  o_custkey = 4\nORDER BY\n  o_orderdate\nLIMIT\n  10;\nThe function SUM that we use in the above query is an aggregate function. Notice how the running_sum adds up (aka aggregates) the o_totalprice over all the rows. The rows themselves are ordered in ascending order by its orderdate.\nReference: The standard aggregate functions are MIN, MAX, AVG, SUM, & COUNT, modern data systems offer a variety of powerful aggregations functions. Check your database documentation for available aggreagate functions. e.g. list of agg functions available in TrinoDB\nWrite a query to calculate the daily running average of totalprice of every customer.\nHint: Figure out the PARTITION BY column first, then the ORDER BY column and finally the FUNCTION to use to compute running average.",
    "crumbs": [
      "Use SQL to transform data",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Use window function when you need to use values from other rows to compute a value for the current row</span>"
    ]
  },
  {
    "objectID": "windows.html#window-functions-have-four-parts",
    "href": "windows.html#window-functions-have-four-parts",
    "title": "3  Use window function when you need to use values from other rows to compute a value for the current row",
    "section": "",
    "text": "Partition: Defines a set of rows based on specified column(s) value. If no partition is specified, the entire table is considered a partition.\nOrder By: This optional clause specifies how to order the rows within a partition. This is an optional clause, without this the rows inside a partition will not be ordered.\nFunction: The function to be applied on the current row.\nWindow frame: Within a partition, a window frame allows you to specify the rows to be considered in the function computation. This enables more options on how one can choose the rows to apply the function on.\n\n\n\n\n\nCreate window function\n\n\n\n\n\n\n\n\n\n3.1.1 Use window frames to define a set of rows to operate on\nWindow functions consider all the rows in a partition (depending on the type of function add:link) by default. However using a window frame one can select a set of rows withing a partition to operate on.\n\n\n\nThree order Sliding window average\n\n\nExample\nConsider a scenario where you have sales data, and you want to calculate a 3-day moving average of sales within each store:\ny%%sql SELECT store_id, sale_date, sales_amount, AVG(sales_amount) OVER ( PARTITION BY store_id ORDER BY sale_date ROWS BETWEEN 2 PRECEDING AND CURRENT ROW ) AS moving_avg_sales FROM sales;\nIn this example:\n\nPARTITION BY store_id ensures the calculation is done separately for each store.\nORDER BY sale_date defines the order of rows within each partition.\nROWS BETWEEN 2 PRECEDING AND CURRENT ROW specifies the window frame, considering the current row and the two preceding rows to calculate the moving average. add: image Without defining the window frame, the function might not be able to provide the specific moving average calculation you need.\n\n\n3.1.1.1 Use ordering of rows to define your window frame with the ROWS clause\n\nROWS: Used to select a set of rows relative to the current row based on position.\n\nRow definition format ROWS BETWEEN start_point AND end_point.\nThe start_point and end_point can be any of the following three (in the proper order:\n\nn PRECEDING: n rows preceding the current row. UNBOUNDED PRECEDING indicates all rows before the current row.\nn FOLLOWING: n rows following the current row. UNBOUNDED FOLLOWING indicates all rows after the current row.\n\n\n\nLet’s see how relative row numbers can be used to define a window range.\nConsider this window function\nAVG(total_price) OVER ( -- FUNCTION: RUNNING AVERAGE\n    PARTITION BY o_custkey -- PARTITIONED BY customer\n    ORDER BY order_month \n    ROWS BETWEEN 1 PRECEDING AND 1 FOLLOWING -- WINDOW FRAME DEFINED AS 1 ROW PRECEDING to 1 ROW FOLLOWING\n    )\n\n\n\nWindow frame with ROWS\n\n\n\n\n3.1.1.2 Use values of the columns to define window frame using RANGE clause\n\nRANGE: Used to select a set of rows relative to the current row based on the value of the columns specified in the ORDER BY clause.\n\nRange definition format RANGE BETWEEN start_point AND end_point.\nThe start_point and end_point can be any of the following:\n\nCURRENT ROW: The current row.\nn PRECEDING: All rows with values within the specified range that are less than or equal to n units preceding the value of the current row.\nn FOLLOWING: All rows with values within the specified range that are greater than or equal to n units following the value of the current row.\nUNBOUNDED PRECEDING: All rows before the current row within the partition.\nUNBOUNDED FOLLOWING: All rows after the current row within the partition.\n\nRANGE is particularly useful when dealing with numeric or date/time ranges, allowing for calculations like running totals, moving averages, or cumulative distributions.\n\n\nLet’s see how RANGE works with AVG(total price) OVER (PARTITION BY customer id ORDER BY date RANGE BETWEEN INTERVAL '1' DAY PRECEDING AND '1' DAY FOLLOWING) using the below visualization:\n\n\n\nRANGE",
    "crumbs": [
      "Use SQL to transform data",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Use window function when you need to use values from other rows to compute a value for the current row</span>"
    ]
  },
  {
    "objectID": "windows.html#ranking-functions-enable-you-to-rank-your-rows-based-on-order-by-clause",
    "href": "windows.html#ranking-functions-enable-you-to-rank-your-rows-based-on-order-by-clause",
    "title": "3  Use window function when you need to use values from other rows to compute a value for the current row",
    "section": "3.2 Ranking functions enable you to rank your rows based on order by clause",
    "text": "3.2 Ranking functions enable you to rank your rows based on order by clause\nIf you are working on a problem to get the top/bottom n rows (as defined by some value) then use the row functions.\nLet’s look at an example of how to use a row function:\nFrom the orders table get the top 3 spending customers per day. The orders table schema is shown below:\n\n\n\nOrders table\n\n\n\n%%sql\nSELECT\n  *\nFROM\n  (\n    SELECT\n      o_orderdate,\n      o_totalprice,\n      o_custkey,\n      RANK() -- RANKING FUNCTION \n      OVER (\n        PARTITION BY\n          o_orderdate -- PARTITION BY order date\n        ORDER BY\n          o_totalprice DESC -- ORDER rows withing partition by totalprice\n      ) AS rnk\n    FROM\n      orders\n  )\nWHERE\n  rnk &lt;= 3\nORDER BY\n  o_orderdate\nLIMIT\n  5;\n\nStandard RANKING functions:\n\nRANK: Ranks the rows starting from 1 to n within the window frame. Ranks the rows with the same value (defined by the “ORDER BY” clause) as the same and skips the ranking numbers that would have been present if the values were different.\nDENSE_RANK: Ranks the rows starting from 1 to n within the window frame. Ranks the rows with the same value (defined by the “ORDER BY” clause) as the same and does not skip any ranking numbers.\nROW_NUMBER: Adds a row number that starts from 1 to n within the window frame and does not create any repeating values.\n\n\n%%sql\n-- Let's look at an example showing the difference between RANK, DENSE_RANK and ROW_NUMBER\nSELECT \n    order_date,\n    order_id,\n    total_price,\n    ROW_NUMBER() OVER (PARTITION BY order_date ORDER BY total_price) AS row_number,\n    RANK() OVER (PARTITION BY order_date ORDER BY total_price) AS rank,\n    DENSE_RANK() OVER (PARTITION BY order_date ORDER BY total_price) AS dense_rank\nFROM (\n    SELECT \n        '2024-07-08' AS order_date, 'order_1' AS order_id, 100 AS total_price UNION ALL\n    SELECT \n        '2024-07-08', 'order_2', 200 UNION ALL\n    SELECT \n        '2024-07-08', 'order_3', 150 UNION ALL\n    SELECT \n        '2024-07-08', 'order_4', 90 UNION ALL\n    SELECT \n        '2024-07-08', 'order_5', 100 UNION ALL\n    SELECT \n        '2024-07-08', 'order_6', 90 UNION ALL\n    SELECT \n        '2024-07-08', 'order_7', 100 UNION ALL\n    SELECT \n        '2024-07-10', 'order_8', 100 UNION ALL\n    SELECT \n        '2024-07-10', 'order_9', 100 UNION ALL\n    SELECT \n        '2024-07-10', 'order_10', 100 UNION ALL\n    SELECT \n        '2024-07-11', 'order_11', 100\n) AS orders\nORDER BY order_date, row_number;\n\nNow that we have see how to define a window function and how to use ranking and aggregation functions, let’s take it a step further by practicing value functions.\nRemember that value functions are used to access other row’s values while operating on the current row\nLet’s take a look at LEAD and LAG functions:\n\n\n\nLAG AND LEAD",
    "crumbs": [
      "Use SQL to transform data",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Use window function when you need to use values from other rows to compute a value for the current row</span>"
    ]
  },
  {
    "objectID": "windows.html#value-functions-are-used-to-access-other-rows-values",
    "href": "windows.html#value-functions-are-used-to-access-other-rows-values",
    "title": "3  Use window function when you need to use values from other rows to compute a value for the current row",
    "section": "3.3 Value functions are used to access other rows values",
    "text": "3.3 Value functions are used to access other rows values\nStandard VALUE functions:\n\nNTILE(n): Divides the rows in the window frame into n approximately equal groups, and assigns a number to each row indicating which group it belongs to.\nFIRST_VALUE(): Returns the first value in the window frame.\nLAST_VALUE(): Returns the last value in the window frame.\nLAG(): Accesses data from a previous row within the window frame.\nLEAD(): Accesses data from a subsequent row within the window frame.",
    "crumbs": [
      "Use SQL to transform data",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Use window function when you need to use values from other rows to compute a value for the current row</span>"
    ]
  },
  {
    "objectID": "windows.html#aggregate-functions-enable-you-to-compute-running-metrics",
    "href": "windows.html#aggregate-functions-enable-you-to-compute-running-metrics",
    "title": "3  Use window function when you need to use values from other rows to compute a value for the current row",
    "section": "3.4 Aggregate functions enable you to compute running metrics",
    "text": "3.4 Aggregate functions enable you to compute running metrics\nThe standard aggregate functions are MIN, MAX, AVG, SUM, & COUNT. In addition to these make sure to check your DB engine documentation, in our case Spark Aggregate functions.\nWhen you need a running sum/min/max/avg, its almost always a use case for aggregate functions with windows.",
    "crumbs": [
      "Use SQL to transform data",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Use window function when you need to use values from other rows to compute a value for the current row</span>"
    ]
  },
  {
    "objectID": "windows.html#exercises",
    "href": "windows.html#exercises",
    "title": "3  Use window function when you need to use values from other rows to compute a value for the current row",
    "section": "3.5 Exercises",
    "text": "3.5 Exercises\n\nWrite a query on the orders table that has the following output:\n\no_custkey\norder_month: In YYYY-MM format, use strftime(o_orderdate, ‘%Y-%m’) AS order_month\ntotal_price: Sum of o_totalprice for that month\nthree_mo_total_price_avg: The 3 month (previous, current & next) average of total_price for that customer\n\n\n\n%%sql\nSELECT\n  order_month,\n  o_custkey,\n  total_price,\n  ROUND(\n    AVG(total_price) OVER ( -- FUNCTION: RUNNING AVERAGE\n      PARTITION BY\n        o_custkey -- PARTITIONED BY customer\n      ORDER BY\n        order_month ROWS BETWEEN 1 PRECEDING\n        AND 1 FOLLOWING -- WINDOW FRAME DEFINED AS 1 ROW PRECEDING to 1 ROW FOLLOWING\n    ),\n    2\n  ) AS three_mo_total_price_avg\nFROM\n  (\n    SELECT\n      date_format(o_orderdate, 'yyyy-MM') AS order_month,\n      o_custkey,\n      SUM(o_totalprice) AS total_price\n    FROM\n      orders\n    GROUP BY\n      1,\n      2\n  )\nLIMIT\n  5;\n\nNow that we have seen how to create a window frame with ROWS, let’ explore how to do this with RANGE.\n\nWrite a query on the orders table that has the following output:\n\norder_month,\no_custkey,\ntotal_price,\nthree_mo_total_price_avg\nconsecutive_three_mo_total_price_avg: The consecutive 3 month average of total_price for that customer. Note that this should only include months that are chronologically next to each other.\n\n\nTime limit during live workshop: 10 min\nHint: Use CAST(strftime(o_orderdate, '%Y-%m-01') AS DATE) to cast order_month to date format.\nHint: Use the INTERVAL format shown above to construct the window function to compute consecutive_three_mo_total_price_avg column.\n\nThe orders table schema is shown below:\n\n\n\n\nOrders table\n\n\n-- write your query here\n\nFrom the orders table get the 3 lowest spending customers per day\n\nTime limit during live workshop: 5 min\nHint 1. Figure out the PARTITION BY column first, then the ORDER BY column and finally the FUNCTION to use to compute running average.\nThe orders table schema is shown below:\n\n\n\nOrders table\n\n\n-- your code here\n\nWrite a SQL query using the orders table that calculates the following columns:\n\no_orderdate: From orders table\no_custkey: From orders table\no_totalprice: From orders table\ntotalprice_diff: The customers current day’s o_totalprice - that same customers most recent previous purchase’s o_totalprice\n\n\n\nTime limit during live workshop: 5 min\nHint:\n\nStart by figuring out what the PARTITION BY column should be, then what the ORDER BY column should be, and then finally the function to use.\nUse the LAG(column_name) ranking function to identify the prior day’s revenue.\n\nThe orders table schema is shown below:\n\n\n\n\nOrders table\n\n\n-- write your query here",
    "crumbs": [
      "Use SQL to transform data",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Use window function when you need to use values from other rows to compute a value for the current row</span>"
    ]
  },
  {
    "objectID": "windows.html#recommended-reading",
    "href": "windows.html#recommended-reading",
    "title": "3  Use window function when you need to use values from other rows to compute a value for the current row",
    "section": "3.6 Recommended reading",
    "text": "3.6 Recommended reading\n\nWindow SQL Youtube workshop",
    "crumbs": [
      "Use SQL to transform data",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Use window function when you need to use values from other rows to compute a value for the current row</span>"
    ]
  },
  {
    "objectID": "python.html",
    "href": "python.html",
    "title": "Python connects the different part of your data pipeline",
    "section": "",
    "text": "Data is stored on disk and processed in memory\nPython is the glue that holds the different parts of your data pipeline. While powerful data processing engines (Snowflake, Spark, big query, etc) have made processing large amounts of data efficient, you still need a programming language to tell these engines what to do.\nIn most companies Python dominates the data stack, you’ll typically use Python to pull data from source system (Extract), Tell the data processing engine how to process the data (e.g, via SQL queries on Snowflake or SQL/Dataframe query on Spark), and load data into its destination.\nIn this section we will go over the basics of Python, how its used in data engineering and finish with a topic that is critical to ensure code changes don’t break existing logic (testing).\nProcessing data is tricky in Python, since Python is not the most optimal language for large scale data manipulation. You would often use Python to tell a data processing engine what to do. For this reason its critical to know how what the difference between disk and memory are.\nWhen we run a Python (or any language) script, it is run as a process. Each process will use a part of your computer’s memory (RAM). Understanding the difference between RAM and Disk will enable you to write efficient data pipelines; let’s go over them:\nRAM is expensive, while disk (HDD, SSD) is cheaper. One issue with data processing is that the memory available to use is often less than the size of the data to be processed. This is when we use distributed systems like Spark or systems like DuckDB, which enable us to process larger-than-memory data.\nAs we will see in the transformation sections, when we use systems like Spark, Snowflake or Duckdb, Python is just the interface the real data processing (and Memory and disk usage) depends on the data processing engine.",
    "crumbs": [
      "Python connects the different part of your data pipeline"
    ]
  },
  {
    "objectID": "python.html#data-is-stored-on-disk-and-processed-in-memory",
    "href": "python.html#data-is-stored-on-disk-and-processed-in-memory",
    "title": "Python connects the different part of your data pipeline",
    "section": "",
    "text": "Disk and Memory\n\n\n\n\nMemory is the space used by a running process to store any information that it may need for its operation. The computer’s RAM is used for this purpose. This is where any variables you define, Pandas dataframe you use will be stored.\nDisk is used to store data. When we process data from disk (read data from csv, etc) it means that our process is reading data from disk into memory and then processing it. Computers generally use HDD or SSD to store your files.",
    "crumbs": [
      "Python connects the different part of your data pipeline"
    ]
  },
  {
    "objectID": "py_basics.html",
    "href": "py_basics.html",
    "title": "4  Manipulate data with standard libraries and co-locate code with classes and functions",
    "section": "",
    "text": "In this chapter we will see how to use Python data structures and OOM and functions.\n\n4.0.1 Use the appropriate data structure based on how data will be used\nLet’s go over some basics of the Python language:\n\nVariables: A storage location identified by its name, containing some value.\nOperations: We can do any operation (arithmetic for numbers, string transformation for text) on variables\nData Structures: They are ways of representing data. Each has its own pros and cons and places where it is the right fit. 3.1. List: A collection of elements that can be accessed by knowing the element’s location (aka index). Lists retain the order of elements in them.\n3.2. Dictionary: A collection of key-value pairs where each key is mapped to a value using a hash function. The dictionary provides fast data retrieval based on keys.\n3.3. Set: A collection of unique elements that do not allow duplicates.\n3.4. Tuple: A collection of immutable(non changeable) elements, tuples retain their order once created.\n\n\na = 10\nb = 20\n\nc = a + b\nprint(c)\n\ns = '  Some string '\nprint(s.strip())\n\nl = [1, 2, 3, 4]\n\nprint(l[0])  # Will print 1\nprint(l[3])  # Will print 4\n\nd = {'a': 1, 'b': 2}\n\nprint(d.get('a'))\nprint(d.get('b'))\n\nmy_set = set()\nmy_set.add(10)\nmy_set.add(10)\nmy_set.add(10)\nmy_set.add(30)\nprint(my_set)\n\n\n\n4.0.2 Manipulate data with control-flow loops\n\nLoops: Looping allows a specific chunk of code to be repeated several times. The most common type is the for loop.\n4.1. Comprehension: Comprehension is a shorthand way of writing a loop. This allows for concise code, great for representing simpler logic.\n\n\nfor i in range(11):\n    print(i)\n\nfor elt in l:\n    print(elt)\n\nfor k, v in d.items():\n    print(f'Key: {k}, Value: {v}')\n\n[elt*2 for elt in l]\n\ndef gt_three(input_list):\n    return [elt for elt in input_list if elt &gt; 3]\n\nlist_1 = [1, 2, 3, 4, 5, 6]\nprint(gt_three(list_1))\n\nlist_2 = [1, 2, 3, 1, 1, 1]\nprint(gt_three(list_2))\n\n\n\n4.0.3 Co-locate logic with classes and functions\n\nFunctions: A block of code that can be reused as needed. This allows us to have logic defined in one place, making it easy to maintain and use. Using it in a location is referred to as calling the function.\nClass and Objects: Think of a class as a blueprint and objects as things created based on that blueprint.\nLibrary: Libraries are code that can be reused. Python comes with standard libraries for common operations, such as a datetime library to work with time (although there are better libraries)—Standard library.\nException handling: When an error occurs, we need our code to gracefully handle it without stopping.\n\n\nclass DataExtractor:\n\n    def __init__(self, some_value):\n        self.some_value = some_value\n\n    def get_connection(self):\n        pass\n\n    def close_connection(self):\n        pass\n\nde_object = DataExtractor(10)\nprint(de_object.some_value)\n\nfrom datetime import datetime\nprint(datetime.now().strftime('%Y %m %d'))\n\nl = [1, 2, 3, 4, 5]\nindex = 10\ntry:\n    element = l[index]\n    print(f\"Element at index {index} is {element}\")\nexcept IndexError:\n    print(f\"Error: Index {index} is out of range for the list.\")\nfinally:\n    print(\"Execution completed.\")",
    "crumbs": [
      "Python connects the different part of your data pipeline",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Manipulate data with standard libraries and co-locate code with classes and functions</span>"
    ]
  },
  {
    "objectID": "py_el.html",
    "href": "py_el.html",
    "title": "5  Python has libraries to read and write data to (almost) any system",
    "section": "",
    "text": "add: image\nPython has multiple libraries that enable reading from and writing to various systems. Almost all systems these days have a Python libraries to interact with it.\nFor data engineering this means that one can use Python to interact with any part of the stack. Let’s look at the types of systems for reading and writing and how Python is used there:\n\nDatabase drivers: These are libraries that you can use to connect to a database. Database drivers require you to use credentials to create a connection to your database. Once you have the connection object, you can run queries, read data from your database in Python, etc. Some examples are psycopg2, sqlite3, duckdb, etc.\nCloud SDKs: Most cloud providers (AWS, GCP, Azure) provide their own SDK(Software Development Kit). You can use the SDK to work with any of the cloud services. In data pipelines, you would typically use the SDK to extract/load data from/to a cloud storage system(S3, GCP Cloud store, etc). Some examples of SDK are AWS, which has boto3; GCP, which has gsutil; etc.\nAPIs: Some systems expose data via APIs. Essentially, a server will accept an HTTPS request and return some data based on the parameters. Python has the popular requests library to work with APIs.\nFiles: Python enables you to read/write data into files with standard libraries(e.g., csv). Python has a plethora of libraries available for specialized files like XML, xlsx, parquet, etc.\nSFTP/FTP: These are servers typically used to provide data to clients outside your company. Python has tools like paramiko, ftplib, etc., to access the data on these servers.\nQueuing systems: These are systems that queue data (e.g., Kafka, AWS Kinesis, Redpanda, etc.). Python has libraries to read data from and write data to these systems, e.g., pykafka, etc.\n\n\nimport requests\nurl = \"https://pokeapi.co/api/v2/pokemon/1\"\nresponse = requests.get(url)\nprint(response.json())\n\n\nimport csv\n\ndata_location = \"./data/customer.csv\"\nwith open(data_location, \"r\", newline=\"\") as csvfile:\n    csvreader = csv.reader(csvfile)\n    next(csvreader)  # Skip header row\n    for row in csvreader:\n        print(row)\n        break\n\n\nimport requests\nfrom bs4 import BeautifulSoup\nurl = 'https://example.com'\n\nresponse = requests.get(url)\nsoup = BeautifulSoup(response.text, 'html.parser')\nfor link in soup.find_all('a'):\n    print(link.get('href'))",
    "crumbs": [
      "Python connects the different part of your data pipeline",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Python has libraries to read and write data to (almost) any system</span>"
    ]
  },
  {
    "objectID": "py_transform.html",
    "href": "py_transform.html",
    "title": "6  Python has libraries to tell the data processing engine (Spark, Trino, Duckdb, Polars, etc) what to do",
    "section": "",
    "text": "Almost every data processing systems has a Python library to interact with it.\nPyspark Trino Python Snowflake Python Duckdb Python Polars Python API add: link\nThe main types of data processing libraries are:\n\nPython standard libraries: Python has a strong set of standard libraries for processing data. When using standard libraries, you’ll usually be working on Python native data structures (add: link). Some popular ones are csv, json, gzip etc.\nDataframe libraries: Libraries like pandas, polars, and Spark enable you to work with tabular data. These libraries use a dataframe (Python’s version of a SQL table) and enable you to do everything (and more) you can do with SQL. Note that some of these libraries are meant for data that can be processed in memory.\nProcessing data on SQL via Python: You can use database drivers and write SQL queries to process the data. The benefit of this approach is that you don’t need to bring data into Python memory and offload it to the database.\n\nNote: When we use systems like Spark, Dask, Snowflake, BigQuery to process data, you should note that we interact with them via Python. Data is processed by the external systems.\n\nprint(\n    \"################################################################################\"\n)\nprint(\"Use standard python libraries to do the transformations\")\nprint(\n    \"################################################################################\"\n)\n\n\nimport csv\n\ndata = []\nwith open(\"./sample_data.csv\", \"r\", newline=\"\") as csvfile:\n    reader = csv.DictReader(csvfile)\n    for row in reader:\n        data.append(row)\nprint(data[:2])\n\n\ndata_unique = []\ncustomer_ids_seen = set()\nfor row in data:\n    if row[\"Customer_ID\"] not in customer_ids_seen:\n        data_unique.append(row)\n        customer_ids_seen.add(row[\"Customer_ID\"])\n    else:\n        print(f'duplicate customer id {row[\"Customer_ID\"]}')\n\n\nfor row in data_unique:\n    if not row[\"Age\"]:\n        print(f'Customer {row[\"Customer_Name\"]} does not have Age value')\n        row[\"Age\"] = 0\n    if not row[\"Purchase_Amount\"]:\n        row[\"Purchase_Amount\"] = 0.0\n\ndata_cleaned = [\n    row\n    for row in data_unique\n    if int(row[\"Age\"]) &lt;= 100 and float(row[\"Purchase_Amount\"]) &lt;= 1000\n]\n\nfor row in data_cleaned:\n    if row[\"Gender\"] == \"Female\":\n        row[\"Gender\"] = 0\n    elif row[\"Gender\"] == \"Male\":\n        row[\"Gender\"] = 1\n\nfor row in data_cleaned:\n    first_name, last_name = row[\"Customer_Name\"].split(\" \", 1)\n    row[\"First_Name\"] = first_name\n    row[\"Last_Name\"] = last_name\n\nprint(data_cleaned[:3])\n\n\nfrom collections import defaultdict\ntotal_purchase_by_gender = defaultdict(float)\nfor row in data_cleaned:\n    total_purchase_by_gender[row[\"Gender\"]] += float(row[\"Purchase_Amount\"])\n\nage_groups = {\"18-30\": [], \"31-40\": [], \"41-50\": [], \"51-60\": [], \"61-70\": []}\nfor row in data_cleaned:\n    age = int(row[\"Age\"])\n    if age &lt;= 30:\n        age_groups[\"18-30\"].append(float(row[\"Purchase_Amount\"]))\n    elif age &lt;= 40:\n        age_groups[\"31-40\"].append(float(row[\"Purchase_Amount\"]))\n    elif age &lt;= 50:\n        age_groups[\"41-50\"].append(float(row[\"Purchase_Amount\"]))\n    elif age &lt;= 60:\n        age_groups[\"51-60\"].append(float(row[\"Purchase_Amount\"]))\n    else:\n        age_groups[\"61-70\"].append(float(row[\"Purchase_Amount\"]))\n\naverage_purchase_by_age_group = {\n    group: sum(amounts) / len(amounts) for group, amounts in age_groups.items()\n}\n\n\nprint(\"Total purchase amount by Gender:\", total_purchase_by_gender)\nprint(\"Average purchase amount by Age group:\", average_purchase_by_age_group)\n\n\nspark\n\n\nprint(\n    \"################################################################################\"\n)\nprint(\"Use PySpark DataFrame API to do the transformations\")\nprint(\n    \"################################################################################\"\n)\n\n\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import col, coalesce, lit, when, split, sum as spark_sum, avg, regexp_replace\nfrom pyspark.sql.types import StructType, StructField, IntegerType, StringType, FloatType, DateType\n\nschema = StructType([\n    StructField(\"Customer_ID\", IntegerType(), True),\n    StructField(\"Customer_Name\", StringType(), True),\n    StructField(\"Age\", IntegerType(), True),\n    StructField(\"Gender\", StringType(), True),\n    StructField(\"Purchase_Amount\", FloatType(), True),\n    StructField(\"Purchase_Date\", DateType(), True)\n])\n\n# Read data from CSV file into DataFrame\ndata = spark.read \\\n    .option(\"header\", \"true\") \\\n    .option(\"inferSchema\", \"false\") \\\n    .schema(schema) \\\n    .csv(\"./sample_data.csv\")\n\n# Question: How do you remove duplicate rows based on customer ID in PySpark?\ndata_unique = data.dropDuplicates()\n\n# Question: How do you handle missing values by replacing them with 0 in PySpark?\ndata_cleaned_missing = data_unique.select(\n    col(\"Customer_ID\"),\n    col(\"Customer_Name\"),\n    coalesce(col(\"Age\"), lit(0)).alias(\"Age\"),\n    col(\"Gender\"),\n    coalesce(col(\"Purchase_Amount\"), lit(0.0)).alias(\"Purchase_Amount\"),\n    col(\"Purchase_Date\")\n)\n\n# Question: How do you remove outliers (e.g., age &gt; 100 or purchase amount &gt; 1000) in PySpark?\ndata_cleaned_outliers = data_cleaned_missing.filter(\n    (col(\"Age\") &lt;= 100) & (col(\"Purchase_Amount\") &lt;= 1000)\n)\n\n# Question: How do you convert the Gender column to a binary format (0 for Female, 1 for Male) in PySpark?\ndata_cleaned_gender = data_cleaned_outliers.withColumn(\n    \"Gender_Binary\",\n    when(col(\"Gender\") == \"Female\", 0).otherwise(1)\n)\n\n# Question: How do you split the Customer_Name column into separate First_Name and Last_Name columns in PySpark?\ndata_cleaned = data_cleaned_gender.select(\n    col(\"Customer_ID\"),\n    split(col(\"Customer_Name\"), \" \").getItem(0).alias(\"First_Name\"),\n    split(col(\"Customer_Name\"), \" \").getItem(1).alias(\"Last_Name\"),\n    col(\"Age\"),\n    col(\"Gender_Binary\"),\n    col(\"Purchase_Amount\"),\n    col(\"Purchase_Date\")\n)\n\n\n# Question: How do you calculate the total purchase amount by Gender in PySpark?\ntotal_purchase_by_gender = data_cleaned_gender.groupBy(\"Gender_Binary\") \\\n    .agg(spark_sum(\"Purchase_Amount\").alias(\"Total_Purchase_Amount\")) \\\n    .collect()\n\n\n# Question: How do you calculate the average purchase amount by Age group in PySpark?\naverage_purchase_by_age_group = data_cleaned.withColumn(\n    \"Age_Group\",\n    when((col(\"Age\") &gt;= 18) & (col(\"Age\") &lt;= 30), \"18-30\")\n    .when((col(\"Age\") &gt;= 31) & (col(\"Age\") &lt;= 40), \"31-40\")\n    .when((col(\"Age\") &gt;= 41) & (col(\"Age\") &lt;= 50), \"41-50\")\n    .when((col(\"Age\") &gt;= 51) & (col(\"Age\") &lt;= 60), \"51-60\")\n    .otherwise(\"61-70\")\n).groupBy(\"Age_Group\") \\\n    .agg(avg(\"Purchase_Amount\").alias(\"Average_Purchase_Amount\")) \\\n    .collect()\n\n\n# Question: How do you print the results for total purchase amount by Gender and average purchase amount by Age group in PySpark?\nprint(\"====================== Results ======================\")\nprint(\"Total purchase amount by Gender:\")\nfor row in total_purchase_by_gender:\n    print(f\"Gender_Binary: {row['Gender_Binary']}, Total_Purchase_Amount: {row['Total_Purchase_Amount']}\")\n\nprint(\"Average purchase amount by Age group:\")\nfor row in average_purchase_by_age_group:\n    print(f\"Age_Group: {row['Age_Group']}, Average_Purchase_Amount: {row['Average_Purchase_Amount']}\")\n\n\n# Optional: Show DataFrame contents for verification\nprint(\"\\n====================== Data Preview ======================\")\nprint(\"Final cleaned data:\")\ndata_cleaned.show(10)\n\n\nprint(\"Total purchase by gender:\")\ndata_cleaned_gender.groupBy(\"Gender_Binary\") \\\n    .agg(spark_sum(\"Purchase_Amount\").alias(\"Total_Purchase_Amount\")) \\\n    .show()\n\n\nprint(\"Average purchase by age group:\")\ndata_cleaned.withColumn(\n    \"Age_Group\",\n    when((col(\"Age\") &gt;= 18) & (col(\"Age\") &lt;= 30), \"18-30\")\n    .when((col(\"Age\") &gt;= 31) & (col(\"Age\") &lt;= 40), \"31-40\")\n    .when((col(\"Age\") &gt;= 41) & (col(\"Age\") &lt;= 50), \"41-50\")\n    .when((col(\"Age\") &gt;= 51) & (col(\"Age\") &lt;= 60), \"51-60\")\n    .otherwise(\"61-70\")\n).groupBy(\"Age_Group\") \\\n    .agg(avg(\"Purchase_Amount\").alias(\"Average_Purchase_Amount\")) \\\n    .show()",
    "crumbs": [
      "Python connects the different part of your data pipeline",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Python has libraries to tell the data processing engine (Spark, Trino, Duckdb, Polars, etc) what to do</span>"
    ]
  }
]