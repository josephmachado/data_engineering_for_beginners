[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data Engineering For Beginners",
    "section": "",
    "text": "Start here\nAre you trying to break into a high paying data engineering job, but\nThen this book is for you. This book is for anyone who wants to get into data engineering, but feels stuck, confused and end up spending a lot of time going in circles. This book is designed to help you lay the foundations for a great career in the field of data.\nAs a data engineering your primary mission will be to enable stakeholders to effectively use data to make decisions. The entierity of this book will focus on how you can do this.",
    "crumbs": [
      "Start here"
    ]
  },
  {
    "objectID": "sql_basics.html",
    "href": "sql_basics.html",
    "title": "1  Read data, Combine tables, & aggregate numbers to understand business performance",
    "section": "",
    "text": "1.1 Setup\nTo run the code, you need to generate the data and load it into Spark tables. Use the script below to do this:\n%%capture\n%%bash\npython ./generate_data.py\npython ./run_ddl.py",
    "crumbs": [
      "Use SQL to transform data",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Read data, Combine tables, & aggregate numbers to understand business performance</span>"
    ]
  },
  {
    "objectID": "sql_basics.html#a-spark-catalog-can-have-multiple-schemas-schemas-can-have-multiple-tables",
    "href": "sql_basics.html#a-spark-catalog-can-have-multiple-schemas-schemas-can-have-multiple-tables",
    "title": "1  Read data, Combine tables, & aggregate numbers to understand business performance",
    "section": "1.2 A Spark catalog can have multiple schemas, & schemas can have multiple tables",
    "text": "1.2 A Spark catalog can have multiple schemas, & schemas can have multiple tables\nTypically, database servers can have multiple databases; each database can have multiple schemas. Each schema can have multiple tables, and each table can have multiple columns.\nNote: We use Trino, which has catalogs that allow it to connect with the different underlying systems. (e.g., Postgres, Redis, Hive, etc.)\nIn our lab, we use Trino, and we can check the available catalogs, their schemas, the tables in a schema, & the columns in a table, as shown below.\n\n%%sql \nshow catalogs;\n\n\n%%sql\nshow schemas IN demo;\n\n-- Catalog -&gt; schema\n\n\n%%sql\nshow schemas IN prod;\n\n-- schema -&gt; namespace\n\n\n%%sql\nshow tables IN prod.db -- namespace -&gt; Table\n\n\n%%sql --show\nselect * from prod.db.customer limit 2\n\nNote how, when referencing the table name, we use the full path, i.e., schema.table_name. We can skip using the full path of the table if we define which schema to use for the entirety of this session, as shown below.\n\n%%sql --show\nuse prod.db\n\n\n%%sql\nDESCRIBE lineitem\n\n\n%%sql\nDESCRIBE extended lineitem",
    "crumbs": [
      "Use SQL to transform data",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Read data, Combine tables, & aggregate numbers to understand business performance</span>"
    ]
  },
  {
    "objectID": "sql_basics.html#use-selectfrom-limit-where-order-by-to-read-the-required-data",
    "href": "sql_basics.html#use-selectfrom-limit-where-order-by-to-read-the-required-data",
    "title": "1  Read data, Combine tables, & aggregate numbers to understand business performance",
    "section": "1.3 Use SELECT…FROM, LIMIT, WHERE, & ORDER BY to read the required data",
    "text": "1.3 Use SELECT…FROM, LIMIT, WHERE, & ORDER BY to read the required data\nThe most common use for querying is to read data from our tables. We can do this using a SELECT ... FROM statement, as shown below.\n\n%%sql\n-- use * to specify all columns\nSELECT\n  *\nFROM\n  orders\nLIMIT\n  4\n\n\n%%sql\n-- use column names to only read data from those columns\nSELECT\n  o_orderkey,\n  o_totalprice\nFROM\n  orders\nLIMIT\n  4\n\nHowever, running a SELECT ... FROM statement can cause issues when the data set is extensive. If you want to examine the data, use LIMIT n to instruct Trino to retrieve only the first n rows.\nWe can use the ‘WHERE’ clause to retrieve rows that match specific criteria. We can specify one or more filters within the’ WHERE’ clause. The WHERE clause with more than one filter can use combinations of AND and OR criteria to combine the filter criteria, as shown below.\n\n%%sql\n-- all customer rows that have c_nationkey = 20\nSELECT\n  *\nFROM\n  customer\nWHERE\n  c_nationkey = 20\nLIMIT\n  10;\n\n\n%%sql\n-- all customer rows that have c_nationkey = 20 and c_acctbal &gt; 1000\nSELECT\n  *\nFROM\n  customer\nWHERE\n  c_nationkey = 20\n  AND c_acctbal &gt; 1000\nLIMIT\n  10;\n\n\n%%sql\n-- all customer rows that have c_nationkey = 20 or c_acctbal &gt; 1000\nSELECT\n  *\nFROM\n  customer\nWHERE\n  c_nationkey = 20\n  OR c_acctbal &gt; 1000\nLIMIT\n  10;\n\n\n%%sql\n-- all customer rows that have (c_nationkey = 20 and c_acctbal &gt; 1000) or rows that have c_nationkey = 11\nSELECT\n  *\nFROM\n  customer\nWHERE\n  (\n    c_nationkey = 20\n    AND c_acctbal &gt; 1000\n  )\n  OR c_nationkey = 11\nLIMIT\n  10;\n\nWe can combine multiple filter clauses, as seen above. We have seen examples of equals (=) and greater than (&gt;) conditional operators. There are 6 conditional operators, they are\n\n&lt; Less than\n&gt; Greater than\n&lt;= Less than or equal to\n&gt;= Greater than or equal to\n= Equal\n&lt;&gt; and != both represent Not equal (some DBs only support one of these)\n\nAdditionally, for string types, we can make pattern matching with like condition. In a like condition, a _ means any single character, and % means zero or more characters, for example.\n\n%%sql\n-- all customer rows where the name has a 381 in it\nSELECT\n  *\nFROM\n  customer\nWHERE\n  c_name LIKE '%381%';\n\n\n%%sql\n-- all customer rows where the name ends with a 381\nSELECT\n  *\nFROM\n  customer\nWHERE\n  c_name LIKE '%381';\n\n\n%%sql\n-- all customer rows where the name starts with a 381\nSELECT\n  *\nFROM\n  customer\nWHERE\n  c_name LIKE '381%';\n\n\n%%sql\n-- all customer rows where the name has a combination of any character and 9 and 1\nSELECT\n  *\nFROM\n  customer\nWHERE\n  c_name LIKE '%_91%';\n\nWe can also filter for more than one value using IN and NOT IN.\n\n%%sql\n-- all customer rows which have nationkey = 10 or nationkey = 20\nSELECT\n  *\nFROM\n  customer\nWHERE\n  c_nationkey IN (10, 20);\n\n\n%%sql\n-- all customer rows which have do not have nationkey as 10 or 20\nSELECT\n  *\nFROM\n  customer\nWHERE\n  c_nationkey NOT IN (10, 20);\n\nWe can get the number of rows in a table using count(*) as shown below.\n\n%%sql\nSELECT\n  COUNT(*)\nFROM\n  customer;\n\n-- 1500\n\n\n%%sql\nSELECT\n  COUNT(*)\nFROM\n  lineitem;\n\n-- 60175\n\nIf we want to get the rows sorted by values in a specific column, we use ORDER BY, for example.\n\n%%sql\n-- Will show the first ten customer records with the lowest custkey\n-- rows are ordered in ASC order by default\nSELECT\n  *\nFROM\n  orders\nORDER BY\n  o_custkey\nLIMIT\n  10;\n\n\n%%sql\n-- Will show the first ten customer's records with the highest custkey\nSELECT\n  *\nFROM\n  orders\nORDER BY\n  o_custkey DESC\nLIMIT\n  10;",
    "crumbs": [
      "Use SQL to transform data",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Read data, Combine tables, & aggregate numbers to understand business performance</span>"
    ]
  },
  {
    "objectID": "sql_basics.html#combine-data-from-multiple-tables-using-joins",
    "href": "sql_basics.html#combine-data-from-multiple-tables-using-joins",
    "title": "1  Read data, Combine tables, & aggregate numbers to understand business performance",
    "section": "1.4 Combine data from multiple tables using JOINs",
    "text": "1.4 Combine data from multiple tables using JOINs\nWe can combine data from multiple tables using joins. When we write a join query, we have a format as shown below.\nSELECT\n    a.*\nFROM\n    table_a a -- LEFT table a\n    JOIN table_b b -- RIGHT table b\n    ON a.id = b.id\nThe table specified first (table_a) is the left table, whereas the table specified second is the right table. When we have multiple tables joined, we consider the joined dataset from the first two tables as the left table and the third table as the right table (The DB optimizes our join for performance).\nSELECT\n    a.*\nFROM\n    table_a a -- LEFT table a\n    JOIN table_b b -- RIGHT table b\n    ON a.id = b.id\n    JOIN table_c c -- LEFT table is the joined data from table_a & table_b, right table is table_c\n    ON a.c_id = c.id\nThere are five main types of joins:\n\n\n\nJoin Types\n\n\n\n1.4.1 1. Inner join (default): Get rows with the same join keys from both tables\n\n%%sql\nSELECT\n  o.o_orderkey,\n  l.l_orderkey\nFROM\n  orders o\n  JOIN lineitem l ON o.o_orderkey = l.l_orderkey\n  AND o.o_orderdate BETWEEN l.l_shipdate - INTERVAL '5' DAY AND l.l_shipdate  + INTERVAL '5' DAY\nLIMIT\n  10;\n\n\n%%sql\nSELECT\n  COUNT(o.o_orderkey) AS order_rows_count,\n  COUNT(l.l_orderkey) AS lineitem_rows_count\nFROM\n  orders o\n  JOIN lineitem l ON o.o_orderkey = l.l_orderkey\n  AND o.o_orderdate BETWEEN l.l_shipdate - INTERVAL '5' DAY AND l.l_shipdate  + INTERVAL '5' DAY;\n-- 2477, 2477\n\nNote: JOIN defaults to INNER JOIN`.\nThe output will contain rows from orders and lineitem that match at least one row from the other table with the specified join condition (same orderkey and orderdate within a 5-day window of the ship date).\nWe can also see that 2,477 rows from the orders and lineitem tables matched.\n\n\n1.4.2 2. Left outer join (aka left join): Get all rows from the left table and only matching rows from the right table.\n\n%%sql\n\nSELECT\n  o.o_orderkey,\n  l.l_orderkey\nFROM\n  orders o\n  LEFT JOIN lineitem l ON o.o_orderkey = l.l_orderkey\n  AND o.o_orderdate BETWEEN l.l_shipdate - INTERVAL '5' DAY AND l.l_shipdate  + INTERVAL '5' DAY\nLIMIT\n  10;\n\n\n%%sql\nSELECT\n  COUNT(o.o_orderkey) AS order_rows_count,\n  COUNT(l.l_orderkey) AS lineitem_rows_count\nFROM\n  orders o\n  LEFT JOIN lineitem l ON o.o_orderkey = l.l_orderkey\n  AND o.o_orderdate BETWEEN l.l_shipdate - INTERVAL '5' DAY AND l.l_shipdate  + INTERVAL '5' DAY;\n-- 15197, 2477\n\nThe output will include all rows from orders and the rows from lineitem that were able to find at least one matching row from the orders table with the specified join condition (same orderkey and orderdate within a 5-day window of the ship date).\nWe can also see that the number of rows from the orders table is 15,197 & from the lineitem table is 2,477. The number of rows in orders is 15,000, but the join condition produces 15,197 since some orders match with multiple line items.\n\n\n1.4.3 3. Right outer join (aka right join): Get matching rows from the left and all rows from the right table.\n\n%%sql\nSELECT\n  o.o_orderkey,\n  l.l_orderkey\nFROM\n  orders o\n  RIGHT JOIN lineitem l ON o.o_orderkey = l.l_orderkey\n  AND o.o_orderdate BETWEEN l.l_shipdate - INTERVAL '5' DAY AND l.l_shipdate  + INTERVAL '5' DAY\nLIMIT\n  10;\n\n\n%%sql\nSELECT\n  COUNT(o.o_orderkey) AS order_rows_count,\n  COUNT(l.l_orderkey) AS lineitem_rows_count\nFROM\n  orders o\n  RIGHT JOIN lineitem l ON o.o_orderkey = l.l_orderkey\n  AND o.o_orderdate BETWEEN l.l_shipdate - INTERVAL '5' DAY AND l.l_shipdate  + INTERVAL '5' DAY;\n-- 2477, 60175\n\nThe output will include the rows from orders that match at least one row from the lineitem table with the specified join condition (same orderkey and orderdate within a 5-day window of the ship date) and all rows from the lineitem table.\nWe can also see that the number of rows from the orders table is 15,197 & from the lineitem table is 2,477.\n\n\n1.4.4 4. Full outer join: Get matched and unmatched rows from both tables.\n\n%%sql\nSELECT\n  o.o_orderkey,\n  l.l_orderkey\nFROM\n  orders o\n  FULL OUTER JOIN lineitem l ON o.o_orderkey = l.l_orderkey\n  AND o.o_orderdate BETWEEN l.l_shipdate - INTERVAL '5' DAY AND l.l_shipdate  + INTERVAL '5' DAY\nLIMIT\n  10\n\n\n%%sql\nSELECT\n  COUNT(o.o_orderkey) AS order_rows_count,\n  COUNT(l.l_orderkey) AS lineitem_rows_count\nFROM\n  orders o\n  FULL OUTER JOIN lineitem l ON o.o_orderkey = l.l_orderkey\n  AND o.o_orderdate BETWEEN l.l_shipdate - INTERVAL '5' DAY AND l.l_shipdate  + INTERVAL '5' DAY;\n-- 15197, 60175\n\nThe output will include all rows from orders that match at least one row from the lineitem table with the specified join condition (same orderkey and orderdate within a 5-day window of the ship date) and all rows from the lineitem table.\nWe can also see that the number of rows from the orders table is 15,197 & from the lineitem table is 2,477.\n\n\n1.4.5 5. Cross join: Join every row in the left table with every row in the right table\n\n%%sql\nSELECT\n  n.n_name AS nation_c_name,\n  r.r_name AS region_c_name\nFROM\n  nation n\n  CROSS JOIN region r;\n\nThe output will have every row of the nation joined with every row of the region. There are 25 nations and five regions, leading to 125 rows in our result from the cross-join.\nThere are cases where we need to join a table with itself, known as a SELF-join. Let’s consider an example.\n\nFor every customer order, get the order placed earlier in the same week (Sunday - Saturday, not the previous seven days). Only show customer orders that have at least one such order.\n\n\n%%sql    \nSELECT\n    o1.o_custkey as o1_custkey,\n    o1.o_totalprice as o1_totalprice,\n    o1.o_orderdate as o1_orderdate,\n    o2.o_totalprice as o2_totalprice,\n    o2.o_orderdate as o2_orderdate\nFROM\n    orders o1\n    JOIN orders o2 ON o1.o_custkey = o2.o_custkey\n    AND year(o1.o_orderdate) = year(o2.o_orderdate)\n    AND weekofyear(o1.o_orderdate) = weekofyear(o2.o_orderdate)\nWHERE\n    o1.o_orderkey != o2.o_orderkey\nLIMIT\n    10;",
    "crumbs": [
      "Use SQL to transform data",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Read data, Combine tables, & aggregate numbers to understand business performance</span>"
    ]
  },
  {
    "objectID": "sql_basics.html#combine-data-from-multiple-rows-into-one-using-group-by",
    "href": "sql_basics.html#combine-data-from-multiple-rows-into-one-using-group-by",
    "title": "1  Read data, Combine tables, & aggregate numbers to understand business performance",
    "section": "1.5 Combine data from multiple rows into one using GROUP BY",
    "text": "1.5 Combine data from multiple rows into one using GROUP BY\nMost analytical queries require calculating metrics that involve combining data from multiple rows. GROUP BY allows us to perform aggregate calculations on data from a set of rows recognized by values of specified column(s). For example:\n\nCreate a report that shows the number of orders per orderpriority segment.\n\n\n%%sql\nSELECT\n  o_orderpriority,\n  COUNT(*) AS num_orders\nFROM\n  orders\nGROUP BY\n  o_orderpriority;\n\nIn the above query, we group the data by orderpriority, and the calculation count(*) will be applied to the rows having a specific orderpriority value.\nThe calculations allowed are typically SUM/MIN/MAX/AVG/COUNT. However, some databases have more complex aggregate functions; check your DB documentation.\n\n1.5.1 Use HAVING to filter based on the aggregates created by GROUP BY",
    "crumbs": [
      "Use SQL to transform data",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Read data, Combine tables, & aggregate numbers to understand business performance</span>"
    ]
  },
  {
    "objectID": "sql_basics.html#replicate-if.else-logic-with-case-statements",
    "href": "sql_basics.html#replicate-if.else-logic-with-case-statements",
    "title": "1  Read data, Combine tables, & aggregate numbers to understand business performance",
    "section": "1.6 Replicate IF.ELSE logic with CASE statements",
    "text": "1.6 Replicate IF.ELSE logic with CASE statements\nWe can do conditional logic in the SELECT ... FROM part of our query, as shown below.\n\n%%sql\nSELECT\n    o_orderkey,\n    o_totalprice,\n    CASE\n        WHEN o_totalprice &gt; 100000 THEN 'high'\n        WHEN o_totalprice BETWEEN 25000\n        AND 100000 THEN 'medium'\n        ELSE 'low'\n    END AS order_price_bucket\nFROM\n    orders;\n\nWe can see how we display different values depending on the totalprice column. We can also use multiple criteria as our conditional criteria (e.g., totalprice &gt; 100000 AND orderpriority = ‘2-HIGH’).",
    "crumbs": [
      "Use SQL to transform data",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Read data, Combine tables, & aggregate numbers to understand business performance</span>"
    ]
  },
  {
    "objectID": "sql_basics.html#stack-tables-on-top-of-each-other-with-union-and-union-all-subtract-tables-with-except",
    "href": "sql_basics.html#stack-tables-on-top-of-each-other-with-union-and-union-all-subtract-tables-with-except",
    "title": "1  Read data, Combine tables, & aggregate numbers to understand business performance",
    "section": "1.7 Stack tables on top of each other with UNION and UNION ALL, subtract tables with EXCEPT",
    "text": "1.7 Stack tables on top of each other with UNION and UNION ALL, subtract tables with EXCEPT\nWhen we want to combine data from tables by stacking them on top of each other, we use the UNION or UNION ALL operator. UNION removes duplicate rows, and UNION ALL does not remove duplicate rows. Let’s look at an example.\n\n%%sql\n\nSELECT c_custkey, c_name FROM customer WHERE c_name LIKE '%_91%' -- 25 rows\n\n\n%%sql\n-- UNION will remove duplicate rows; the below query will produce 25 rows\nSELECT c_custkey, c_name FROM customer WHERE c_name LIKE'%_91%'\nUNION\nSELECT c_custkey, c_name FROM customer WHERE c_name LIKE '%_91%'\nUNION\nSELECT c_custkey, c_name FROM customer WHERE c_name LIKE '%_91'\n\n\n%%sql\n-- UNION ALL will not remove duplicate rows; the below query will produce 75 rows\nSELECT c_custkey, c_name FROM customer WHERE c_name LIKE '%_91%'\nUNION ALL\nSELECT c_custkey, c_name FROM customer WHERE c_name LIKE '%_91%'\nUNION ALL\nSELECT c_custkey, c_name FROM customer WHERE c_name LIKE '%_91%';\n\nWhen we want to retrieve all rows from the first dataset that are not present in the second dataset, we can use EXCEPT.\n\n%%sql\n-- EXCEPT will get the rows in the first query result that is not in the second query result, 0 rows\nSELECT c_custkey, c_name FROM customer WHERE c_name LIKE '%_91%'\nEXCEPT\nSELECT c_custkey, c_name FROM customer WHERE c_name LIKE '%_91%';\n\n\n%%sql\n-- The below query will result in 23 rows; the first query has 25 rows, and the second has two rows\nSELECT c_custkey, c_name FROM customer WHERE c_name LIKE'%_91%'\nEXCEPT\nSELECT c_custkey, c_name FROM customer WHERE c_name LIKE '%191%';",
    "crumbs": [
      "Use SQL to transform data",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Read data, Combine tables, & aggregate numbers to understand business performance</span>"
    ]
  },
  {
    "objectID": "sql_basics.html#sub-query-use-query-instead-of-a-table",
    "href": "sql_basics.html#sub-query-use-query-instead-of-a-table",
    "title": "1  Read data, Combine tables, & aggregate numbers to understand business performance",
    "section": "1.8 Sub-query: Use query instead of a table",
    "text": "1.8 Sub-query: Use query instead of a table\nWhen we want to use the result of a query as a table in another query, we use subqueries. Let’s consider an example:\n\nCreate a report that shows the nation, how many items it supplied (by suppliers in that nation), and how many items it purchased (by customers in that nation).\n\n\n%%sql\nSELECT\n  n.n_name AS nation_c_name,\n  s.quantity AS supplied_items_quantity,\n  c.quantity AS purchased_items_quantity\nFROM\n  nation n\n  LEFT JOIN (\n    SELECT\n      n.n_nationkey,\n      SUM(l.l_quantity) AS quantity\n    FROM\n      lineitem l\n      JOIN supplier s ON l.l_suppkey = s.s_suppkey\n      JOIN nation n ON s.s_nationkey = n.n_nationkey\n    GROUP BY\n      n.n_nationkey\n  ) s ON n.n_nationkey = s.n_nationkey\n  LEFT JOIN (\n    SELECT\n      n.n_nationkey,\n      SUM(l.l_quantity) AS quantity\n    FROM\n      lineitem l\n      JOIN orders o ON l.l_orderkey = o.o_orderkey\n      JOIN customer c ON o.o_custkey = c.c_custkey\n      JOIN nation n ON c.c_nationkey = n.n_nationkey\n    GROUP BY\n      n.n_nationkey\n  ) c ON n.n_nationkey = c.n_nationkey;\n\nIn the above query, we can see that there are two sub-queries, one to calculate the quantity supplied by a nation and the other to calculate the quantity purchased by the customers of a nation.",
    "crumbs": [
      "Use SQL to transform data",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Read data, Combine tables, & aggregate numbers to understand business performance</span>"
    ]
  },
  {
    "objectID": "sql_basics.html#change-data-types-cast-and-handle-nulls-coalesce",
    "href": "sql_basics.html#change-data-types-cast-and-handle-nulls-coalesce",
    "title": "1  Read data, Combine tables, & aggregate numbers to understand business performance",
    "section": "1.9 Change data types (CAST) and handle NULLS (COALESCE)",
    "text": "1.9 Change data types (CAST) and handle NULLS (COALESCE)\nEvery column in a table has a specific data type. The data types fall under one of the following categories.\n\nNumerical: Data types used to store numbers.\n\nInteger: Positive and negative numbers. Different types of Integer, such as tinyint, int, and bigint, allow storage of different ranges of values. Integers cannot have decimal digits.\nFloating: These can have decimal digits but store an approximate value.\nDecimal: These can have decimal digits and store the exact value. The decimal type allows you to specify the scale and precision. Where scale denotes the count of numbers allowed as a whole & precision denotes the count of numbers allowed after the decimal point. E.g., DECIMAL(8,3) allows eight numbers in total, with three allowed after the decimal point.\n\nBoolean: Data types used to store True or False values.\nString: Data types used to store alphanumeric characters.\n\nVarchar(n): Data type allows storage of a variable character string, with a permitted max length n.\nChar(n): Data type allows storage of a fixed character string. A column of char(n) type adds (length(string) - n) empty spaces to a string that does not have n characters.\n\nDate & time: Data types used to store dates, time, & timestamps(date + time).\nObjects (STRUCT, ARRAY, MAP, JSON): Data types used to store JSON and ARRAY data.\n\nSome databases have data types that are unique to them as well. We should check the database documents to understand the data types offered.\nIt is best practice to use the appropriate data type for your columns. We can convert data types using the CAST function, as shown below.\nA NULL will be used for that field when a value is not present. In cases where we want to use the first non-NULL value from a list of columns, we use COALESCE as shown below.\nLet’s consider the following example. We can see how when l.orderkey is NULL, the DB uses 999999 as the output.\n\n%%sql\nSELECT\n    o.o_orderkey,\n    o.o_orderdate,\n    COALESCE(l.l_orderkey, 9999999) AS lineitem_orderkey,\n    l.l_shipdate\nFROM\n    orders o\n    LEFT JOIN lineitem l ON o.o_orderkey = l.l_orderkey\n    AND o.o_orderdate BETWEEN l.l_shipdate - INTERVAL '5' DAY\n    AND l.l_shipdate + INTERVAL '5' DAY\nLIMIT\n    10;",
    "crumbs": [
      "Use SQL to transform data",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Read data, Combine tables, & aggregate numbers to understand business performance</span>"
    ]
  },
  {
    "objectID": "sql_basics.html#use-these-standard-inbuilt-db-functions-for-string-time-and-numeric-data-manipulation",
    "href": "sql_basics.html#use-these-standard-inbuilt-db-functions-for-string-time-and-numeric-data-manipulation",
    "title": "1  Read data, Combine tables, & aggregate numbers to understand business performance",
    "section": "1.10 Use these standard inbuilt DB functions for String, Time, and Numeric data manipulation",
    "text": "1.10 Use these standard inbuilt DB functions for String, Time, and Numeric data manipulation\nWhen processing data, more often than not, we will need to change values in columns; shown below are a few standard functions to be aware of:\n\nString functions\n\nLENGTH is used to calculate the length of a string. E.g., SELECT LENGTH('hi'); will output 2.\nCONCAT combines multiple string columns into one. E.g., SELECT CONCAT(clerk, '-', orderpriority) FROM ORDERS LIMIT 5; will concatenate clerk and orderpriority columns with a dash in between them.\nSPLIT is used to split a value into an array based on a given delimiter. E.g., SELECT SPLIT(clerk, '#') FROM ORDERS LIMIT 5; will output a column with arrays formed by splitting clerk values on #.\nSUBSTRING is used to get a sub-string from a value, given the start and length. E.g., SELECT clerk, SUBSTRING(clerk, 1, 5) FROM orders LIMIT 5; will get the first five characters of the clerk column. Note that indexing starts from 1 in Spark SQL.\nTRIM is used to remove empty spaces to the left and right of the value. E.g., SELECT TRIM(' hi '); will output hi without any spaces around it. LTRIM and RTRIM are similar but only remove spaces before and after the string, respectively.\n\nDate and Time functions\n\nAdding and subtracting dates: Is used to add and subtract periods; the format heavily depends on the DB. E.g., In Spark SQL, the query\n  SELECT\n  DATEDIFF(DATE '2023-11-05', DATE '2022-10-01') AS diff_in_days,\n  MONTHS_BETWEEN(DATE '2023-11-05', DATE '2022-10-01') AS diff_in_months,\n  YEAR(DATE '2023-11-05') - YEAR(DATE '2022-10-01') AS diff_in_years;\nIt will show the difference between the two dates in the specified period. We can also add/subtract an arbitrary period from a date/time column. E.g., SELECT DATE_ADD(DATE '2022-11-05', 10); will show the output 2022-11-15.\nstring &lt;=&gt; date/time conversions: When we want to change the data type of a string to date/time, we can use the DATE 'YYYY-MM-DD' or TIMESTAMP 'YYYY-MM-DD HH:mm:SS' functions. But when the data is in a different date/time format such as MM/DD/YYYY, we will need to specify the input structure; we do this using TO_DATE or TO_TIMESTAMP. E.g. SELECT TO_DATE('11-05-2023', 'MM-dd-yyyy');. We can convert a timestamp/date into a string with the required format using DATE_FORMAT. E.g., SELECT DATE_FORMAT(orderdate, 'yyyy-MM-01') AS first_month_date FROM orders LIMIT 5; will map every orderdate to the first of their month.\nTime frame functions (YEAR/MONTH/DAY): When we want to extract specific periods from a date/time column, we can use these functions. E.g., SELECT YEAR(DATE '2023-11-05'); will return 2023. Similarly, we have MONTH, DAY, HOUR, MINUTE, etc.\n\nNumeric\n\nROUND is used to specify the number of digits allowed after the decimal point. E.g. SELECT ROUND(100.102345, 2);",
    "crumbs": [
      "Use SQL to transform data",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Read data, Combine tables, & aggregate numbers to understand business performance</span>"
    ]
  },
  {
    "objectID": "sql_basics.html#save-queries-as-views-for-more-straightforward-reads",
    "href": "sql_basics.html#save-queries-as-views-for-more-straightforward-reads",
    "title": "1  Read data, Combine tables, & aggregate numbers to understand business performance",
    "section": "1.11 Save queries as views for more straightforward reads",
    "text": "1.11 Save queries as views for more straightforward reads\nWhen we have large/complex queries that we need to run often, we can save them as views. Views are database objects that operate similarly to tables. The OLAP DB executes the underlying query when we query a view.\nUse views to hide query complexities and limit column access (by exposing only specific table columns) for end-users.\nFor example, we can create a view for the nation-level report from the above section, as shown below.\n\n%%sql\nDROP VIEW IF EXISTS nation_supplied_purchased_quantity\n\n\n%%sql\nCREATE VIEW nation_supplied_purchased_quantity AS\nSELECT\n    n.n_name AS nation_name,\n    s.quantity AS supplied_items_quantity,\n    c.quantity AS purchased_items_quantity\nFROM\n    nation n\n    LEFT JOIN (\n        SELECT\n            n_nationkey as nationkey,\n            sum(l_quantity) AS quantity\n        FROM\n            lineitem l\n            JOIN supplier s ON l.l_suppkey = s.s_suppkey\n            JOIN nation n ON s.s_nationkey = n.n_nationkey\n        GROUP BY\n            n.n_nationkey\n    ) s ON n.n_nationkey = s.nationkey\n    LEFT JOIN (\n        SELECT\n            n_nationkey as nationkey,\n            sum(l_quantity) AS quantity\n        FROM\n            lineitem l\n            JOIN orders o ON l.l_orderkey = o.o_orderkey\n            JOIN customer c ON o.o_custkey = c.c_custkey\n            JOIN nation n ON c.c_nationkey = n.n_nationkey\n        GROUP BY\n            n.n_nationkey\n    ) c ON n.n_nationkey = c.nationkey;\n\n\n%%sql\nSELECT\n    *\nFROM\n    nation_supplied_purchased_quantity;\n\nNow the view nation_supplied_purchased_quantity will run the underlying query when used.",
    "crumbs": [
      "Use SQL to transform data",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Read data, Combine tables, & aggregate numbers to understand business performance</span>"
    ]
  },
  {
    "objectID": "sql_basics.html#exercises",
    "href": "sql_basics.html#exercises",
    "title": "1  Read data, Combine tables, & aggregate numbers to understand business performance",
    "section": "1.12 Exercises",
    "text": "1.12 Exercises\n\nCreate a report that shows the number of returns for each region name Topics: JOINs, GROUP BY, COUNT\nTop 10 most selling parts Topics: aggregations, ranking, LIMIT, top N analysis\nSellers who sell at least one of the top 10 selling parts Topics: subqueries\nNumber of returns per order price bucket Topics: CASE statements, data bucketing, conditional logic\n\nCASE\n    WHEN o_totalprice &gt; 100000 THEN 'high'\n    WHEN o_totalprice BETWEEN 25000 AND 100000 THEN 'medium'\n    ELSE 'low'\nEND AS order_price_bucket\n\nAverage time (in days) between receiptdate and shipdate for each nation Topics: date arithmetic, time calculations",
    "crumbs": [
      "Use SQL to transform data",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Read data, Combine tables, & aggregate numbers to understand business performance</span>"
    ]
  },
  {
    "objectID": "sql_basics.html#recommended-reading",
    "href": "sql_basics.html#recommended-reading",
    "title": "1  Read data, Combine tables, & aggregate numbers to understand business performance",
    "section": "1.13 Recommended reading",
    "text": "1.13 Recommended reading\n\nhttps://www.startdataengineering.com/post/improve-sql-skills-de/\nhttps://www.startdataengineering.com/post/n-sql-tips-de/\nhttps://www.startdataengineering.com/post/advanced-sql/",
    "crumbs": [
      "Use SQL to transform data",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Read data, Combine tables, & aggregate numbers to understand business performance</span>"
    ]
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "7  Summary",
    "section": "",
    "text": "In summary, this book has no content whatsoever.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Summary</span>"
    ]
  },
  {
    "objectID": "sql.html",
    "href": "sql.html",
    "title": "Use SQL to transform data",
    "section": "",
    "text": "SQL is the foundation on which data engineering works. Most data pipelines consist of SQL scripts tied together. Knowing how to manipulate data with SQL expands to other interfaces, such as Dataframe, since they are used for similar processing but with a different API.\nIn the data engineering context, SQL is used for\n\nAnalytical querying, which involves significant amounts of data and aggregating them to create metrics that define how well the business has been performing (e.g., daily active users for a social media company) and how to predict the future.\nData processing, which involves transforming the data from multiple systems into well-modelled datasets that can be used for analytics.\n\nKnowing SQL in depth will enable you to build and maintain data systems effectively and troubleshoot any data issues.\nIn this section, we will explore how to utilize SQL to transform data and how to leverage window functions to facilitate complex computations within SQL.",
    "crumbs": [
      "Use SQL to transform data"
    ]
  },
  {
    "objectID": "cte.html",
    "href": "cte.html",
    "title": "2  CTE (Common Table Expression) improves code readability and reduces repetition",
    "section": "",
    "text": "2.1 Why to use a CTE\nA CTE is a named select statement that can be reused in a single query.\nComplex SQL queries often involve multiple sub-queries. Multiple sub-queries make the code hard to read.Use a Common Table Expression (CTE) to make your queries readable\nCTEs also make testing complex queries simpler",
    "crumbs": [
      "Use SQL to transform data",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>CTE (Common Table Expression) improves code readability and reduces repetition</span>"
    ]
  },
  {
    "objectID": "cte.html#how-to-define-a-cte",
    "href": "cte.html#how-to-define-a-cte",
    "title": "2  CTE (Common Table Expression) improves code readability and reduces repetition",
    "section": "2.2 How to define a CTE",
    "text": "2.2 How to define a CTE\nUse the WITH key word to start defining a CTE, the with key word is not necessary for consequetive CTE definitions.\n\n%%sql\nuse prod.db\n\n\n%%sql\n-- CTE definition\nWITH\n  supplier_nation_metrics AS ( -- CTE 1 defined using WITH keyword\n    SELECT\n      n.n_nationkey,\n      SUM(l.l_QUANTITY) AS num_supplied_parts\n    FROM\n      lineitem l\n      JOIN supplier s ON l.l_suppkey = s.s_suppkey\n      JOIN nation n ON s.s_nationkey = n.n_nationkey\n    GROUP BY\n      n.n_nationkey\n  ),\n  buyer_nation_metrics AS ( -- CTE 2 defined just as a name\n    SELECT\n      n.n_nationkey,\n      SUM(l.l_QUANTITY) AS num_purchased_parts\n    FROM\n      lineitem l\n      JOIN orders o ON l.l_orderkey = o.o_orderkey\n      JOIN customer c ON o.o_custkey = c.c_custkey\n      JOIN nation n ON c.c_nationkey = n.n_nationkey\n    GROUP BY\n      n.n_nationkey\n  )\nSELECT -- The final select will not have a comma before it\n  n.n_name AS nation_name,\n  s.num_supplied_parts,\n  b.num_purchased_parts\nFROM\n  nation n\n  LEFT JOIN supplier_nation_metrics s ON n.n_nationkey = s.n_nationkey\n  LEFT JOIN buyer_nation_metrics b ON n.n_nationkey = b.n_nationkey\nLIMIT 10;\n\nNote that the last CTE does not have a , after it.\nLet’s look another example: Calculate the money lost due to discounts. Use lineitem to get the price of items (without discounts) that are part of an order and compare it to the order.\nadd: details extn\nHint: Figure out the grain that the comparison need to be made in. Think in steps i.e. get the price of all the items in an order without discounts and then compare it to the orders data whose totalprice has been computed with discounts.\n\n%%sql\nWITH lineitem_agg AS (\n    SELECT \n        l_orderkey,\n        SUM(l_extendedprice) AS total_price_without_discount\n    FROM \n        lineitem\n    GROUP BY \n        l_orderkey\n)\nSELECT \n    o.o_orderkey,\n    o.o_totalprice, \n    l.total_price_without_discount - o.o_totalprice AS amount_lost_to_discount\nFROM \n    orders o\nJOIN \n    lineitem_agg l ON o.o_orderkey = l.l_orderkey\nORDER BY \n    o.o_orderkey;\n\nHere are the schemas of orders and lineitem tables. add: TPCH image",
    "crumbs": [
      "Use SQL to transform data",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>CTE (Common Table Expression) improves code readability and reduces repetition</span>"
    ]
  },
  {
    "objectID": "cte.html#recreating-similar-cte-is-a-sign-that-it-should-be-a-table",
    "href": "cte.html#recreating-similar-cte-is-a-sign-that-it-should-be-a-table",
    "title": "2  CTE (Common Table Expression) improves code readability and reduces repetition",
    "section": "2.3 Recreating similar CTE is a sign that it should be a table",
    "text": "2.3 Recreating similar CTE is a sign that it should be a table\nA sql query with multiple temporary tables is better than a 1000-line SQL query with numerous CTEs.\nKeep the number of CTE per query small (depends on the size of the query, but typically &lt; 5)\nCasestudy:\nRead the query below and answer the question\nwith orders as (\nselect\n        order_id,\n        customer_id,\n        order_status,\n        order_purchase_timestamp::TIMESTAMP AS order_purchase_timestamp,\n        order_approved_at::TIMESTAMP AS order_approved_at,\n        order_delivered_carrier_date::TIMESTAMP AS order_delivered_carrier_date,\n        order_delivered_customer_date::TIMESTAMP AS order_delivered_customer_date,\n        order_estimated_delivery_date::TIMESTAMP AS order_estimated_delivery_date\n    from raw_layer.orders\n    ),\n stg_customers as (\n    select\n        customer_id,\n        zipcode,\n        city,\n        state_code,\n        datetime_created::TIMESTAMP as datetime_created,\n        datetime_updated::TIMESTAMP as datetime_updated,\n        dbt_valid_from,\n        dbt_valid_to\n    from customer_snapshot\n),\nstate as (\nselect\n        state_id::INT as state_id,\n        state_code::VARCHAR(2) as state_code,\n        state_name::VARCHAR(30) as state_name\n    from raw_layer.state\n    ),\ndim_customers as (\nselect\n    c.customer_id,\n    c.zipcode,\n    c.city,\n    c.state_code,\n    s.state_name,\n    c.datetime_created,\n    c.datetime_updated,\n    c.dbt_valid_from::TIMESTAMP as valid_from,\n    case\n        when c.dbt_valid_to is NULL then '9999-12-31'::TIMESTAMP\n        else c.dbt_valid_to::TIMESTAMP\n    end as valid_to\nfrom stg_customers as c\ninner join state as s on c.state_code = s.state_code\n)\nselect\n    o.order_id,\n    o.customer_id,\n    o.order_status,\n    o.order_purchase_timestamp,\n    o.order_approved_at,\n    o.order_delivered_carrier_date,\n    o.order_delivered_customer_date,\n    o.order_estimated_delivery_date,\n    c.zipcode as customer_zipcode,\n    c.city as customer_city,\n    c.state_code as customer_state_code,\n    c.state_name as customer_state_name\nfrom orders as o\ninner join dim_customers as c on\n    o.customer_id = c.customer_id\n    and o.order_purchase_timestamp &gt;= c.valid_from\n    and o.order_purchase_timestamp &lt;= c.valid_to;",
    "crumbs": [
      "Use SQL to transform data",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>CTE (Common Table Expression) improves code readability and reduces repetition</span>"
    ]
  },
  {
    "objectID": "cte.html#exercises",
    "href": "cte.html#exercises",
    "title": "2  CTE (Common Table Expression) improves code readability and reduces repetition",
    "section": "2.4 Exercises",
    "text": "2.4 Exercises\n\nSellers who sell atleast one of the top 10 selling parts.",
    "crumbs": [
      "Use SQL to transform data",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>CTE (Common Table Expression) improves code readability and reduces repetition</span>"
    ]
  },
  {
    "objectID": "cte.html#recommended-reading",
    "href": "cte.html#recommended-reading",
    "title": "2  CTE (Common Table Expression) improves code readability and reduces repetition",
    "section": "2.5 Recommended reading",
    "text": "2.5 Recommended reading\n\nhttps://www.startdataengineering.com/post/using-common-table-expression-in-redshift/",
    "crumbs": [
      "Use SQL to transform data",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>CTE (Common Table Expression) improves code readability and reduces repetition</span>"
    ]
  },
  {
    "objectID": "windows.html",
    "href": "windows.html",
    "title": "3  Use window function when you need to use values from other rows to compute a value for the current row",
    "section": "",
    "text": "3.1 Window functions have four parts\nWindow functions allow you to operate on a set of rows at a time and produce output which has the same grain as the input (vs GROUP BY which operates on a set of rows, but also changes the meaning of an output row).\nLets see why we may need window functions as opposed to a GROUP BY.\nNOTE Notice how GROUP BY changes granularity, i.e. the input data had one row per order (aka order grain or order level) but the output had one row per date (aka date grain or date level).\nWhen you perform some operation that requires data from multiple rows to produce the data for one row without changing the grain Window functions are almost always a good fit.\nCommon scenarios when you want to use window functions:\n%%sql\nuse prod.db\n%%sql\nSELECT\n  o_custkey,\n  o_orderdate,\n  o_totalprice,\n  SUM(o_totalprice) -- FUNCTION \n  OVER (\n    PARTITION BY\n      o_custkey -- PARTITION\n    ORDER BY\n      o_orderdate -- ORDER BY; ASCENDING ORDER unless specified as DESC\n  ) AS running_sum\nFROM\n  orders\nWHERE\n  o_custkey = 4\nORDER BY\n  o_orderdate\nLIMIT\n  10;\nThe function SUM that we use in the above query is an aggregate function. Notice how the running_sum adds up (aka aggregates) the o_totalprice over all the rows. The rows themselves are ordered in ascending order by its orderdate.\nReference: The standard aggregate functions are MIN, MAX, AVG, SUM, & COUNT, modern data systems offer a variety of powerful aggregations functions. Check your database documentation for available aggreagate functions. e.g. list of agg functions available in TrinoDB\nWrite a query to calculate the daily running average of totalprice of every customer.\nHint: Figure out the PARTITION BY column first, then the ORDER BY column and finally the FUNCTION to use to compute running average.",
    "crumbs": [
      "Use SQL to transform data",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Use window function when you need to use values from other rows to compute a value for the current row</span>"
    ]
  },
  {
    "objectID": "windows.html#window-functions-have-four-parts",
    "href": "windows.html#window-functions-have-four-parts",
    "title": "3  Use window function when you need to use values from other rows to compute a value for the current row",
    "section": "",
    "text": "Partition: Defines a set of rows based on specified column(s) value. If no partition is specified, the entire table is considered a partition.\nOrder By: This optional clause specifies how to order the rows within a partition. This is an optional clause, without this the rows inside a partition will not be ordered.\nFunction: The function to be applied on the current row.\nWindow frame: Within a partition, a window frame allows you to specify the rows to be considered in the function computation. This enables more options on how one can choose the rows to apply the function on.\n\n\n\n\n\nCreate window function\n\n\n\n\n\n\n\n\n\n3.1.1 Use window frames to define a set of rows to operate on\nWindow functions consider all the rows in a partition (depending on the type of function add:link) by default. However using a window frame one can select a set of rows withing a partition to operate on.\n\n\n\nThree order Sliding window average\n\n\nExample\nConsider a scenario where you have sales data, and you want to calculate a 3-day moving average of sales within each store:\ny%%sql SELECT store_id, sale_date, sales_amount, AVG(sales_amount) OVER ( PARTITION BY store_id ORDER BY sale_date ROWS BETWEEN 2 PRECEDING AND CURRENT ROW ) AS moving_avg_sales FROM sales;\nIn this example:\n\nPARTITION BY store_id ensures the calculation is done separately for each store.\nORDER BY sale_date defines the order of rows within each partition.\nROWS BETWEEN 2 PRECEDING AND CURRENT ROW specifies the window frame, considering the current row and the two preceding rows to calculate the moving average. add: image Without defining the window frame, the function might not be able to provide the specific moving average calculation you need.\n\n\n3.1.1.1 Use ordering of rows to define your window frame with the ROWS clause\n\nROWS: Used to select a set of rows relative to the current row based on position.\n\nRow definition format ROWS BETWEEN start_point AND end_point.\nThe start_point and end_point can be any of the following three (in the proper order:\n\nn PRECEDING: n rows preceding the current row. UNBOUNDED PRECEDING indicates all rows before the current row.\nn FOLLOWING: n rows following the current row. UNBOUNDED FOLLOWING indicates all rows after the current row.\n\n\n\nLet’s see how relative row numbers can be used to define a window range.\nConsider this window function\nAVG(total_price) OVER ( -- FUNCTION: RUNNING AVERAGE\n    PARTITION BY o_custkey -- PARTITIONED BY customer\n    ORDER BY order_month \n    ROWS BETWEEN 1 PRECEDING AND 1 FOLLOWING -- WINDOW FRAME DEFINED AS 1 ROW PRECEDING to 1 ROW FOLLOWING\n    )\n\n\n\nWindow frame with ROWS\n\n\n\n\n3.1.1.2 Use values of the columns to define window frame using RANGE clause\n\nRANGE: Used to select a set of rows relative to the current row based on the value of the columns specified in the ORDER BY clause.\n\nRange definition format RANGE BETWEEN start_point AND end_point.\nThe start_point and end_point can be any of the following:\n\nCURRENT ROW: The current row.\nn PRECEDING: All rows with values within the specified range that are less than or equal to n units preceding the value of the current row.\nn FOLLOWING: All rows with values within the specified range that are greater than or equal to n units following the value of the current row.\nUNBOUNDED PRECEDING: All rows before the current row within the partition.\nUNBOUNDED FOLLOWING: All rows after the current row within the partition.\n\nRANGE is particularly useful when dealing with numeric or date/time ranges, allowing for calculations like running totals, moving averages, or cumulative distributions.\n\n\nLet’s see how RANGE works with AVG(total price) OVER (PARTITION BY customer id ORDER BY date RANGE BETWEEN INTERVAL '1' DAY PRECEDING AND '1' DAY FOLLOWING) using the below visualization:\n\n\n\nRANGE",
    "crumbs": [
      "Use SQL to transform data",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Use window function when you need to use values from other rows to compute a value for the current row</span>"
    ]
  },
  {
    "objectID": "windows.html#ranking-functions-enable-you-to-rank-your-rows-based-on-order-by-clause",
    "href": "windows.html#ranking-functions-enable-you-to-rank-your-rows-based-on-order-by-clause",
    "title": "3  Use window function when you need to use values from other rows to compute a value for the current row",
    "section": "3.2 Ranking functions enable you to rank your rows based on order by clause",
    "text": "3.2 Ranking functions enable you to rank your rows based on order by clause\nIf you are working on a problem to get the top/bottom n rows (as defined by some value) then use the row functions.\nLet’s look at an example of how to use a row function:\nFrom the orders table get the top 3 spending customers per day. The orders table schema is shown below:\n\n\n\nOrders table\n\n\n\n%%sql\nSELECT\n  *\nFROM\n  (\n    SELECT\n      o_orderdate,\n      o_totalprice,\n      o_custkey,\n      RANK() -- RANKING FUNCTION \n      OVER (\n        PARTITION BY\n          o_orderdate -- PARTITION BY order date\n        ORDER BY\n          o_totalprice DESC -- ORDER rows withing partition by totalprice\n      ) AS rnk\n    FROM\n      orders\n  )\nWHERE\n  rnk &lt;= 3\nORDER BY\n  o_orderdate\nLIMIT\n  5;\n\nStandard RANKING functions:\n\nRANK: Ranks the rows starting from 1 to n within the window frame. Ranks the rows with the same value (defined by the “ORDER BY” clause) as the same and skips the ranking numbers that would have been present if the values were different.\nDENSE_RANK: Ranks the rows starting from 1 to n within the window frame. Ranks the rows with the same value (defined by the “ORDER BY” clause) as the same and does not skip any ranking numbers.\nROW_NUMBER: Adds a row number that starts from 1 to n within the window frame and does not create any repeating values.\n\n\n%%sql\n-- Let's look at an example showing the difference between RANK, DENSE_RANK and ROW_NUMBER\nSELECT \n    order_date,\n    order_id,\n    total_price,\n    ROW_NUMBER() OVER (PARTITION BY order_date ORDER BY total_price) AS row_number,\n    RANK() OVER (PARTITION BY order_date ORDER BY total_price) AS rank,\n    DENSE_RANK() OVER (PARTITION BY order_date ORDER BY total_price) AS dense_rank\nFROM (\n    SELECT \n        '2024-07-08' AS order_date, 'order_1' AS order_id, 100 AS total_price UNION ALL\n    SELECT \n        '2024-07-08', 'order_2', 200 UNION ALL\n    SELECT \n        '2024-07-08', 'order_3', 150 UNION ALL\n    SELECT \n        '2024-07-08', 'order_4', 90 UNION ALL\n    SELECT \n        '2024-07-08', 'order_5', 100 UNION ALL\n    SELECT \n        '2024-07-08', 'order_6', 90 UNION ALL\n    SELECT \n        '2024-07-08', 'order_7', 100 UNION ALL\n    SELECT \n        '2024-07-10', 'order_8', 100 UNION ALL\n    SELECT \n        '2024-07-10', 'order_9', 100 UNION ALL\n    SELECT \n        '2024-07-10', 'order_10', 100 UNION ALL\n    SELECT \n        '2024-07-11', 'order_11', 100\n) AS orders\nORDER BY order_date, row_number;\n\nNow that we have see how to define a window function and how to use ranking and aggregation functions, let’s take it a step further by practicing value functions.\nRemember that value functions are used to access other row’s values while operating on the current row\nLet’s take a look at LEAD and LAG functions:\n\n\n\nLAG AND LEAD",
    "crumbs": [
      "Use SQL to transform data",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Use window function when you need to use values from other rows to compute a value for the current row</span>"
    ]
  },
  {
    "objectID": "windows.html#value-functions-are-used-to-access-other-rows-values",
    "href": "windows.html#value-functions-are-used-to-access-other-rows-values",
    "title": "3  Use window function when you need to use values from other rows to compute a value for the current row",
    "section": "3.3 Value functions are used to access other rows values",
    "text": "3.3 Value functions are used to access other rows values\nStandard VALUE functions:\n\nNTILE(n): Divides the rows in the window frame into n approximately equal groups, and assigns a number to each row indicating which group it belongs to.\nFIRST_VALUE(): Returns the first value in the window frame.\nLAST_VALUE(): Returns the last value in the window frame.\nLAG(): Accesses data from a previous row within the window frame.\nLEAD(): Accesses data from a subsequent row within the window frame.",
    "crumbs": [
      "Use SQL to transform data",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Use window function when you need to use values from other rows to compute a value for the current row</span>"
    ]
  },
  {
    "objectID": "windows.html#aggregate-functions-enable-you-to-compute-running-metrics",
    "href": "windows.html#aggregate-functions-enable-you-to-compute-running-metrics",
    "title": "3  Use window function when you need to use values from other rows to compute a value for the current row",
    "section": "3.4 Aggregate functions enable you to compute running metrics",
    "text": "3.4 Aggregate functions enable you to compute running metrics\nThe standard aggregate functions are MIN, MAX, AVG, SUM, & COUNT. In addition to these make sure to check your DB engine documentation, in our case Spark Aggregate functions.\nWhen you need a running sum/min/max/avg, its almost always a use case for aggregate functions with windows.",
    "crumbs": [
      "Use SQL to transform data",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Use window function when you need to use values from other rows to compute a value for the current row</span>"
    ]
  },
  {
    "objectID": "windows.html#exercises",
    "href": "windows.html#exercises",
    "title": "3  Use window function when you need to use values from other rows to compute a value for the current row",
    "section": "3.5 Exercises",
    "text": "3.5 Exercises\n\nWrite a query on the orders table that has the following output:\n\no_custkey\norder_month: In YYYY-MM format, use strftime(o_orderdate, ‘%Y-%m’) AS order_month\ntotal_price: Sum of o_totalprice for that month\nthree_mo_total_price_avg: The 3 month (previous, current & next) average of total_price for that customer\n\n\n\n%%sql\nSELECT\n  order_month,\n  o_custkey,\n  total_price,\n  ROUND(\n    AVG(total_price) OVER ( -- FUNCTION: RUNNING AVERAGE\n      PARTITION BY\n        o_custkey -- PARTITIONED BY customer\n      ORDER BY\n        order_month ROWS BETWEEN 1 PRECEDING\n        AND 1 FOLLOWING -- WINDOW FRAME DEFINED AS 1 ROW PRECEDING to 1 ROW FOLLOWING\n    ),\n    2\n  ) AS three_mo_total_price_avg\nFROM\n  (\n    SELECT\n      date_format(o_orderdate, 'yyyy-MM') AS order_month,\n      o_custkey,\n      SUM(o_totalprice) AS total_price\n    FROM\n      orders\n    GROUP BY\n      1,\n      2\n  )\nLIMIT\n  5;\n\nNow that we have seen how to create a window frame with ROWS, let’ explore how to do this with RANGE.\n\nWrite a query on the orders table that has the following output:\n\norder_month,\no_custkey,\ntotal_price,\nthree_mo_total_price_avg\nconsecutive_three_mo_total_price_avg: The consecutive 3 month average of total_price for that customer. Note that this should only include months that are chronologically next to each other.\n\n\nTime limit during live workshop: 10 min\nHint: Use CAST(strftime(o_orderdate, '%Y-%m-01') AS DATE) to cast order_month to date format.\nHint: Use the INTERVAL format shown above to construct the window function to compute consecutive_three_mo_total_price_avg column.\n\nThe orders table schema is shown below:\n\n\n\n\nOrders table\n\n\n-- write your query here\n\nFrom the orders table get the 3 lowest spending customers per day\n\nTime limit during live workshop: 5 min\nHint 1. Figure out the PARTITION BY column first, then the ORDER BY column and finally the FUNCTION to use to compute running average.\nThe orders table schema is shown below:\n\n\n\nOrders table\n\n\n-- your code here\n\nWrite a SQL query using the orders table that calculates the following columns:\n\no_orderdate: From orders table\no_custkey: From orders table\no_totalprice: From orders table\ntotalprice_diff: The customers current day’s o_totalprice - that same customers most recent previous purchase’s o_totalprice\n\n\n\nTime limit during live workshop: 5 min\nHint:\n\nStart by figuring out what the PARTITION BY column should be, then what the ORDER BY column should be, and then finally the function to use.\nUse the LAG(column_name) ranking function to identify the prior day’s revenue.\n\nThe orders table schema is shown below:\n\n\n\n\nOrders table\n\n\n-- write your query here",
    "crumbs": [
      "Use SQL to transform data",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Use window function when you need to use values from other rows to compute a value for the current row</span>"
    ]
  },
  {
    "objectID": "windows.html#recommended-reading",
    "href": "windows.html#recommended-reading",
    "title": "3  Use window function when you need to use values from other rows to compute a value for the current row",
    "section": "3.6 Recommended reading",
    "text": "3.6 Recommended reading\n\nWindow SQL Youtube workshop",
    "crumbs": [
      "Use SQL to transform data",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Use window function when you need to use values from other rows to compute a value for the current row</span>"
    ]
  },
  {
    "objectID": "python.html",
    "href": "python.html",
    "title": "Python connects the different part of your data pipeline",
    "section": "",
    "text": "Data is stored on disk and processed in memory\nPython is the glue that holds the different parts of your data pipeline. While powerful data processing engines (Snowflake, Spark, big query, etc) have made processing large amounts of data efficient, you still need a programming language to tell these engines what to do.\nIn most companies Python dominates the data stack, you’ll typically use Python to pull data from source system (Extract), Tell the data processing engine how to process the data (e.g, via SQL queries on Snowflake or SQL/Dataframe query on Spark), and load data into its destination.\nIn this section we will go over the basics of Python, how its used in data engineering and finish with a topic that is critical to ensure code changes don’t break existing logic (testing).\nProcessing data is tricky in Python, since Python is not the most optimal language for large scale data manipulation. You would often use Python to tell a data processing engine what to do. For this reason its critical to know how what the difference between disk and memory are.\nWhen we run a Python (or any language) script, it is run as a process. Each process will use a part of your computer’s memory (RAM). Understanding the difference between RAM and Disk will enable you to write efficient data pipelines; let’s go over them:\nRAM is expensive, while disk (HDD, SSD) is cheaper. One issue with data processing is that the memory available to use is often less than the size of the data to be processed. This is when we use distributed systems like Spark or systems like DuckDB, which enable us to process larger-than-memory data.\nAs we will see in the transformation sections, when we use systems like Spark, Snowflake or Duckdb, Python is just the interface the real data processing (and Memory and disk usage) depends on the data processing engine.",
    "crumbs": [
      "Python connects the different part of your data pipeline"
    ]
  },
  {
    "objectID": "python.html#data-is-stored-on-disk-and-processed-in-memory",
    "href": "python.html#data-is-stored-on-disk-and-processed-in-memory",
    "title": "Python connects the different part of your data pipeline",
    "section": "",
    "text": "Disk and Memory\n\n\n\n\nMemory is the space used by a running process to store any information that it may need for its operation. The computer’s RAM is used for this purpose. This is where any variables you define, Pandas dataframe you use will be stored.\nDisk is used to store data. When we process data from disk (read data from csv, etc) it means that our process is reading data from disk into memory and then processing it. Computers generally use HDD or SSD to store your files.",
    "crumbs": [
      "Python connects the different part of your data pipeline"
    ]
  },
  {
    "objectID": "py_basics.html",
    "href": "py_basics.html",
    "title": "4  Manipulate data with standard libraries and co-locate code with classes and functions",
    "section": "",
    "text": "In this chapter we will see how to use Python data structures and OOM and functions.\n\n4.0.1 Use the appropriate data structure based on how data will be used\nLet’s go over some basics of the Python language:\n\nVariables: A storage location identified by its name, containing some value.\nOperations: We can do any operation (arithmetic for numbers, string transformation for text) on variables\nData Structures: They are ways of representing data. Each has its own pros and cons and places where it is the right fit. 3.1. List: A collection of elements that can be accessed by knowing the element’s location (aka index). Lists retain the order of elements in them.\n3.2. Dictionary: A collection of key-value pairs where each key is mapped to a value using a hash function. The dictionary provides fast data retrieval based on keys.\n3.3. Set: A collection of unique elements that do not allow duplicates.\n3.4. Tuple: A collection of immutable(non changeable) elements, tuples retain their order once created.\n\n\na = 10\nb = 20\n\nc = a + b\nprint(c)\n\ns = '  Some string '\nprint(s.strip())\n\nl = [1, 2, 3, 4]\n\nprint(l[0])  # Will print 1\nprint(l[3])  # Will print 4\n\nd = {'a': 1, 'b': 2}\n\nprint(d.get('a'))\nprint(d.get('b'))\n\nmy_set = set()\nmy_set.add(10)\nmy_set.add(10)\nmy_set.add(10)\nmy_set.add(30)\nprint(my_set)\n\n\n\n4.0.2 Manipulate data with control-flow loops\n\nLoops: Looping allows a specific chunk of code to be repeated several times. The most common type is the for loop.\n4.1. Comprehension: Comprehension is a shorthand way of writing a loop. This allows for concise code, great for representing simpler logic.\n\n\nfor i in range(11):\n    print(i)\n\nfor elt in l:\n    print(elt)\n\nfor k, v in d.items():\n    print(f'Key: {k}, Value: {v}')\n\n[elt*2 for elt in l]\n\ndef gt_three(input_list):\n    return [elt for elt in input_list if elt &gt; 3]\n\nlist_1 = [1, 2, 3, 4, 5, 6]\nprint(gt_three(list_1))\n\nlist_2 = [1, 2, 3, 1, 1, 1]\nprint(gt_three(list_2))\n\n\n\n4.0.3 Co-locate logic with classes and functions\n\nFunctions: A block of code that can be reused as needed. This allows us to have logic defined in one place, making it easy to maintain and use. Using it in a location is referred to as calling the function.\nClass and Objects: Think of a class as a blueprint and objects as things created based on that blueprint.\nLibrary: Libraries are code that can be reused. Python comes with standard libraries for common operations, such as a datetime library to work with time (although there are better libraries)—Standard library.\nException handling: When an error occurs, we need our code to gracefully handle it without stopping.\n\n\nclass DataExtractor:\n\n    def __init__(self, some_value):\n        self.some_value = some_value\n\n    def get_connection(self):\n        pass\n\n    def close_connection(self):\n        pass\n\nde_object = DataExtractor(10)\nprint(de_object.some_value)\n\nfrom datetime import datetime\nprint(datetime.now().strftime('%Y %m %d'))\n\nl = [1, 2, 3, 4, 5]\nindex = 10\ntry:\n    element = l[index]\n    print(f\"Element at index {index} is {element}\")\nexcept IndexError:\n    print(f\"Error: Index {index} is out of range for the list.\")\nfinally:\n    print(\"Execution completed.\")",
    "crumbs": [
      "Python connects the different part of your data pipeline",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Manipulate data with standard libraries and co-locate code with classes and functions</span>"
    ]
  },
  {
    "objectID": "py_el.html",
    "href": "py_el.html",
    "title": "5  Python has libraries to read and write data to (almost) any system",
    "section": "",
    "text": "add: image\nPython has multiple libraries that enable reading from and writing to various systems. Almost all systems these days have a Python libraries to interact with it.\nFor data engineering this means that one can use Python to interact with any part of the stack. Let’s look at the types of systems for reading and writing and how Python is used there:\n\nDatabase drivers: These are libraries that you can use to connect to a database. Database drivers require you to use credentials to create a connection to your database. Once you have the connection object, you can run queries, read data from your database in Python, etc. Some examples are psycopg2, sqlite3, duckdb, etc.\nCloud SDKs: Most cloud providers (AWS, GCP, Azure) provide their own SDK(Software Development Kit). You can use the SDK to work with any of the cloud services. In data pipelines, you would typically use the SDK to extract/load data from/to a cloud storage system(S3, GCP Cloud store, etc). Some examples of SDK are AWS, which has boto3; GCP, which has gsutil; etc.\nAPIs: Some systems expose data via APIs. Essentially, a server will accept an HTTPS request and return some data based on the parameters. Python has the popular requests library to work with APIs.\nFiles: Python enables you to read/write data into files with standard libraries(e.g., csv). Python has a plethora of libraries available for specialized files like XML, xlsx, parquet, etc.\nSFTP/FTP: These are servers typically used to provide data to clients outside your company. Python has tools like paramiko, ftplib, etc., to access the data on these servers.\nQueuing systems: These are systems that queue data (e.g., Kafka, AWS Kinesis, Redpanda, etc.). Python has libraries to read data from and write data to these systems, e.g., pykafka, etc.\n\n\nimport requests\nurl = \"https://pokeapi.co/api/v2/pokemon/1\"\nresponse = requests.get(url)\nprint(response.json())\n\n\nimport csv\n\ndata_location = \"./data/customer.csv\"\nwith open(data_location, \"r\", newline=\"\") as csvfile:\n    csvreader = csv.reader(csvfile)\n    next(csvreader)  # Skip header row\n    for row in csvreader:\n        print(row)\n        break\n\n\nimport requests\nfrom bs4 import BeautifulSoup\nurl = 'https://example.com'\n\nresponse = requests.get(url)\nsoup = BeautifulSoup(response.text, 'html.parser')\nfor link in soup.find_all('a'):\n    print(link.get('href'))",
    "crumbs": [
      "Python connects the different part of your data pipeline",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Python has libraries to read and write data to (almost) any system</span>"
    ]
  },
  {
    "objectID": "py_transform.html",
    "href": "py_transform.html",
    "title": "6  Python has libraries to tell the data processing engine (Spark, Trino, Duckdb, Polars, etc) what to do",
    "section": "",
    "text": "Almost every data processing systems has a Python library to interact with it.\nPyspark Trino Python Snowflake Python Duckdb Python Polars Python API add: link\nThe main types of data processing libraries are:\n\nPython standard libraries: Python has a strong set of standard libraries for processing data. When using standard libraries, you’ll usually be working on Python native data structures (add: link). Some popular ones are csv, json, gzip etc.\nDataframe libraries: Libraries like pandas, polars, and Spark enable you to work with tabular data. These libraries use a dataframe (Python’s version of a SQL table) and enable you to do everything (and more) you can do with SQL. Note that some of these libraries are meant for data that can be processed in memory.\nProcessing data on SQL via Python: You can use database drivers and write SQL queries to process the data. The benefit of this approach is that you don’t need to bring data into Python memory and offload it to the database.\n\nNote: When we use systems like Spark, Dask, Snowflake, BigQuery to process data, you should note that we interact with them via Python. Data is processed by the external systems.\n\nprint(\n    \"################################################################################\"\n)\nprint(\"Use standard python libraries to do the transformations\")\nprint(\n    \"################################################################################\"\n)\n\n\nimport csv\n\ndata = []\nwith open(\"./sample_data.csv\", \"r\", newline=\"\") as csvfile:\n    reader = csv.DictReader(csvfile)\n    for row in reader:\n        data.append(row)\nprint(data[:2])\n\n\ndata_unique = []\ncustomer_ids_seen = set()\nfor row in data:\n    if row[\"Customer_ID\"] not in customer_ids_seen:\n        data_unique.append(row)\n        customer_ids_seen.add(row[\"Customer_ID\"])\n    else:\n        print(f'duplicate customer id {row[\"Customer_ID\"]}')\n\n\nfor row in data_unique:\n    if not row[\"Age\"]:\n        print(f'Customer {row[\"Customer_Name\"]} does not have Age value')\n        row[\"Age\"] = 0\n    if not row[\"Purchase_Amount\"]:\n        row[\"Purchase_Amount\"] = 0.0\n\ndata_cleaned = [\n    row\n    for row in data_unique\n    if int(row[\"Age\"]) &lt;= 100 and float(row[\"Purchase_Amount\"]) &lt;= 1000\n]\n\nfor row in data_cleaned:\n    if row[\"Gender\"] == \"Female\":\n        row[\"Gender\"] = 0\n    elif row[\"Gender\"] == \"Male\":\n        row[\"Gender\"] = 1\n\nfor row in data_cleaned:\n    first_name, last_name = row[\"Customer_Name\"].split(\" \", 1)\n    row[\"First_Name\"] = first_name\n    row[\"Last_Name\"] = last_name\n\nprint(data_cleaned[:3])\n\n\nfrom collections import defaultdict\ntotal_purchase_by_gender = defaultdict(float)\nfor row in data_cleaned:\n    total_purchase_by_gender[row[\"Gender\"]] += float(row[\"Purchase_Amount\"])\n\nage_groups = {\"18-30\": [], \"31-40\": [], \"41-50\": [], \"51-60\": [], \"61-70\": []}\nfor row in data_cleaned:\n    age = int(row[\"Age\"])\n    if age &lt;= 30:\n        age_groups[\"18-30\"].append(float(row[\"Purchase_Amount\"]))\n    elif age &lt;= 40:\n        age_groups[\"31-40\"].append(float(row[\"Purchase_Amount\"]))\n    elif age &lt;= 50:\n        age_groups[\"41-50\"].append(float(row[\"Purchase_Amount\"]))\n    elif age &lt;= 60:\n        age_groups[\"51-60\"].append(float(row[\"Purchase_Amount\"]))\n    else:\n        age_groups[\"61-70\"].append(float(row[\"Purchase_Amount\"]))\n\naverage_purchase_by_age_group = {\n    group: sum(amounts) / len(amounts) for group, amounts in age_groups.items()\n}\n\n\nprint(\"Total purchase amount by Gender:\", total_purchase_by_gender)\nprint(\"Average purchase amount by Age group:\", average_purchase_by_age_group)\n\n\nspark\n\n\nprint(\n    \"################################################################################\"\n)\nprint(\"Use PySpark DataFrame API to do the transformations\")\nprint(\n    \"################################################################################\"\n)\n\n\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import col, coalesce, lit, when, split, sum as spark_sum, avg, regexp_replace\nfrom pyspark.sql.types import StructType, StructField, IntegerType, StringType, FloatType, DateType\n\nschema = StructType([\n    StructField(\"Customer_ID\", IntegerType(), True),\n    StructField(\"Customer_Name\", StringType(), True),\n    StructField(\"Age\", IntegerType(), True),\n    StructField(\"Gender\", StringType(), True),\n    StructField(\"Purchase_Amount\", FloatType(), True),\n    StructField(\"Purchase_Date\", DateType(), True)\n])\n\n# Read data from CSV file into DataFrame\ndata = spark.read \\\n    .option(\"header\", \"true\") \\\n    .option(\"inferSchema\", \"false\") \\\n    .schema(schema) \\\n    .csv(\"./sample_data.csv\")\n\n# Question: How do you remove duplicate rows based on customer ID in PySpark?\ndata_unique = data.dropDuplicates()\n\n# Question: How do you handle missing values by replacing them with 0 in PySpark?\ndata_cleaned_missing = data_unique.select(\n    col(\"Customer_ID\"),\n    col(\"Customer_Name\"),\n    coalesce(col(\"Age\"), lit(0)).alias(\"Age\"),\n    col(\"Gender\"),\n    coalesce(col(\"Purchase_Amount\"), lit(0.0)).alias(\"Purchase_Amount\"),\n    col(\"Purchase_Date\")\n)\n\n# Question: How do you remove outliers (e.g., age &gt; 100 or purchase amount &gt; 1000) in PySpark?\ndata_cleaned_outliers = data_cleaned_missing.filter(\n    (col(\"Age\") &lt;= 100) & (col(\"Purchase_Amount\") &lt;= 1000)\n)\n\n# Question: How do you convert the Gender column to a binary format (0 for Female, 1 for Male) in PySpark?\ndata_cleaned_gender = data_cleaned_outliers.withColumn(\n    \"Gender_Binary\",\n    when(col(\"Gender\") == \"Female\", 0).otherwise(1)\n)\n\n# Question: How do you split the Customer_Name column into separate First_Name and Last_Name columns in PySpark?\ndata_cleaned = data_cleaned_gender.select(\n    col(\"Customer_ID\"),\n    split(col(\"Customer_Name\"), \" \").getItem(0).alias(\"First_Name\"),\n    split(col(\"Customer_Name\"), \" \").getItem(1).alias(\"Last_Name\"),\n    col(\"Age\"),\n    col(\"Gender_Binary\"),\n    col(\"Purchase_Amount\"),\n    col(\"Purchase_Date\")\n)\n\n\n# Question: How do you calculate the total purchase amount by Gender in PySpark?\ntotal_purchase_by_gender = data_cleaned_gender.groupBy(\"Gender_Binary\") \\\n    .agg(spark_sum(\"Purchase_Amount\").alias(\"Total_Purchase_Amount\")) \\\n    .collect()\n\n\n# Question: How do you calculate the average purchase amount by Age group in PySpark?\naverage_purchase_by_age_group = data_cleaned.withColumn(\n    \"Age_Group\",\n    when((col(\"Age\") &gt;= 18) & (col(\"Age\") &lt;= 30), \"18-30\")\n    .when((col(\"Age\") &gt;= 31) & (col(\"Age\") &lt;= 40), \"31-40\")\n    .when((col(\"Age\") &gt;= 41) & (col(\"Age\") &lt;= 50), \"41-50\")\n    .when((col(\"Age\") &gt;= 51) & (col(\"Age\") &lt;= 60), \"51-60\")\n    .otherwise(\"61-70\")\n).groupBy(\"Age_Group\") \\\n    .agg(avg(\"Purchase_Amount\").alias(\"Average_Purchase_Amount\")) \\\n    .collect()\n\n\n# Question: How do you print the results for total purchase amount by Gender and average purchase amount by Age group in PySpark?\nprint(\"====================== Results ======================\")\nprint(\"Total purchase amount by Gender:\")\nfor row in total_purchase_by_gender:\n    print(f\"Gender_Binary: {row['Gender_Binary']}, Total_Purchase_Amount: {row['Total_Purchase_Amount']}\")\n\nprint(\"Average purchase amount by Age group:\")\nfor row in average_purchase_by_age_group:\n    print(f\"Age_Group: {row['Age_Group']}, Average_Purchase_Amount: {row['Average_Purchase_Amount']}\")\n\n\n# Optional: Show DataFrame contents for verification\nprint(\"\\n====================== Data Preview ======================\")\nprint(\"Final cleaned data:\")\ndata_cleaned.show(10)\n\n\nprint(\"Total purchase by gender:\")\ndata_cleaned_gender.groupBy(\"Gender_Binary\") \\\n    .agg(spark_sum(\"Purchase_Amount\").alias(\"Total_Purchase_Amount\")) \\\n    .show()\n\n\nprint(\"Average purchase by age group:\")\ndata_cleaned.withColumn(\n    \"Age_Group\",\n    when((col(\"Age\") &gt;= 18) & (col(\"Age\") &lt;= 30), \"18-30\")\n    .when((col(\"Age\") &gt;= 31) & (col(\"Age\") &lt;= 40), \"31-40\")\n    .when((col(\"Age\") &gt;= 41) & (col(\"Age\") &lt;= 50), \"41-50\")\n    .when((col(\"Age\") &gt;= 51) & (col(\"Age\") &lt;= 60), \"51-60\")\n    .otherwise(\"61-70\")\n).groupBy(\"Age_Group\") \\\n    .agg(avg(\"Purchase_Amount\").alias(\"Average_Purchase_Amount\")) \\\n    .show()",
    "crumbs": [
      "Python connects the different part of your data pipeline",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Python has libraries to tell the data processing engine (Spark, Trino, Duckdb, Polars, etc) what to do</span>"
    ]
  },
  {
    "objectID": "data_model.html",
    "href": "data_model.html",
    "title": "Data modeling is the process of getting data ready for analyticsUse SQL to transform data",
    "section": "",
    "text": "As a data engineer your key objective is to enable stakeholders to be able to use data effectively to answer questions about business performance and predict how a business may perform in the future.\nMost companies production system store data in denormalized tables across multiple microservices, which make analytics hard as the data user will now be required to join across multiple tables. If your company uses a microservice architetcure this becomes impossible.\nAnalytical querying often require processing large amounts of data which can have a significant impact on the database performance which is usually unacceptable for production systems.\nProduction system ususaly only stores current state and does not store a log of changes, which is typically necessary for historical analysis\nMost companies produce event(click tracking, e-commerce ordering, server logs monitoring, etc ) which are usually too large to be stored and queried efficiently in a production database\nThese are some of the reasons you need a warehouse system to be able to analyze historical information.\nadd: image moving from denormalized tables to star schema to normalized tables",
    "crumbs": [
      "Data modeling is the process of getting data ready for analyticsUse SQL to transform data"
    ]
  },
  {
    "objectID": "dw.html",
    "href": "dw.html",
    "title": "7  Data warehouse contains historical data and is used to analyze business performance",
    "section": "",
    "text": "7.1 OLTP vs OLAP based data warehouses\nLet’s assume you work for an online bike parts seller (add: index link) company.\nAs our company grows, we may want to analyze data for various business goals. Seller will also want to analyze performance and trends to optimize their inventory. Some of the questions and feature requests may be\nThese questions ask about things that happened in the past, require reading a large amount of data, and aggregating data to get a result. These questions can be answered by querying a data warehouse.\nA data warehouse is a database that has all your company’s historical data and data warehousing is the process of getting data ready for analytics.\nThere are multiple design patterns for data warehouses, a few popular ones are\nKimball is by far the most commonly used, while companies don’t always follow it to a T, facts and dimensions form the basis of most of the data warehouses in the wild.\nThere are two main types of databases: OLTP and OLAP. Their differences are shown below.\nNote Apache Spark started as a pure data processing system and over time with the increased need for structure introduced capabilities to manage tables.",
    "crumbs": [
      "Data modeling is the process of getting data ready for analyticsUse SQL to transform data",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Data warehouse contains historical data and is used to analyze business performance</span>"
    ]
  },
  {
    "objectID": "dw.html#oltp-vs-olap-based-data-warehouses",
    "href": "dw.html#oltp-vs-olap-based-data-warehouses",
    "title": "7  Data warehouse contains historical data and is used to analyze business performance",
    "section": "",
    "text": "OLTP\nOLAP\n\n\n\n\nStands for\nOnline transaction processing\nOnline analytical processing\n\n\nUsage pattern\nOptimized for fast CRUD(create, read, update, delete) of a small number of rows\nOptimized for running select c1, c2, sum(c3),.. where .. group by on a large number of rows (aka analytical queries), and ingesting large amounts of data via bulk import or event stream\n\n\nStorage type\nRow oriented\nColumn-oriented\n\n\nData modeling\nData modeling is based on normalization\nData modeling is based on denormalization. Some popular ones are dimensional modeling and data vaults\n\n\nData state\nRepresents current state of the data\nContains historical events that have already happened\n\n\nData size\nGigabytes to Terabytes\nTerabytes and above\n\n\nExample database\nMySQL, Postgres, etc\nClickhouse, AWS Redshift, Snowflake, GCP Bigquery, etc",
    "crumbs": [
      "Data modeling is the process of getting data ready for analyticsUse SQL to transform data",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Data warehouse contains historical data and is used to analyze business performance</span>"
    ]
  },
  {
    "objectID": "dw.html#column-encoding-enables-efficient-processing-of-a-small-number-of-columns-from-a-large-wide-table",
    "href": "dw.html#column-encoding-enables-efficient-processing-of-a-small-number-of-columns-from-a-large-wide-table",
    "title": "7  Data warehouse contains historical data and is used to analyze business performance",
    "section": "7.2 Column encoding enables efficient processing of a small number of columns from a large wide table",
    "text": "7.2 Column encoding enables efficient processing of a small number of columns from a large wide table\nThe major improvement in analytical queries on OLAP is due to its column store technique. Let’s consider a table items, with the data shown below.\n\n\n\n\n\n\n\n\n\n\n\nitem_id\nitem_name\nitem_type\nitem_price\ndatetime_created\ndatetime_updated\n\n\n\n\n1\nitem_1\ngaming\n10\n‘2021-10-02 00:00:00’\n‘2021-11-02 13:00:00’\n\n\n2\nitem_2\ngaming\n20\n‘2021-10-02 01:00:00’\n‘2021-11-02 14:00:00’\n\n\n3\nitem_3\nbiking\n30\n‘2021-10-02 02:00:00’\n‘2021-11-02 15:00:00’\n\n\n4\nitem_4\nsurfing\n40\n‘2021-10-02 03:00:00’\n‘2021-11-02 16:00:00’\n\n\n5\nitem_5\nbiking\n50\n‘2021-10-02 04:00:00’\n‘2021-11-02 17:00:00’\n\n\n\nLet’s see how this table will be stored in a row and column-oriented storage. Data is stored as continuous pages (group of records) on the disk.\nRow oriented storage:\nLet’s assume that there is one row per page.\nPage 1: [1,item_1,gaming,10,'2021-10-02 00:00:00','2021-11-02 13:00:00'],\nPage 2: [2,item_2,gaming,20,'2021-10-02 01:00:00','2021-11-02 14:00:00']\nPage 3: [3,item_3,biking,30, '2021-10-02 02:00:00','2021-11-02 15:00:00'],\nPage 4: [4,item_4,surfing,40, '2021-10-02 03:00:00','2021-11-02 16:00:00'],\nPage 5: [5,item_5,biking,50, '2021-10-02 04:00:00','2021-11-02 17:00:00']\nColumn-oriented storage:\nLet’s assume that there is one column per page.\nPage 1: [1,2,3,4,5],\nPage 2: [item_1,item_2,item_3,item_4,item_5],\nPage 3: [gaming,gaming,biking,surfing,biking],\nPage 4: [10,20,30,40,50],\nPage 5: ['2021-10-02 00:00:00','2021-10-02 01:00:00','2021-10-02 02:00:00','2021-10-02 03:00:00','2021-10-02 04:00:00'],\nPage 6: ['2021-11-02 13:00:00','2021-11-02 14:00:00','2021-11-02 15:00:00','2021-11-02 16:00:00','2021-11-02 17:00:00']\nLet’s see how a simple analytical query will be executed.\nSELECT item_type,\n    SUM(price) total_price\nFROM items\nGROUP BY item_type;\nIn a row-oriented database\n\nAll the pages will need to be loaded into memory\nSum price column for same item_type values\n\nIn a column-oriented database\n\nOnly pages 3 and 4 will need to be loaded into memory. The information mapping page 3 and 4 to item_type and total_price will be encoded in the column oriented file and also stored in an OLTP called metadata db.\nSum price column for same item_type values\n\nAs you can see from this approach, we only need to read 2 pages in a column-oriented database vs 5 pages in a row-oriented database. In addition to this, a column-oriented database also provides\n\nBetter compression, as similar data types are next to each other and can be compressed more efficiently.\nVectorized processing\n\nAll of these features make a column-oriented database a great choice for storing and analyzing large amounts of data.",
    "crumbs": [
      "Data modeling is the process of getting data ready for analyticsUse SQL to transform data",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Data warehouse contains historical data and is used to analyze business performance</span>"
    ]
  },
  {
    "objectID": "dw_table_types.html",
    "href": "dw_table_types.html",
    "title": "8  Data warehouse modeling (Kimball) is based off of 2 types of tables: Fact and dimensions",
    "section": "",
    "text": "8.1 Facts represents events that occured & dimensions the entities to which events occur to\nAs we saw in the previous chapter, Kimball is by far the most commonly used, while companies don’t always follow it to a T, facts and dimensions form the basis of most of the data warehouses in the wild.\nA data warehouse is a database that stores your company’s historical data. The main types of tables you need to create to power analytics are:\nA fact table’s grain (aka granularity, level) refers to what a row in a fact table represents. E.g., In our checkout process, we can have two fact tables, one for the order and another for the individual items in the order. The items table will have one row per item purchased, whereas the order table will have one row per order made.\n%%sql\nuse prod.db\n%%sql\n-- calculating the totalprice of an order (with orderkey = 1) from it's individual items\nSELECT\n    l_orderkey,\n    round( sum(l_extendedprice * (1 - l_discount) * (1 + l_tax)),\n        2\n    ) AS totalprice\nFROM\n    lineitem\nWHERE\n    l_orderkey = 1\nGROUP BY\n    l_orderkey;\n%%sql\n-- The totalprice of an order (with orderkey = 1)\nSELECT\n    o_orderkey,\n    o_totalprice\nFROM\n    orders\nWHERE\n    o_orderkey = 1;\nNote: If you notice the slight difference in the decimal digits, it’s due to using a double datatype which is an inexact data type.\nWe can see how the lineitem table can be “rolled up” to get the data in the orders table. But having just the orders table is not sufficient since the lineitem table will provide us with individual item details such as discount and quantity details.",
    "crumbs": [
      "Data modeling is the process of getting data ready for analyticsUse SQL to transform data",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Data warehouse modeling (Kimball) is based off of 2 types of tables: Fact and dimensions</span>"
    ]
  },
  {
    "objectID": "dw_table_types.html#facts-represents-events-that-occured-dimensions-the-entities-to-which-events-occur-to",
    "href": "dw_table_types.html#facts-represents-events-that-occured-dimensions-the-entities-to-which-events-occur-to",
    "title": "8  Data warehouse modeling (Kimball) is based off of 2 types of tables: Fact and dimensions",
    "section": "",
    "text": "Dimension: Each row in a dimension table represents a business entity that is important to the business. For example, An car parts seller’s data warehouse will have a customer dimension table, where each row will represent an individual customer. Other examples of dimension tables in a car parts seller’s data warehouse would be supplier & part tables.\nFacts: Each row in a fact table represents a business process that occurred. E.g., In our data warehouse, each row in the orders fact table will represent an individual order, and each row in the lineitem fact table will represent an item sold as part of an order. Each fact row will have a unique identifier; in our case, it’s orderkey for orders and a combination of orderkey & linenumber for lineitem.\n\n\n\n\n\n\n\n\n\n8.1.1 Popular dimension types: Full Snapshot & SCD2\n\nFull snapshot In this type of dimension, the entire dimension table is re-loaded each run. As the dimension tables are much smaller than the fact table this is usually an acceptable tradeoff. Typically each run would create a new copy while retaining older copy for a certain time period (say 6 months).\nSCD2 SCD2 stands for slowly changing dimension type 2. Any change to a column value will be tracked as a new row.\n\nIf your customer makes an address change in SCD2 it will be created as a new table. SCD2 has 3 key columns that allow us to see historical changes\n\nvalid_from\nvalid_to\nis_current\n\nadd: image showing snapshot dimension and SCD2 dimension model",
    "crumbs": [
      "Data modeling is the process of getting data ready for analyticsUse SQL to transform data",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Data warehouse modeling (Kimball) is based off of 2 types of tables: Fact and dimensions</span>"
    ]
  },
  {
    "objectID": "dw_table_types.html#one-big-table-obt-is-a-fact-table-left-joined-with-all-its-dimensions",
    "href": "dw_table_types.html#one-big-table-obt-is-a-fact-table-left-joined-with-all-its-dimensions",
    "title": "8  Data warehouse modeling (Kimball) is based off of 2 types of tables: Fact and dimensions",
    "section": "8.2 One Big Table (OBT) is a fact table left joined with all its dimensions",
    "text": "8.2 One Big Table (OBT) is a fact table left joined with all its dimensions\nAs the number of facts and dimensions grow you will notice that most of the queries that are run to get data for the end users use the same tables and the same joins.\nIn this scenario the expensive reprocessing of data can be avoided by creating an OBT. In an OBT you left join all the dimensions into a fact table. This big table can then be used to aggregate to different grains as needed for end user reproting.\nNote that the OBT should have the same grain as the fact table that it is based on or have the lower grain if you have to join multiple fact tables.\nIn our bike-part seller warehouse we can create an OBT by joining all the tables to the lineitem table\nadd: code",
    "crumbs": [
      "Data modeling is the process of getting data ready for analyticsUse SQL to transform data",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Data warehouse modeling (Kimball) is based off of 2 types of tables: Fact and dimensions</span>"
    ]
  },
  {
    "objectID": "dw_table_types.html#summary-or-pre-aggregated-tables-are-stakeholder-team-specific-tables-built-for-reporting",
    "href": "dw_table_types.html#summary-or-pre-aggregated-tables-are-stakeholder-team-specific-tables-built-for-reporting",
    "title": "8  Data warehouse modeling (Kimball) is based off of 2 types of tables: Fact and dimensions",
    "section": "8.3 Summary or pre-aggregated tables are stakeholder-team specific tables built for reporting",
    "text": "8.3 Summary or pre-aggregated tables are stakeholder-team specific tables built for reporting\nStakeholders often require data aggregated at various grains and similar metrics. Creating pre-aggregated or summary tables is creating these report for stakeholders so all they would have to do is select from the table without the need to recompute metrics. This has 2 benefits\n\nSame metric formula, as the data engineering will keep the metric definition in the code base, vs each stakeholder using a slightly different version and ending up with different numbers\nAvoid unnecessary recomputation as multiple stakeholders can now use the same table\n\nHowever the down side is that the data may not be as fresh as what a stakeholder would get if they just write a query.",
    "crumbs": [
      "Data modeling is the process of getting data ready for analyticsUse SQL to transform data",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Data warehouse modeling (Kimball) is based off of 2 types of tables: Fact and dimensions</span>"
    ]
  },
  {
    "objectID": "dw_table_types.html#exercises",
    "href": "dw_table_types.html#exercises",
    "title": "8  Data warehouse modeling (Kimball) is based off of 2 types of tables: Fact and dimensions",
    "section": "8.4 Exercises",
    "text": "8.4 Exercises\n\nWhat are the fact tables in our TPCH data model?\nWhat source tables in TPCH data model would you consider to create a customer dimension table?",
    "crumbs": [
      "Data modeling is the process of getting data ready for analyticsUse SQL to transform data",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Data warehouse modeling (Kimball) is based off of 2 types of tables: Fact and dimensions</span>"
    ]
  },
  {
    "objectID": "dw_table_types.html#recommended-reading",
    "href": "dw_table_types.html#recommended-reading",
    "title": "8  Data warehouse modeling (Kimball) is based off of 2 types of tables: Fact and dimensions",
    "section": "8.5 Recommended reading",
    "text": "8.5 Recommended reading\n\nhttps://www.startdataengineering.com/post/metrics_sot/\nhttps://www.startdataengineering.com/post/n-steps-avoid-messy-dw/\nhttps://www.startdataengineering.com/post/data-lake-warehouse-diff/\nhttps://www.startdataengineering.com/post/what-is-a-data-warehouse/",
    "crumbs": [
      "Data modeling is the process of getting data ready for analyticsUse SQL to transform data",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Data warehouse modeling (Kimball) is based off of 2 types of tables: Fact and dimensions</span>"
    ]
  },
  {
    "objectID": "dw_data_flow.html",
    "href": "dw_data_flow.html",
    "title": "9  Most companies use the multi-hop architecture",
    "section": "",
    "text": "Following an established way of processing data accounts for handling common potential issues, and you have plenty of resources to reference. Most industry-standard patterns follow a 3-hop (or layered) architecture. They are\n\nRaw layer stores data from upstream sources as is. This layer sometimes involves changing data types and standardizing column names.\nTransformed layer: Data from the raw layer is transformed based on a chosen modeling principle to form a set of tables. Common modeling principles are Dimensional modeling (Kimball), Data Vault model, entity-relationship data model, etc.\nConsumption layer: Data from the transformed layer are combined to form datasets that directly map to the end-user use case. The consumption layer typically involves joining and aggregating transformed layer tables to enable end-users to analyze historical performance. Business-specific metrics are often defined at this layer. We should ensure that a metric is only defined in one place.\nInterface layer [Optional]: Data in the consumption layer often confirms the data warehouse naming/data types, etc. However, the data presented to the end user should be easy to use and understand. An interface layer is often a view that acts as an interface between the warehouse table and the consumers of this table.\n\nMost frameworks/tools propose their version of the 3-hop architecture: 1. Apache Spark: Medallion architecture 2. dbt: Project Structure.\nShown below is our project’s 3-hop architecture:\n\n\n\nData architecture\n\n\nThe bronze, silver, gold, and interface layers correspond to the abovementioned raw, transformed, consumption, and interface layers. We have used dimensional modeling (with SCD2 for dim_customers) for our silver layer.\nFor a pipeline/transformation function/table, inputs are called upstream, and output users are called downstream consumers.\nAt larger companies, multiple teams work on different layers. A data ingestion team may bring the data into the bronze layer, and other teams may build their own silver and gold tables as necessary.",
    "crumbs": [
      "Data modeling is the process of getting data ready for analyticsUse SQL to transform data",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Most companies use the multi-hop architecture</span>"
    ]
  },
  {
    "objectID": "working_in_a_team.html",
    "href": "working_in_a_team.html",
    "title": "Working in a team",
    "section": "",
    "text": "Data engineering has multiple components and its crucial that you have the same environment that runs in production is also aavailable to the dta engineers to test and develop in.\nIn this section we will see how docker is used to simulate the same running environment across multiple systems.",
    "crumbs": [
      "Working in a team"
    ]
  },
  {
    "objectID": "docker.html",
    "href": "docker.html",
    "title": "10  Docker recreates the same environment for your code in any machine",
    "section": "",
    "text": "10.1 Docker image is a blueprint for your container\nYou can think of docker as running a separate OS (not exactly, but close enough) on the machine. What Docker provides is the ability to replicate the OS and its packages (e.g., Python modules) across machines so that you don’t run into “hey, that worked on my computer” type issues.\nHere is an overview of the concepts needed to understand docker for data infra:\nWindows users: please setup WSL and a local Ubuntu Virtual machine following the instructions here. Install the above prerequisites on your Ubuntu terminal; if you have trouble installing docker, follow the steps here (only Step 1 is necessary).\nWe used docker to run Spark and access it via Jupyter Notebooks.\nAn image is a blueprint to create your docker container. You can define the modules to install, variables to set, etc. Let’s consider our example:\nadd: screenshot of https://github.com/databricks/docker-spark-iceberg/blob/main/spark/Dockerfile\nThe commands in the docker image are run in order. Let’s go over the key commands:\nNote that this code is on github and maintained by the Iceverg/Spark community. This repo is used to build and host official spark-iceberg images on docker hub.\nA docker hub is an online repository where one can store and retrieve docker images and it is where most official systems docker images are stored. https://hub.docker.com/",
    "crumbs": [
      "Working in a team",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Docker recreates the same environment for your code in any machine</span>"
    ]
  },
  {
    "objectID": "docker.html#docker-image-is-a-blueprint-for-your-container",
    "href": "docker.html#docker-image-is-a-blueprint-for-your-container",
    "title": "10  Docker recreates the same environment for your code in any machine",
    "section": "",
    "text": "Docker image\n\n\n\n\n\nFROM: We need a base operating system on which to set our configurations. We can also use existing Docker images available at the Docker Hub and add our config on top of them. In our example, we use the official Python Docker image.\nCOPY: Copy is used to copy files or folders from our local filesystem to the image. The copy command is usually used when building the docker image to copy settings, static files, etc. In our example, spark configs, entrypoint.sh, and iceberg yaml settings.\nENV: This command sets the image’s environment variables. In our example, we set Python, and Spark Paths.\nENTRYPOINT: The entrypoint command executes a script when the image starts. In our example, we use a script file (entrypoint.sh) to start spark master and worker nodes add entrypoint.sh.",
    "crumbs": [
      "Working in a team",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Docker recreates the same environment for your code in any machine</span>"
    ]
  },
  {
    "objectID": "docker.html#start-containers-based-on-docker-image",
    "href": "docker.html#start-containers-based-on-docker-image",
    "title": "10  Docker recreates the same environment for your code in any machine",
    "section": "10.2 Start containers based on docker image",
    "text": "10.2 Start containers based on docker image\nWe use images to create docker containers. We can create one or more containers from a given image.\n\n10.2.1 Communicate between containers and local OS\nTypically, with data infra, we need multiple systems to run simultaneously. Most data systems also expose runtime information, documentation, etc via ports. We have to inform docker which ports to keep open so that they are accessible from the “outside”, in our case your local browser.\nWhen we are developing, we’d want to make changes to the code and see its impact immediately. While you can use COPY to copy over your code when building a docker image, it will not reflect changes in real time and you will have to rebuild your container each time you need change your code.\nIn cases where you want data/code to sync 2 ways between your local machine and the running docker container use mounted volumes. In addition to syncing local files, volumes also sync files between our containers.\n\n\n\ndocker port\n\n\n\n\n10.2.2 Start containers with docker CLI or compose\nWe can use the docker cli to start containers based on an image. Let’s look at an example. To start our Spark master container, we can use the following:\ndocker run -d \\\n  --name spark-master \\\n  --entrypoint ./entrypoint.sh \\\n  -p 4040:4040 \\\n  -p 9090:8080 \\\n  -p 7077:7077 \\\n  --env-file .env.spark \\\n  spark-image master\nHowever, with most data systems, we will need to ensure multiple systems are running. While we can use docker cli to do this, a better option is to use docker compose to orchestrate the different containers required. With docker compose, we can define all our settings in one file and ensure that they are started in the order we prefer.\nOur docker compose is defined here. With our docker compose defined, starting our containers is a simple command, as shown below:\ndocker compose up -d\nThe command will, by default, look for a file called docker-compose.yml in the directory in which it is run.\n\n\n10.2.3 Containers can be always-running or short-lived\nDepending on what you want your containers to do, they can either be short lived (start, run a process, stop) or long lived (start, start spark master and worker nodes, wait for data processing request). Most data infra is long lived.\n\n\n10.2.4 Executing commands in your docker container\nUsing the exec command, you can submit commands to be run in a specific container. For example, we can use the following to open a bash terminal in our spark-master container:\ndocker exec -ti spark-master bash\n# You will be in the master container bash shell\n# try some commands\npwd \nexit # exit the container\nNote that the -ti indicates that this will be run in an interactive mode. As shown below, we can run a command without interactive mode and get an output.\ndocker exec spark-master echo hello\n# prints hello",
    "crumbs": [
      "Working in a team",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Docker recreates the same environment for your code in any machine</span>"
    ]
  },
  {
    "objectID": "data_pipeline.html",
    "href": "data_pipeline.html",
    "title": "Scheduler defines when & Orchestrator defines how to, run your data pipelines",
    "section": "",
    "text": "Schedulers define when to start your data pipeline, e.g. cron, Airflow, etc.\nOrchestrators defines the order in which the tasks of a data pipeline should run. For example extract before transform, complex branching logic, excuting across multiple systems such as spark and snowflake, etc. E.g., dbt-core, Airflow, etc",
    "crumbs": [
      "Scheduler defines when & Orchestrator defines how to, run your data pipelines"
    ]
  },
  {
    "objectID": "airflow.html",
    "href": "airflow.html",
    "title": "11  Airflow is both a scheduler and an orchestrator",
    "section": "",
    "text": "11.1 Schedulers run data pipelines at specified frequency\nLet’s look at the key features essential for most data pipelines and explore how Apache Airflow enables data engineers to do these.\nAirflow by default looks for python files which are under the /dags folder. The location of dags are specified in the airflow.cfg file among other settings. add: cfg link\nData pipelines will need to be run at specific intervals. The frequency required may be as simple as hourly, daily, weekly, monthly, etc., or complex, e.g., 2nd Tuesday of every month.\nAirflow uses a process called Scheduler that checks our DAGs(data pipeline) every minute (add: config setting) to see if it needs to be started.\nIn Airflow you can define when the pipeline should be run with a cron format or python timedelta. add:links\nIn our code, we define our pipeline to run every day(ref link).",
    "crumbs": [
      "Scheduler defines when & Orchestrator defines how to, run your data pipelines",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Airflow is both a scheduler and an orchestrator</span>"
    ]
  },
  {
    "objectID": "airflow.html#schedulers-run-data-pipelines-at-specified-frequency",
    "href": "airflow.html#schedulers-run-data-pipelines-at-specified-frequency",
    "title": "11  Airflow is both a scheduler and an orchestrator",
    "section": "",
    "text": "with DAG(\n    \"user_analytics_dag\",\n    description=\"A DAG to Pull user data and movie review data \\\n        to analyze their behaviour\",\n    schedule_interval=timedelta(days=1),\n    start_date=datetime(2023, 1, 1),\n    catchup=False,\n) as dag:",
    "crumbs": [
      "Scheduler defines when & Orchestrator defines how to, run your data pipelines",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Airflow is both a scheduler and an orchestrator</span>"
    ]
  },
  {
    "objectID": "airflow.html#orchestrators-define-the-order-of-execution-of-your-pipeline-tasks",
    "href": "airflow.html#orchestrators-define-the-order-of-execution-of-your-pipeline-tasks",
    "title": "11  Airflow is both a scheduler and an orchestrator",
    "section": "11.2 Orchestrators define the order of execution of your pipeline tasks",
    "text": "11.2 Orchestrators define the order of execution of your pipeline tasks\nWith complex data pipelines, we want parts of our pipelines to run in a specific order. For example, if your pipeline is pulling data from multiple independent sources, we may want to run them in parallel.\n\n11.2.1 Define the order of execution of pipeline tasks with a DAG\nDAG stands for Directed Acyclic Graph, i.e a series of steps that flow one way with a defined end condition. DAG is also Airflow code API that you can use to define your pipeline.\n\n\n\nDAG\n\n\nIn data pipelines, we use the following terminology: 1. DAG: Represents an entire data pipeline. 2. Task: Individual node in a DAG. The tasks usually correspond to some data task. 3. Dependency: The edges between nodes represent the dependency between tasks. 1. Upstream: All the tasks that run before the task under consideration. 2. Downstream: All the tasks that run after the task under consideration.\nIn our example, if we consider the movie_classifier task, we can see its upstream and downstream tasks as shown below.\n\n\n\nDAG Dependency\n\n\nWith DAG, we can define dependencies, i.e., we can define when a task runs depending on upstream tasks (ref: trigger rules)\nWe can also set individual task-level settings, such as the number of retries per task and branch logic, where you can define logic to choose one or more tasks out of multiple tasks. We can dynamically create tasks based on your logic.\nIn our code we define our DAG using the &gt;&gt; syntax (ref link).\n# Define the tasks \ncreate_s3_bucket &gt;&gt; [user_purchase_to_s3, movie_review_to_s3]\n\nuser_purchase_to_s3 &gt;&gt; get_user_purchase_to_warehouse\n\nmovie_review_to_s3 &gt;&gt; movie_classifier &gt;&gt; get_movie_review_to_warehouse\n\n(\n    [get_user_purchase_to_warehouse, get_movie_review_to_warehouse]\n    &gt;&gt; get_user_behaviour_metric\n    &gt;&gt; gen_dashboard\n)\n\n\n11.2.2 Define where to run your code\nWhen we run our DAG, each task will be run individually. Airflow enables us to run our tasks in multiple ways:\n\nRun code in the same machine as your scheduler process with Local and sequential executor\nRun code in a task queue (i.e. a system that will run tasks in individual machines) with a celery executor.\nRun code as k8s pods with Kubernetes executor.\nWrite custom logic to run your tasks.\n\nSee this link for more details about running your tasks. In our project, we use the default LocalExecutor, which is set up by default.\n\n\n11.2.3 Use operators to connect to popular services\nAs we saw in the Python section, data pipelines involve interacting with external system, such as a cloud storage system (S3), data warehouse (Snowflake), or data processing system (Apache Spark).\nWhile we can write code to work with these external systems, Apache Airflow provides a robust set of Operators for most services we can reuse.\nIn our code, we use multiple Airflow operators to reduce the amount of code we have to write (ref code):\ncreate_s3_bucket = S3CreateBucketOperator(\n  task_id=\"create_s3_bucket\", bucket_name=user_analytics_bucket\n)\nmovie_review_to_s3 = LocalFilesystemToS3Operator(\n  task_id=\"movie_review_to_s3\",\n  filename=\"/opt/airflow/data/movie_review.csv\",\n  dest_key=\"raw/movie_review.csv\",\n  dest_bucket=user_analytics_bucket,\n  replace=True,\n)\n\nuser_purchase_to_s3 = SqlToS3Operator(\n  task_id=\"user_purchase_to_s3\",\n  sql_conn_id=\"postgres_default\",\n  query=\"select * from retail.user_purchase\",\n  s3_bucket=user_analytics_bucket,\n  s3_key=\"raw/user_purchase/user_purchase.csv\",\n  replace=True,\n)\nThe above code shows how we use Airflow operators to create S3 buckets and copy data from local files and a Postgres db into an S3 bucket. See here for a list of Airflow operators.",
    "crumbs": [
      "Scheduler defines when & Orchestrator defines how to, run your data pipelines",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Airflow is both a scheduler and an orchestrator</span>"
    ]
  },
  {
    "objectID": "airflow.html#user-interface-to-see-the-how-your-pipelines-are-running-and-their-history",
    "href": "airflow.html#user-interface-to-see-the-how-your-pipelines-are-running-and-their-history",
    "title": "11  Airflow is both a scheduler and an orchestrator",
    "section": "11.3 User interface to see the how your pipelines are running and their history",
    "text": "11.3 User interface to see the how your pipelines are running and their history\nWhen a data pipeline runs, all information is stored in a metadata db and as logs. The historical information allows us to observe your data pipelines’ current state and historical state.\nHere is a list of the tables used to store metadata about our pipelines:\n\n\n\nMetadata DB\n\n\nApache Airflow enables us to store logs in multiple locations (ref docs). In our project we store them locally in the file system, which can be accessed by clicking on a specific task -&gt; Logs as shown below.\n\n\n\nSpark logs\n\n\n\n11.3.1 See progress & historical information on UI\nWhen we run data pipelines, we can use a nice web UI to see the progress, failures, and other details. Powered by the metadata db and the logs, we can also see individual task logs and the inputs to a specific task, among others.\nThe web UI provides good visibility into our pipelines’ current and historical state.\n\n\n\nDAG logs\n\n\n\n\n\nTask inputs\n\n\n\n\n11.3.2 Analyze data pipeline performance with Web UI\nWe can see how pipelines have performed over time, inspect task run time, and see how long a task had to wait to get started. The performance metrics provide us with the necessary insights to optimize our systems.\n\n\n\nPerformance\n\n\n\n\n11.3.3 Re-run data pipelines via UI\nIn addition to seeing how our pipelines are running, we can manually trigger DAGs with custom inputs as necessary. The ability to trigger/re-run DAGs helps us quickly resolve one-off issues. See this link for information on triggering dags with UI and CLI.\n\n\n\nTriggers\n\n\n\n\n11.3.4 Reuse variables and connections across your pipelines\nApache Airflow also allows us to create and store variables and connection settings that can be reused across our data pipelines. In our code, we create variables using Airflow CLI here.\nVariables and connections are especially important when you want to use a variable across data pipelines and to connect to external systems respectively.\nOnce the connection/variables are set, we can see them in our UI:\n\n\n\nConnection\n\n\nAnd use them in your DAG code as such\nadd: code",
    "crumbs": [
      "Scheduler defines when & Orchestrator defines how to, run your data pipelines",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Airflow is both a scheduler and an orchestrator</span>"
    ]
  },
  {
    "objectID": "dbt.html",
    "href": "dbt.html",
    "title": "12  dbt-core is an orchestrator that makes managing pipelines simpler",
    "section": "",
    "text": "12.1 Create tables with select sql files\nWe saw in the data warehouse (add:link) how data warehouse-ing is the process of getting data ready for analytics. In order to do that we need to create fact, dimensions, OBT and pre-aggregated tables.\nBuilding data pipelines to setup and manage these pipelines involve a lot of code especially for functions that are common across pipelines such as data testing, figuring out how to structure the code, etc. This is where dbt-core comes in.\ndbt-core is a python library that enables you to build complex data pipelines with only SQL queries. By using a special ref function and placing SQL select queries within the appropriate file you can get comples data pipeline graphs, and ability to run pipelines (fully or partially) using the dbt-core library.\nIn addition dbt-core also enables best practices like\ndbt-core assumes that the data is already accessible to the DB engine that you run it on, and as such it is mostly used for the T (Transform) part of the ETL (not necessarily in that order) pipeline.\nWe will run dbt inside the airflow container, as shown below\nIn dbt every .sql file has a select statement and is created as a data model (usually a table or a view). The select statement defines the data schema of the data model. The name of the .sql file defines the name of the data model.\nLet’s take a look at one of our silver tables (add: link)\nWe can see how the final select query is created as a data model. Note the ref function refers to another table that is defined by the folder path and file name (which is also its data model name).\nThe setting that defines which data models should be tables/views/materialized views, etc will be defined in the dbt_project.yml file (add: link).\nWhen you run the dbt run command all your models will be created as tables.",
    "crumbs": [
      "Scheduler defines when & Orchestrator defines how to, run your data pipelines",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>dbt-core is an orchestrator that makes managing pipelines simpler</span>"
    ]
  },
  {
    "objectID": "dbt.html#create-tables-with-select-sql-files",
    "href": "dbt.html#create-tables-with-select-sql-files",
    "title": "12  dbt-core is an orchestrator that makes managing pipelines simpler",
    "section": "",
    "text": "add: silver table code",
    "crumbs": [
      "Scheduler defines when & Orchestrator defines how to, run your data pipelines",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>dbt-core is an orchestrator that makes managing pipelines simpler</span>"
    ]
  },
  {
    "objectID": "dbt.html#document-test-tables-with-yml-files",
    "href": "dbt.html#document-test-tables-with-yml-files",
    "title": "12  dbt-core is an orchestrator that makes managing pipelines simpler",
    "section": "12.2 Document & test tables with yml files",
    "text": "12.2 Document & test tables with yml files\nYou can also document what the table and the columns of your tables mean in yml files. These yml files have to be within the same folder and also reference the data model’s name and the column names.\nIn addition to descriptions you can also specify any tests to be run on the columns as needed.\nThe documentation will be rendered when you run the dbt render command and HTML files will be created which we will view with a dbt serve command in a later section.\nThe tests can be run with the dbt test command, note that the tests can only be run after the data is available, so it is not entirely suitable for the WAP pattern.\nadd: core.yml\nRun the tests with dbt test command.",
    "crumbs": [
      "Scheduler defines when & Orchestrator defines how to, run your data pipelines",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>dbt-core is an orchestrator that makes managing pipelines simpler</span>"
    ]
  },
  {
    "objectID": "dbt.html#define-db-connections-in-profiles.yml",
    "href": "dbt.html#define-db-connections-in-profiles.yml",
    "title": "12  dbt-core is an orchestrator that makes managing pipelines simpler",
    "section": "12.3 Define db connections in profiles.yml",
    "text": "12.3 Define db connections in profiles.yml\ndbt uses a yml file to define how it connects to your db engine. Let’s look at our example\nadd: profiles.yml\nWe tell dbt to connect to Apache Spark. The target variable defines the environment. The default is dev, but you can specify which environment to run on with --target flag in the dbt run command.\nBy default dbt will look for a profiles.yml in your HOME directory. We can tell dbt to look for the profiles.yml file in a specific folder using the --profiles-dir flag as shown below.\ndbt run --profiles-dir .",
    "crumbs": [
      "Scheduler defines when & Orchestrator defines how to, run your data pipelines",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>dbt-core is an orchestrator that makes managing pipelines simpler</span>"
    ]
  },
  {
    "objectID": "dbt.html#define-your-project-setting-at-dbt_project.yml",
    "href": "dbt.html#define-your-project-setting-at-dbt_project.yml",
    "title": "12  dbt-core is an orchestrator that makes managing pipelines simpler",
    "section": "12.4 Define your project setting at dbt_project.yml",
    "text": "12.4 Define your project setting at dbt_project.yml\nIn dbt_project.yml file you specify all the project specific settings that you want applied, such as\n\nThe folders to look for the .sql files\nThe folder to look for seeds, downloaded packages, SQL functions (aka macros), etc.\nHow to materialize a data model (ie. should a data model be created as a table/view/materialized view/temporal table, etc)\n\nMaterialization is a variable that controls how dbt creates a model. By default, every model will be a view. This can be overridden in dbt_project.yml. We have set the models under models/marts/core/ to materialize as tables.\n# Configuring models\nmodels:\n    sde_dbt_tutorial:\n        # Applies to all files under models/marts/core/\n        marts:\n            core:\n                materialized: table\nIf you need to define how your dbt project",
    "crumbs": [
      "Scheduler defines when & Orchestrator defines how to, run your data pipelines",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>dbt-core is an orchestrator that makes managing pipelines simpler</span>"
    ]
  },
  {
    "objectID": "dbt.html#dbt-recommends-the-3-hop-architecture-with-stage-core-data-marts",
    "href": "dbt.html#dbt-recommends-the-3-hop-architecture-with-stage-core-data-marts",
    "title": "12  dbt-core is an orchestrator that makes managing pipelines simpler",
    "section": "12.5 dbt recommends the 3-hop architecture with stage, core & data marts",
    "text": "12.5 dbt recommends the 3-hop architecture with stage, core & data marts\nWe will see how the customer_orders table is created from the source tables. These transformations follow warehouse and dbt best practices.\n\n12.5.1 Source\nSource tables refer to tables loaded into the warehouse by an EL process. In our case these are the base tpch tables, which are created by the extract step.\nWe need to define what tables are the sources in the src.yml file, this will be used by the stage tables with source function.\nsource add:\nadd: usage with source function\n\n\n12.5.2 Staging\nThe staging area is where raw data is cast into correct data types, given consistent column names, and prepared to be transformed into models used by end-users.\nYou can think of this stage as the first layer of transformations. We will place staging data models inside the staging folder, as shown below.\nadd: folder path\nTheir documentation and tests will be defined in a yaml file, as shown below.\nadd: staging yaml\n\n\n12.5.3 Marts\nMarts consist of the core tables for end-users and business vertical-specific tables.\n\n12.5.3.1 Core\nThe core defines the fact and dimension models to be used by end-users. We define our facts and tables under the marts/core folder. add: folder path.\nYou can see that we store the facts, dimensions and OBT under this folder.\n\n\n12.5.3.2 Stakeholder team specific\nIn this section, we define the models for marketing stakeholders, A project can have multiple business verticals. Having one folder per business vertical provides an easy way to organize the models.\nIn our example we store the metrics.sql in this location.",
    "crumbs": [
      "Scheduler defines when & Orchestrator defines how to, run your data pipelines",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>dbt-core is an orchestrator that makes managing pipelines simpler</span>"
    ]
  },
  {
    "objectID": "dbt.html#dbt-core-is-a-cli",
    "href": "dbt.html#dbt-core-is-a-cli",
    "title": "12  dbt-core is an orchestrator that makes managing pipelines simpler",
    "section": "12.6 dbt-core is a cli",
    "text": "12.6 dbt-core is a cli\nWith all our data model defined, we can use the dbt cli to run, test and create documentation. dbt command will look for the profiles.yml file in your $HOME directory by default so we either have to set the PROFILES_DIR environment variable (add: docekr file command) or use the --profiles-dir as part of the cli command.\n\n12.6.1 dbt run\nWe have the necessary model definitions in place. Let’s create the models.\ndbt run \n# Finished running 5 view models, 2 table models, 2 hooks in 0 hours 0 minutes and 3.22 seconds (3.22s).\nOur staging and marketing models are as materialized views, and the two core models are materialized as views as defined in ourlized as views as defined in our dbt_project.yml.\n\n\n12.6.2 dbt test\nWith the models defined, we can run tests on them. Note that, unlike standard testing, these tests run after the data has been processed. You can run tests as shown below.\ndbt test # or run \"just test\"\n# Finished running 14 tests...\n\n\n12.6.3 dbt docs\nOne of the powerful features of dbt is its docs. To generate documentation and serve them, run the following commands:\ndbt docs generate\ndbt docs serve\nThe generate command will create documentation in html format. The serve command will start a webserver that serves this html file.\nNavigate to customer_orders within the sde_dbt_tutorial project in the left pane. Click on the view lineage graph icon on the lower right side. The lineage graph shows the dependencies of a model. You can also see the tests defined, descriptions (set in the corresponding YAML file), and the compiled sql statements.\n\n\n\nour project structure",
    "crumbs": [
      "Scheduler defines when & Orchestrator defines how to, run your data pipelines",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>dbt-core is an orchestrator that makes managing pipelines simpler</span>"
    ]
  },
  {
    "objectID": "dbt.html#scheduling",
    "href": "dbt.html#scheduling",
    "title": "12  dbt-core is an orchestrator that makes managing pipelines simpler",
    "section": "12.7 Scheduling",
    "text": "12.7 Scheduling\nWe have seen how to create snapshots, models, run tests and generate documentation. These are all commands run via the cli. Dbt compiles the models into sql queries under the target folder (not part of git repo) and executes them on the data warehouse.\nTo schedule dbt runs, snapshots, and tests we need to use a scheduler. In the final capstone project we will use Airflow to schedule this dbt pipeline.",
    "crumbs": [
      "Scheduler defines when & Orchestrator defines how to, run your data pipelines",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>dbt-core is an orchestrator that makes managing pipelines simpler</span>"
    ]
  },
  {
    "objectID": "capstone_project.html",
    "href": "capstone_project.html",
    "title": "13  Capstone Project: Spark, dbt, Airflow with Docker",
    "section": "",
    "text": "13.1 Presentation matters\nIn the previous chapters we learn’t how to manipulate data with Spark SQL, how to create pipeline transformations with dbt and how to schedule & orchestrate them with Airflow.\nIn this capstone project we will put them together to create an end-to-end project.\nThe main objectives for this capstone project are 1. Understanding how the different components common in data engineering work with each other 2. How to model and transform data in the 3-hop architecture 3. Clearly explain what your pipeline is doing and why and how\nLet’s assume that we are working on modeling the TPCH data and creating a data mart for sales team to create customer metrics which they can use to strategize how to cold call customers.\nWhen a hiring manager looks at your project, assume that they will not read over the code. Typically when people look at projects they browse high level sections, these includes\nWe will see how you can address these",
    "crumbs": [
      "Scheduler defines when & Orchestrator defines how to, run your data pipelines",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Capstone Project: Spark, dbt, Airflow with Docker</span>"
    ]
  },
  {
    "objectID": "capstone_project.html#presentation-matters",
    "href": "capstone_project.html#presentation-matters",
    "title": "13  Capstone Project: Spark, dbt, Airflow with Docker",
    "section": "",
    "text": "Outcome of your project\nHigh level architecture\nProject structure to understand how your code works\nBrowse code for clarity and code-cleanliness",
    "crumbs": [
      "Scheduler defines when & Orchestrator defines how to, run your data pipelines",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Capstone Project: Spark, dbt, Airflow with Docker</span>"
    ]
  },
  {
    "objectID": "capstone_project.html#start-with-the-outcome",
    "href": "capstone_project.html#start-with-the-outcome",
    "title": "13  Capstone Project: Spark, dbt, Airflow with Docker",
    "section": "13.2 Start with the outcome",
    "text": "13.2 Start with the outcome\nWe are creating data for the sales team to enable customer outreach and for this we need to present customers who will most likely convert. While this is a complex data science question, a simple approach could be to target customers who have the highest average order value (assuming high/low order values are outliers).\nCreate a dashboard to show the top 10 customers by average order values as a descending bar chart.\nadd: gif\nadd: image of bar chart",
    "crumbs": [
      "Scheduler defines when & Orchestrator defines how to, run your data pipelines",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Capstone Project: Spark, dbt, Airflow with Docker</span>"
    ]
  },
  {
    "objectID": "capstone_project.html#high-level-architecture",
    "href": "capstone_project.html#high-level-architecture",
    "title": "13  Capstone Project: Spark, dbt, Airflow with Docker",
    "section": "13.3 High level architecture",
    "text": "13.3 High level architecture\nThe objective of this is to show your expertise in\n\nDesigning data pipelines, by following industry standard 3-hop architecture\nIndustry standard tools like dbt, Airflow and Spark\nWriting clean code using auto formatters and linters\n\nOur base repo comes with all of these setup and installed for you to copy over and use.\nadd: 3-hop dbt folder structure add: auto formatter and linter for python and sql",
    "crumbs": [
      "Scheduler defines when & Orchestrator defines how to, run your data pipelines",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Capstone Project: Spark, dbt, Airflow with Docker</span>"
    ]
  },
  {
    "objectID": "capstone_project.html#code-explanation",
    "href": "capstone_project.html#code-explanation",
    "title": "13  Capstone Project: Spark, dbt, Airflow with Docker",
    "section": "13.4 Code explanation",
    "text": "13.4 Code explanation\nWe have code that does the following\n\nUse dbt to define Spark SQL pipelines following the 3-hop architecture that we saw in the dbt chapter. Add: folder structure screenshot\nUse Airflow to load data into the warehouse and run dbt pipeline on a schedule and be able to monitor it add; dag screenshot\nFollow Kimball dimensional modeling for our dbt pipeline, add: 3-hop arch",
    "crumbs": [
      "Scheduler defines when & Orchestrator defines how to, run your data pipelines",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Capstone Project: Spark, dbt, Airflow with Docker</span>"
    ]
  },
  {
    "objectID": "capstone_project.html#exercise",
    "href": "capstone_project.html#exercise",
    "title": "13  Capstone Project: Spark, dbt, Airflow with Docker",
    "section": "13.5 Exercise",
    "text": "13.5 Exercise\n\nFind a dataset that you are interested in, showing an interesting take for the data. Outcome should be shown with data.\n\nYou can copy paste the airflow folder into a separate folder and make the necessary changes. You can add requirements to the requirements.txt file\nadd: code wit unix commands",
    "crumbs": [
      "Scheduler defines when & Orchestrator defines how to, run your data pipelines",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Capstone Project: Spark, dbt, Airflow with Docker</span>"
    ]
  },
  {
    "objectID": "capstone_project.html#read-this",
    "href": "capstone_project.html#read-this",
    "title": "13  Capstone Project: Spark, dbt, Airflow with Docker",
    "section": "13.6 Read this",
    "text": "13.6 Read this\n\nApproach on identifying problem space and datasets https://www.startdataengineering.com/post/what-data-project-to-build/#23-data\nhttps://www.startdataengineering.com/post/data-engineering-project-to-impress-hiring-managers/",
    "crumbs": [
      "Scheduler defines when & Orchestrator defines how to, run your data pipelines",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Capstone Project: Spark, dbt, Airflow with Docker</span>"
    ]
  },
  {
    "objectID": "index.html#how-to-use-this-book",
    "href": "index.html#how-to-use-this-book",
    "title": "Data Engineering For Beginners",
    "section": "How to use this book",
    "text": "How to use this book\nThis book is written in order to guide you from not knowing much about data engineering to being proficient in the core ideas that underpins modern data engineering.\nI recommend you read the book in order and follow along with the code examples. Each chapter has exercises, the solutions for which you will receive via emails (sign up here add:link).",
    "crumbs": [
      "Start here"
    ]
  },
  {
    "objectID": "index.html#setup",
    "href": "index.html#setup",
    "title": "Data Engineering For Beginners",
    "section": "Setup",
    "text": "Setup\nFollow the steps here for setup.\nIn order to follow along with code, please make a copy of this starter notebook and follow along.",
    "crumbs": [
      "Start here"
    ]
  },
  {
    "objectID": "index.html#data",
    "href": "index.html#data",
    "title": "Data Engineering For Beginners",
    "section": "Data",
    "text": "Data\nWe will use the TPCH dataset for exercises and example throughout this book. he TPC-H data represents a car parts seller’s data warehouse, where we record orders, items that make up that order (lineitem), supplier, customer, part (parts sold), region, nation, and partsupp (parts supplier).\nNote: Have a copy of the data model as you follow along; this will help in understanding the examples provided and in answering exercise questions.",
    "crumbs": [
      "Start here"
    ]
  },
  {
    "objectID": "sql_basics.html#sub-query-use-a-query-instead-of-a-table",
    "href": "sql_basics.html#sub-query-use-a-query-instead-of-a-table",
    "title": "1  Read data, Combine tables, & aggregate numbers to understand business performance",
    "section": "1.8 Sub-query: Use a query instead of a table",
    "text": "1.8 Sub-query: Use a query instead of a table\nWhen we want to use the result of a query as a table in another query, we use subqueries. Let’s consider an example:\n\nCreate a report that shows the nation, how many items it supplied (by suppliers in that nation), and how many items it purchased (by customers in that nation).\n\n\n%%sql\nSELECT\n  n.n_name AS nation_c_name,\n  s.quantity AS supplied_items_quantity,\n  c.quantity AS purchased_items_quantity\nFROM\n  nation n\n  LEFT JOIN (\n    SELECT\n      n.n_nationkey,\n      SUM(l.l_quantity) AS quantity\n    FROM\n      lineitem l\n      JOIN supplier s ON l.l_suppkey = s.s_suppkey\n      JOIN nation n ON s.s_nationkey = n.n_nationkey\n    GROUP BY\n      n.n_nationkey\n  ) s ON n.n_nationkey = s.n_nationkey\n  LEFT JOIN (\n    SELECT\n      n.n_nationkey,\n      SUM(l.l_quantity) AS quantity\n    FROM\n      lineitem l\n      JOIN orders o ON l.l_orderkey = o.o_orderkey\n      JOIN customer c ON o.o_custkey = c.c_custkey\n      JOIN nation n ON c.c_nationkey = n.n_nationkey\n    GROUP BY\n      n.n_nationkey\n  ) c ON n.n_nationkey = c.n_nationkey;\n\nIn the above query, we can see that there are two sub-queries, one to calculate the quantity supplied by a nation and the other to calculate the quantity purchased by the customers of a nation.",
    "crumbs": [
      "Use SQL to transform data",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Read data, Combine tables, & aggregate numbers to understand business performance</span>"
    ]
  }
]