{
  "hash": "8953d7ecdd71a7f75d9fe16cc5e0fab8",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: Python has libraries to tell the data processing engine (Spark, Trino, Duckdb, Polars, etc) what to do\nformat:\n  html:\n    toc: true\nexecute:\n  eval: false\n  output: true\njupyter: python3\n---\n\n\nAlmost every data processing systems has a Python library to interact with it. \n\nPyspark\nTrino Python \nSnowflake Python\nDuckdb Python \nPolars Python API\nadd: link\n\nThe main types of data processing libraries are:\n\n1. **`Python standard libraries`**: Python has a strong set of standard libraries for processing data. When using standard libraries, you'll usually be working on Python native data structures (add: link). Some popular ones are [csv](https://docs.python.org/3/library/csv.html), [json](https://docs.python.org/3/library/json.html), [gzip](https://docs.python.org/3/library/gzip.html) etc.\n2. **`Dataframe libraries`**: Libraries like pandas, polars, and Spark enable you to work with tabular data. These libraries use a dataframe (Python's version of a SQL table) and enable you to do everything (and more) you can do with SQL. **Note** that some of these libraries are meant for data that can be processed in memory.\n3. **`Processing data on SQL via Python`**: You can use [database drivers]({{< ref \"/post/python-for-de.md#extract--load-read-and-write-data-to-any-system\" >}}) and write SQL queries to process the data. The benefit of this approach is that you don't need to bring data into Python memory and offload it to the database.\n\n**Note**: When we use systems like `Spark`, `Dask`, `Snowflake`, `BigQuery` to process data, you should note that we interact with them via Python. Data is processed by the external systems.\n\n::: {#2bdf9753 .cell execution_count=1}\n``` {.python .cell-code}\nprint(\n    \"################################################################################\"\n)\nprint(\"Use standard python libraries to do the transformations\")\nprint(\n    \"################################################################################\"\n)\n```\n:::\n\n\n::: {#22111337 .cell execution_count=2}\n``` {.python .cell-code}\nimport csv\n\ndata = []\nwith open(\"./sample_data.csv\", \"r\", newline=\"\") as csvfile:\n    reader = csv.DictReader(csvfile)\n    for row in reader:\n        data.append(row)\nprint(data[:2])\n```\n:::\n\n\n::: {#6e8ca9d9 .cell execution_count=3}\n``` {.python .cell-code}\ndata_unique = []\ncustomer_ids_seen = set()\nfor row in data:\n    if row[\"Customer_ID\"] not in customer_ids_seen:\n        data_unique.append(row)\n        customer_ids_seen.add(row[\"Customer_ID\"])\n    else:\n        print(f'duplicate customer id {row[\"Customer_ID\"]}')\n```\n:::\n\n\n::: {#4dcc94b3 .cell execution_count=4}\n``` {.python .cell-code}\nfor row in data_unique:\n    if not row[\"Age\"]:\n        print(f'Customer {row[\"Customer_Name\"]} does not have Age value')\n        row[\"Age\"] = 0\n    if not row[\"Purchase_Amount\"]:\n        row[\"Purchase_Amount\"] = 0.0\n\ndata_cleaned = [\n    row\n    for row in data_unique\n    if int(row[\"Age\"]) <= 100 and float(row[\"Purchase_Amount\"]) <= 1000\n]\n\nfor row in data_cleaned:\n    if row[\"Gender\"] == \"Female\":\n        row[\"Gender\"] = 0\n    elif row[\"Gender\"] == \"Male\":\n        row[\"Gender\"] = 1\n\nfor row in data_cleaned:\n    first_name, last_name = row[\"Customer_Name\"].split(\" \", 1)\n    row[\"First_Name\"] = first_name\n    row[\"Last_Name\"] = last_name\n\nprint(data_cleaned[:3])\n```\n:::\n\n\n::: {#29f92386 .cell execution_count=5}\n``` {.python .cell-code}\nfrom collections import defaultdict\ntotal_purchase_by_gender = defaultdict(float)\nfor row in data_cleaned:\n    total_purchase_by_gender[row[\"Gender\"]] += float(row[\"Purchase_Amount\"])\n\nage_groups = {\"18-30\": [], \"31-40\": [], \"41-50\": [], \"51-60\": [], \"61-70\": []}\nfor row in data_cleaned:\n    age = int(row[\"Age\"])\n    if age <= 30:\n        age_groups[\"18-30\"].append(float(row[\"Purchase_Amount\"]))\n    elif age <= 40:\n        age_groups[\"31-40\"].append(float(row[\"Purchase_Amount\"]))\n    elif age <= 50:\n        age_groups[\"41-50\"].append(float(row[\"Purchase_Amount\"]))\n    elif age <= 60:\n        age_groups[\"51-60\"].append(float(row[\"Purchase_Amount\"]))\n    else:\n        age_groups[\"61-70\"].append(float(row[\"Purchase_Amount\"]))\n\naverage_purchase_by_age_group = {\n    group: sum(amounts) / len(amounts) for group, amounts in age_groups.items()\n}\n```\n:::\n\n\n::: {#26f21fe0 .cell execution_count=6}\n``` {.python .cell-code}\nprint(\"Total purchase amount by Gender:\", total_purchase_by_gender)\nprint(\"Average purchase amount by Age group:\", average_purchase_by_age_group)\n```\n:::\n\n\n::: {#e548d80b .cell execution_count=7}\n``` {.python .cell-code}\nspark\n```\n:::\n\n\n::: {#ee40b835 .cell execution_count=8}\n``` {.python .cell-code}\nprint(\n    \"################################################################################\"\n)\nprint(\"Use PySpark DataFrame API to do the transformations\")\nprint(\n    \"################################################################################\"\n)\n```\n:::\n\n\n::: {#2555d212 .cell execution_count=9}\n``` {.python .cell-code}\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import col, coalesce, lit, when, split, sum as spark_sum, avg, regexp_replace\nfrom pyspark.sql.types import StructType, StructField, IntegerType, StringType, FloatType, DateType\n\nschema = StructType([\n    StructField(\"Customer_ID\", IntegerType(), True),\n    StructField(\"Customer_Name\", StringType(), True),\n    StructField(\"Age\", IntegerType(), True),\n    StructField(\"Gender\", StringType(), True),\n    StructField(\"Purchase_Amount\", FloatType(), True),\n    StructField(\"Purchase_Date\", DateType(), True)\n])\n\n# Read data from CSV file into DataFrame\ndata = spark.read \\\n    .option(\"header\", \"true\") \\\n    .option(\"inferSchema\", \"false\") \\\n    .schema(schema) \\\n    .csv(\"./sample_data.csv\")\n\n# Question: How do you remove duplicate rows based on customer ID in PySpark?\ndata_unique = data.dropDuplicates()\n\n# Question: How do you handle missing values by replacing them with 0 in PySpark?\ndata_cleaned_missing = data_unique.select(\n    col(\"Customer_ID\"),\n    col(\"Customer_Name\"),\n    coalesce(col(\"Age\"), lit(0)).alias(\"Age\"),\n    col(\"Gender\"),\n    coalesce(col(\"Purchase_Amount\"), lit(0.0)).alias(\"Purchase_Amount\"),\n    col(\"Purchase_Date\")\n)\n\n# Question: How do you remove outliers (e.g., age > 100 or purchase amount > 1000) in PySpark?\ndata_cleaned_outliers = data_cleaned_missing.filter(\n    (col(\"Age\") <= 100) & (col(\"Purchase_Amount\") <= 1000)\n)\n\n# Question: How do you convert the Gender column to a binary format (0 for Female, 1 for Male) in PySpark?\ndata_cleaned_gender = data_cleaned_outliers.withColumn(\n    \"Gender_Binary\",\n    when(col(\"Gender\") == \"Female\", 0).otherwise(1)\n)\n\n# Question: How do you split the Customer_Name column into separate First_Name and Last_Name columns in PySpark?\ndata_cleaned = data_cleaned_gender.select(\n    col(\"Customer_ID\"),\n    split(col(\"Customer_Name\"), \" \").getItem(0).alias(\"First_Name\"),\n    split(col(\"Customer_Name\"), \" \").getItem(1).alias(\"Last_Name\"),\n    col(\"Age\"),\n    col(\"Gender_Binary\"),\n    col(\"Purchase_Amount\"),\n    col(\"Purchase_Date\")\n)\n```\n:::\n\n\n::: {#c6a3fa48 .cell execution_count=10}\n``` {.python .cell-code}\n# Question: How do you calculate the total purchase amount by Gender in PySpark?\ntotal_purchase_by_gender = data_cleaned_gender.groupBy(\"Gender_Binary\") \\\n    .agg(spark_sum(\"Purchase_Amount\").alias(\"Total_Purchase_Amount\")) \\\n    .collect()\n```\n:::\n\n\n::: {#546998ff .cell execution_count=11}\n``` {.python .cell-code}\n# Question: How do you calculate the average purchase amount by Age group in PySpark?\naverage_purchase_by_age_group = data_cleaned.withColumn(\n    \"Age_Group\",\n    when((col(\"Age\") >= 18) & (col(\"Age\") <= 30), \"18-30\")\n    .when((col(\"Age\") >= 31) & (col(\"Age\") <= 40), \"31-40\")\n    .when((col(\"Age\") >= 41) & (col(\"Age\") <= 50), \"41-50\")\n    .when((col(\"Age\") >= 51) & (col(\"Age\") <= 60), \"51-60\")\n    .otherwise(\"61-70\")\n).groupBy(\"Age_Group\") \\\n    .agg(avg(\"Purchase_Amount\").alias(\"Average_Purchase_Amount\")) \\\n    .collect()\n```\n:::\n\n\n::: {#371db619 .cell execution_count=12}\n``` {.python .cell-code}\n# Question: How do you print the results for total purchase amount by Gender and average purchase amount by Age group in PySpark?\nprint(\"====================== Results ======================\")\nprint(\"Total purchase amount by Gender:\")\nfor row in total_purchase_by_gender:\n    print(f\"Gender_Binary: {row['Gender_Binary']}, Total_Purchase_Amount: {row['Total_Purchase_Amount']}\")\n\nprint(\"Average purchase amount by Age group:\")\nfor row in average_purchase_by_age_group:\n    print(f\"Age_Group: {row['Age_Group']}, Average_Purchase_Amount: {row['Average_Purchase_Amount']}\")\n```\n:::\n\n\n::: {#bf41f0e7 .cell execution_count=13}\n``` {.python .cell-code}\n# Optional: Show DataFrame contents for verification\nprint(\"\\n====================== Data Preview ======================\")\nprint(\"Final cleaned data:\")\ndata_cleaned.show(10)\n```\n:::\n\n\n::: {#a8f0a4fe .cell execution_count=14}\n``` {.python .cell-code}\nprint(\"Total purchase by gender:\")\ndata_cleaned_gender.groupBy(\"Gender_Binary\") \\\n    .agg(spark_sum(\"Purchase_Amount\").alias(\"Total_Purchase_Amount\")) \\\n    .show()\n```\n:::\n\n\n::: {#dc61511f .cell execution_count=15}\n``` {.python .cell-code}\nprint(\"Average purchase by age group:\")\ndata_cleaned.withColumn(\n    \"Age_Group\",\n    when((col(\"Age\") >= 18) & (col(\"Age\") <= 30), \"18-30\")\n    .when((col(\"Age\") >= 31) & (col(\"Age\") <= 40), \"31-40\")\n    .when((col(\"Age\") >= 41) & (col(\"Age\") <= 50), \"41-50\")\n    .when((col(\"Age\") >= 51) & (col(\"Age\") <= 60), \"51-60\")\n    .otherwise(\"61-70\")\n).groupBy(\"Age_Group\") \\\n    .agg(avg(\"Purchase_Amount\").alias(\"Average_Purchase_Amount\")) \\\n    .show()\n```\n:::\n\n\n",
    "supporting": [
      "py_transform_files"
    ],
    "filters": [],
    "includes": {}
  }
}