{
  "hash": "1627045ce6a06eaa7c15da468a31bac8",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: Airflow is both a scheduler and an orchestrator\nformat:\n  html:\n    toc: true\nexecute:\n  eval: false\n  output: true\njupyter: python3\n---\n\n\n\n\nLet's look at the key features essential for most data pipelines and explore how Apache Airflow enables data engineers to do these.\n\nAirflow by default looks for python files which are under the `/dags` folder. The location of dags are specified in the airflow.cfg file among other settings. add: cfg link\n\n## Schedulers run data pipelines at specified frequency\n\nData pipelines will need to be run at specific intervals. The frequency required may be as simple as hourly, daily, weekly, monthly, etc., or complex, e.g., 2nd Tuesday of every month.\n\nAirflow uses a process called **[Scheduler](https://airflow.apache.org/docs/apache-airflow/stable/administration-and-deployment/scheduler.html#dag-file-processing)** that checks our DAGs(data pipeline) every minute (add: config setting) to see if it needs to be started.\n\nIn Airflow you can define when the pipeline should be run with a cron format or python timedelta. add:links\n\n\nIn our code, we define our pipeline to run every day(**[ref link](https://github.com/josephmachado/beginner_de_project/blob/de8073bfa627373b5a19d8d5d6fa6622718b0785/dags/user_analytics.py#L44-L45)**).\n\n```python\nwith DAG(\n    \"user_analytics_dag\",\n    description=\"A DAG to Pull user data and movie review data \\\n        to analyze their behaviour\",\n    schedule_interval=timedelta(days=1),\n    start_date=datetime(2023, 1, 1),\n    catchup=False,\n) as dag:\n```\n\n## Orchestrators define the order of execution of your pipeline tasks\n\nWith complex data pipelines, we want parts of our pipelines to run in a specific order. For example, if your pipeline is pulling data from multiple independent sources, we may want to run them in parallel. \n\n### Define the order of execution of pipeline tasks with a DAG\n\nDAG stands for Directed Acyclic Graph, i.e a series of steps that flow one way with a defined end condition. DAG is also Airflow code API that you can use to define your pipeline.\n\n\n![DAG](/images/why-orc/dag.png)\n\nIn data pipelines, we use the following terminology:\n1. **`DAG`**: Represents an entire data pipeline.\n2. **`Task`**: Individual node in a DAG. The tasks usually correspond to some data task.\n3. **`Dependency`**: The edges between nodes represent the dependency between tasks.\n      1. **`Upstream`**: All the tasks that run before the task under consideration.\n      2. **`Downstream`**: All the tasks that run after the task under consideration.\n\nIn our example, if we consider the `movie_classifier` task, we can see its upstream and downstream tasks as shown below.\n\n![DAG Dependency](/images/why-orc/dagd.png)\n\nWith DAG, we can define dependencies, i.e., we can define when a task runs depending on upstream tasks (ref: **[trigger rules](https://airflow.apache.org/docs/apache-airflow/stable/core-concepts/dags.html#trigger-rules)**)\n\nWe can also set individual task-level settings, such as the number of retries per task and branch logic, where you can define logic to choose one or more tasks out of multiple tasks. We can dynamically create tasks based on your logic.\n\nIn our code we define our DAG using the `>>` syntax (**[ref link](https://github.com/josephmachado/beginner_de_project/blob/de8073bfa627373b5a19d8d5d6fa6622718b0785/dags/user_analytics.py#L126-L136)**).\n\n```python\n# Define the tasks \ncreate_s3_bucket >> [user_purchase_to_s3, movie_review_to_s3]\n\nuser_purchase_to_s3 >> get_user_purchase_to_warehouse\n\nmovie_review_to_s3 >> movie_classifier >> get_movie_review_to_warehouse\n\n(\n    [get_user_purchase_to_warehouse, get_movie_review_to_warehouse]\n    >> get_user_behaviour_metric\n    >> gen_dashboard\n)\n```\n\n### Define where to run your code\n\nWhen we run our DAG, each task will be run individually. Airflow enables us to run our tasks in multiple ways:\n\n1. Run code in the same machine as your scheduler process with Local and sequential executor\n2. Run code in a task queue (i.e. a system that will run tasks in individual machines) with a celery executor.\n3. Run code as k8s pods with Kubernetes executor.\n4. Write custom logic to run your tasks.\n\nSee **[this link](https://airflow.apache.org/docs/apache-airflow/stable/core-concepts/executor/index.html#)** for more details about running your tasks. In our project, we use the default `LocalExecutor,` which is set up by default.\n\n### Use operators to connect to popular services\n\nAs we saw in the Python section, data pipelines involve interacting with external system, such as a cloud storage system (S3), data warehouse (Snowflake), or data processing system (Apache Spark).\n\nWhile we can write code to work with these external systems, Apache Airflow provides a robust set of `Operators` for most services we can reuse.\n\nIn our code, we use multiple Airflow operators to reduce the amount of code we have to write (**[ref code](https://github.com/josephmachado/beginner_de_project/blob/master/dags/user_analytics.py)**):\n\n```python\ncreate_s3_bucket = S3CreateBucketOperator(\n  task_id=\"create_s3_bucket\", bucket_name=user_analytics_bucket\n)\nmovie_review_to_s3 = LocalFilesystemToS3Operator(\n  task_id=\"movie_review_to_s3\",\n  filename=\"/opt/airflow/data/movie_review.csv\",\n  dest_key=\"raw/movie_review.csv\",\n  dest_bucket=user_analytics_bucket,\n  replace=True,\n)\n\nuser_purchase_to_s3 = SqlToS3Operator(\n  task_id=\"user_purchase_to_s3\",\n  sql_conn_id=\"postgres_default\",\n  query=\"select * from retail.user_purchase\",\n  s3_bucket=user_analytics_bucket,\n  s3_key=\"raw/user_purchase/user_purchase.csv\",\n  replace=True,\n)\n```\n\nThe above code shows how we use Airflow operators to create S3 buckets and copy data from local files and a Postgres db into an S3 bucket. See **[here for a list of Airflow operators](https://airflow.apache.org/docs/apache-airflow-providers/operators-and-hooks-ref/index.html)**.\n\n## User interface to see the how your pipelines are running and their history\n\nWhen a data pipeline runs, all information is stored in a metadata db and as logs. The historical information allows us to observe your data pipelines' current state and historical state.\n\nHere is a list of the tables used to store metadata about our pipelines:\n\n![Metadata DB](/images/why-orc/meta.png)\n\nApache Airflow enables us to store logs in multiple locations (**[ref docs](https://airflow.apache.org/docs/apache-airflow/stable/administration-and-deployment/logging-monitoring/logging-architecture.html)**). In our project we store them locally in the file system, which can be accessed by clicking on a specific `task -> Logs` as shown below.\n\n![Spark logs](/images/why-orc/tasklogs.png)\n\n### See progress & historical information on UI\n\nWhen we run data pipelines, we can use a nice web UI to see the progress, failures, and other details. Powered by the metadata db and the logs, we can also see individual task logs and the inputs to a specific task, among others.\n\nThe web UI provides good visibility into our pipelines' current and historical state.\n\n![DAG logs](/images/why-orc/dagtask.png)\n\n![Task inputs](/images/why-orc/taskip.png)\n\n### Analyze data pipeline performance with Web UI\n\nWe can see how pipelines have performed over time, inspect task run time, and see how long a task had to wait to get started. The performance metrics provide us with the necessary insights to optimize our systems.\n\n![Performance](/images/why-orc/perf.png)\n\n### Re-run data pipelines via UI\n\nIn addition to seeing how our pipelines are running, we can manually trigger DAGs with custom inputs as necessary. The ability to trigger/re-run DAGs helps us quickly resolve one-off issues. See **[this link](https://airflow.apache.org/docs/apache-airflow/stable/core-concepts/dag-run.html#using-ui)** for information on triggering dags with UI and CLI.\n\n![Triggers](/images/why-orc/trigger.png)\n\n### Reuse variables and connections across your pipelines\n\nApache Airflow also allows us to create and store variables and connection settings that can be reused across our data pipelines. In our code, we create variables using Airflow CLI **[here](https://github.com/josephmachado/beginner_de_project/blob/master/containers/airflow/setup_conn.py)**.\n\nVariables and connections are especially important when you want to use a variable across data pipelines and to connect to external systems respectively.\n\nOnce the connection/variables are set, we can see them in our UI:\n\n![Connection](/images/why-orc/conn.png)\n\nAnd use them in your DAG code as such\n\nadd: code\n\n",
    "supporting": [
      "airflow_files"
    ],
    "filters": [],
    "includes": {}
  }
}