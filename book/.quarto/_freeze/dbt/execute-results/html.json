{
  "hash": "a62809cabe349970effd9d93cb0a8115",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: dbt-core is an orchestrator that makes managing pipelines simpler\nformat:\n  html:\n    toc: true\nexecute:\n  eval: false\n  output: true\njupyter: python3\n---\n\n\n\n\nWe saw in the data warehouse (add:link) how data warehouse-ing is the process of getting data ready for analytics. In order to do that we need to create fact, dimensions, OBT and pre-aggregated tables.\n\nBuilding data pipelines to setup and manage these pipelines involve a lot of code especially for functions that are common across pipelines such as data testing, figuring out how to structure the code, etc. This is where dbt-core comes in.\n\ndbt-core is a python library that enables you to build complex data pipelines with only SQL queries. By using a special `ref` function and placing SQL select queries within the appropriate file you can get comples data pipeline graphs, and ability to run pipelines (fully or partially) using the dbt-core library.\n\nIn addition dbt-core also enables best practices like\n\n1. data testing\n2. Full snapshot & incremental data processing capabilities\n3. Functionality to easily create SCD2 tables\n4. Version controlled data pipelines\n5. Separation of folder (recommended add:link) based on the multi-hop architecture (add: link)\nand much more.\n\ndbt-core assumes that the data is already accessible to the DB engine that you run it on, and as such it is mostly used for the T (Transform) part of the ETL (not necessarily in that order) pipeline.\n\nWe will run dbt inside the airflow container, as shown below\n\n```bash\ndocker exec -ti scheduler bash # bash into the running docker container\ncd tpch_analytics # cd into the dbt project directory\ndbt run --profiles-dir . # run dbt models\ndbt test --profiles-dir . # test the created dbt models\n```\n\n## Create tables with select sql files\n\nIn dbt every `.sql` file has a `select` statement and is created as a data model (usually a table or a view). The `select` statement defines the data schema of the data model. The name of the `.sql` file defines the name of the data model.\n\nLet's take a look at one of our silver tables (add: link)\n\n```sql\nadd: silver table code\n```\n\nWe can see how the final select query is created as a data model. Note the `ref` function refers to another table that is defined by the folder path and file name (which is also its data model name).\n\nThe setting that defines which data models should be tables/views/materialized views, etc will be defined in the dbt_project.yml file (add: link).\n\nWhen you run the `dbt run` command all your models will be created as tables.\n\n## Document & test tables with yml files\n\nYou can also document what the table and the columns of your tables mean in `yml` files. These `yml` files have to be within the same folder and also reference the data model's name and the column names.\n\nIn addition to descriptions you can also specify any tests to be run on the columns as needed.\n\nThe documentation will be rendered when you run the `dbt render` command and HTML files will be created which we will view with a `dbt serve` command in a later section.\n\nThe tests can be run with the `dbt test` command, note that the tests can only be run after the data is available, so it is not entirely suitable for the WAP pattern.\n\n```yaml\nadd: core.yml\n```\n\nRun the tests with `dbt test` command.\n\n## Define db connections in profiles.yml\n\ndbt uses a yml file to define how it connects to your db engine. Let's look at our example\n\n```yaml\nadd: profiles.yml\n```\n\nWe tell dbt to connect to Apache Spark. The `target` variable defines the environment. The default is dev, but you can specify which environment to run on with `--target` flag in the dbt run command.\n\nBy default dbt will look for a profiles.yml in your HOME directory. We can tell dbt to look for the profiles.yml file in a specific folder using the `--profiles-dir` flag as shown below.\n\n```bash\ndbt run --profiles-dir .\n```\n\n## Define your project setting at dbt_project.yml\n\nIn `dbt_project.yml` file you specify all the project specific settings that you want applied, such as \n\n1. The folders to look for the `.sql` files\n2. The folder to look for seeds, downloaded packages, SQL functions (aka macros), etc.\n3. How to materialize a data model (ie. should a data model be created as a table/view/materialized view/temporal table, etc)\n\nMaterialization is a variable that controls how dbt creates a model. By default, every model will be a view. This can be overridden in `dbt_project.yml`. We have set the models under `models/marts/core/` to materialize as tables.\n\n```yml\n# Configuring models\nmodels:\n    sde_dbt_tutorial:\n        # Applies to all files under models/marts/core/\n        marts:\n            core:\n                materialized: table\n```\n\nIf you need to define how your dbt project\n\n## dbt recommends the 3-hop architecture with stage, core & data marts\n\nWe will see how the customer_orders table is created from the source tables. These transformations follow warehouse and dbt best practices.\n\n### Source\n\nSource tables refer to tables loaded into the warehouse by an EL process. In our case these are the base tpch tables, which are created by the extract step.\n\nWe need to define what tables are the sources in the src.yml file, this will be used by the stage tables with `source` function.\n\n```yaml\nsource add:\n```\n\n```yaml\nadd: usage with source function\n```\n\n### Staging\n\nThe staging area is where **raw data is cast into correct data types, given consistent column names, and prepared to be transformed into models used by end-users**.\n\nYou can think of this stage as the first layer of transformations. We will place staging data models inside the `staging` folder, as shown below.\n\nadd: folder path\n\nTheir documentation and tests will be defined in a `yaml` file, as shown below.\n\n```yaml\nadd: staging yaml\n```\n\n### Marts\n\nMarts consist of the core tables for end-users and business vertical-specific tables. \n\n#### Core\n\nThe core defines the fact and dimension models to be used by end-users. We define our facts and tables under the `marts/core` folder. \nadd: folder path.\n\nYou can see that we store the facts, dimensions and OBT under this folder.\n\n#### Stakeholder team specific \n\nIn this section, we define the models for `marketing` stakeholders, A project can have multiple business verticals. Having one folder per business vertical provides an easy way to organize the models.\n\nIn our example we store the metrics.sql in this location.\n\n## dbt-core is a cli \n\nWith all our data model defined, we can use the dbt cli to run, test and create documentation. `dbt` command will look for the `profiles.yml` file in your $HOME directory by default so we either have to set the `PROFILES_DIR` environment variable (add: docekr file command) or use the `--profiles-dir` as part of the cli command.\n\n### dbt run\n\nWe have the necessary model definitions in place. Let's create the models.\n\n```bash\ndbt run \n# Finished running 5 view models, 2 table models, 2 hooks in 0 hours 0 minutes and 3.22 seconds (3.22s).\n```\n\nOur staging and marketing models are as materialized views, and the two core models are materialized as views as defined in ourlized as views as defined in our dbt_project.yml.\n\n### dbt test\n\nWith the models defined, we can run tests on them. Note that, unlike standard testing, these tests run after the data has been processed. You can run tests as shown below.\n\n```bash\ndbt test # or run \"just test\"\n# Finished running 14 tests...\n```\n\n### dbt docs\n\nOne of the powerful features of dbt is its docs. To generate documentation and serve them, run the following commands:\n\n```bash\ndbt docs generate\ndbt docs serve\n```\n\nThe `generate` command will create documentation in html format. The `serve` command will start a webserver that serves this html file.\n\nNavigate to `customer_orders` within the `sde_dbt_tutorial` project in the left pane. Click on the view lineage graph icon on the lower right side. The lineage graph shows the dependencies of a model. You can also see the tests defined, descriptions (set in the corresponding YAML file), and the compiled sql statements.\n\n![our project structure](/images/dbt_tutorial/customer_orders_lg.png)\n\n## Scheduling\n\nWe have seen how to create snapshots, models, run tests and generate documentation. These are all commands run via the cli. Dbt compiles the models into sql queries under the `target` folder (not part of git repo) and executes them on the data warehouse.\n\nTo schedule dbt runs, snapshots, and tests we need to use a scheduler. In the final capstone project we will use Airflow to schedule this dbt pipeline.\n\n",
    "supporting": [
      "dbt_files"
    ],
    "filters": [],
    "includes": {}
  }
}