# Capstone Project: Spark, dbt, Airflow with Docker

In the previous chapters we learn't how to manipulate data with Spark SQL, how to create pipeline transformations with dbt and how to schedule  & orchestrate them with Airflow. 

In this capstone project we will put them together to create an end-to-end project.

The main objectives for this capstone project are
1. Understanding how the different components common in data engineering work with each other
2. How to model and transform data in the 3-hop architecture
3. Clearly explain what your pipeline is doing and why and how

## Presentation matters

When a hiring manager looks at your project, assume that they will not read over the code. Typically when people look at projects they browse high level sections, these includes

1. Outcome of your project
2. High level architecture
3. Project structure to understand how your code works
4. Browse code for clarity and code-cleanliness

We will see how you can address these 

## Start with the outcome

We will use the TPCH (add:link) data as the source and our dashboard should be aimed as a dashboard for the marketing team. Somethings they may want to look at are the 

1. Top 20 performing sellers
2. Top selling products
3. Performance of the top selling products over time (say past 12 months)

etc 

We can see how our marketing data model from dbt (add:link) will be useful here. We can use rill to create dashboards which we can then paste in our README.md 

add: gif/step on how to use rill

## High level architecture

The objective of this is to show your expertise in 

1. Designing data pipelines, by following industry standard 3-hop architecture
2. 
