# Python connects the different part of your data pipeline

Python is the glue that holds the different parts of your data pipeline. While powerful data processing engines (Snowflake, Spark, big query, etc) have made processing large amounts of data efficient, you still need a programming language to tell these engines what to do.

In most companies Python dominates the data stack, you'll typically use Python to pull data from source system (Extract), Tell the data processing engine how to process the data (e.g, via SQL queries on Snowflake or SQL/Dataframe query on Spark), and load data into its destination.

In this section we will go over the basics of Python, how its used in data engineering and finish with a topic that is critical to ensure code changes don't break existing logic (testing).

## Data is stored on disk and processed in memory

Processing data is tricky in Python, since Python is not the most optimal language for large scale data manipulation. You would often use Python to tell a data processing engine what to do. For this reason its critical to know how what the difference between disk and memory are.

![Disk and Memory](/images/py-de/python-de.png)

When we run a Python (or any language) script, it is run as a process. Each process will use a part of your computer's memory (RAM). Understanding the difference between RAM and Disk will enable you to write efficient data pipelines; let's go over them:

1. **`Memory`** is the space used by a running process to store any information that it may need for its operation. The computer's RAM is used for this purpose. This is where any variables you define, Pandas dataframe you use will be stored.
2. **`Disk`** is used to store data. When we process data from disk (read data from csv, etc) it means that our process is reading data from disk into memory and then processing it. Computers generally use HDD or SSD to store your files.

RAM is expensive, while disk (HDD, SSD) is cheaper. One issue with data processing is that the memory available to use is often less than the size of the data to be processed. This is when we use distributed systems like Spark or systems like DuckDB, which enable us to process larger-than-memory data.

As we will see in the transformation sections, when we use systems like Spark, Snowflake or Duckdb, Python is just the interface the real data processing (and Memory and disk usage) depends on the data processing engine.

