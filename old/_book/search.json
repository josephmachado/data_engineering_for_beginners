[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data Engineering For Beginners",
    "section": "",
    "text": "Start here\nAre you trying to break into a high paying data engineering job, but\nThen this book is for you. This book is for anyone who wants to get into data engineering, but feels stuck, confused and end up spending a lot of time going in circles. This book is designed to help you lay the foundations for a great career in the field of data.\nAs a data engineering your primary mission will be to enable stakeholders to effectively use data to make decisions. The entierity of this book will focus on how you can do this.",
    "crumbs": [
      "Start here"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "3  Introduction",
    "section": "",
    "text": "This is a book created from markdown and executable code.\nSee Knuth (1984) for additional discussion of literate programming.\n\n\n\n\nKnuth, Donald E. 1984. “Literate Programming.” Comput. J. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "2  Summary",
    "section": "",
    "text": "In summary, this book has no content whatsoever.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Summary</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Knuth, Donald E. 1984. “Literate Programming.” Comput.\nJ. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97.",
    "crumbs": [
      "References"
    ]
  },
  {
    "objectID": "sql.html",
    "href": "sql.html",
    "title": "Use SQL to transform data",
    "section": "",
    "text": "SQL is the foundation on which data engineering works. Most data pipelines are SQL scripts strung together. While there are caveats for using Python v SQL (add:link) in data engineering, almost always SQL (or SQL-like dataframe) are the way you will code.\nIn data engineering context, SQL is used for\n\nAnalytical querying, which involves large amounts of data and aggeregating them to create metrics that define how well business has been performing (e.g. daily active users for a social media company) and how to predict the future.\nData processing, which involves transforming the data from multiple systems into well modelled datasets that can be used for analytics.\n\nIn this section we will see how to use SQL to transform data, and how to use window functions to enable complex computations in SQL.",
    "crumbs": [
      "Use SQL to transform data"
    ]
  },
  {
    "objectID": "data_pipeline.html",
    "href": "data_pipeline.html",
    "title": "Scheduler defines when & Orchestrator defines how to, run your data pipelines",
    "section": "",
    "text": "Schedulers define when to start your data pipeline, e.g. cron, Airflow, etc.\nOrchestrators defines the order in which the tasks of a data pipeline should run. For example extract before transform, complex branching logic, excuting across multiple systems such as spark and snowflake, etc. E.g., dbt-core, Airflow, etc",
    "crumbs": [
      "Scheduler defines when & Orchestrator defines how to, run your data pipelines"
    ]
  },
  {
    "objectID": "sql_basics.html",
    "href": "sql_basics.html",
    "title": "2  Read data, Combine tables, & aggregate numbers to understand business performance",
    "section": "",
    "text": "2.1 A Spark catalog can have multiple schemas, & schemas can have multiple tables\nTypically database servers can have multiple databases; each database can have multiple schemas. Each schema can have multiple tables, and each table can have multiple columns.\nNote: We use Trino, which has catalogs that allow it to connect with the different underlying systems. (e.g., Postgres, Redis, Hive, etc.)\nIn our lab, we use Trino, and we can check the available catalogs, their schemas, the tables in a schema, & the columns in a table, as shown below.\nNote how, when referencing the table name, we use the full path, i.e., database.schema.table_name. We can skip using the full path of the table if we let Trino know which schema to use by default, as shown below.",
    "crumbs": [
      "Use SQL to transform data",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Read data, Combine tables, & aggregate numbers to understand business performance</span>"
    ]
  },
  {
    "objectID": "sql_basics.html#use-selectfrom-limit-where-order-by-to-read-data-from-tables",
    "href": "sql_basics.html#use-selectfrom-limit-where-order-by-to-read-data-from-tables",
    "title": "1  Read data, Combine tables, & aggregate numbers to understand business performance",
    "section": "1.2 Use SELECT…FROM, LIMIT, WHERE, & ORDER BY to read data from tables",
    "text": "1.2 Use SELECT…FROM, LIMIT, WHERE, & ORDER BY to read data from tables\nThe most common use for querying is to read data in our tables. We can do this using a SELECT ... FROM statement, as shown below.\nUSE tpch.tiny;\n\n-- use * to specify all columns\nSELECT * FROM orders;\n\n-- use column names to only read data from those columns\nSELECT orderkey, totalprice FROM orders;\nHowever, running a SELECT ... FROM statement can cause issues when the data set is extensive. If you want to look at the data, use LIMIT n to tell Trino only to get n number of rows.\nUSE tpch.tiny;\n\nSELECT orderkey, totalprice FROM orders LIMIT 5;\nWe can use the’ WHERE’ clause if we want to get the rows that match specific criteria. We can specify one or more filters within the’ WHERE’ clause. The WHERE clause with more than one filter can use combinations of AND and OR criteria to combine the filter criteria, as shown below.\nUSE tpch.tiny;\n\n-- all customer rows that have nationkey = 20\nSELECT * FROM customer WHERE nationkey = 20 LIMIT 10;\n\n-- all customer rows that have nationkey = 20 and acctbal &gt; 1000\nSELECT * FROM customer WHERE nationkey = 20 AND acctbal &gt; 1000 LIMIT 10;\n\n-- all customer rows that have nationkey = 20 or acctbal &gt; 1000\nSELECT * FROM customer WHERE nationkey = 20 OR acctbal &gt; 1000 LIMIT 10;\n\n-- all customer rows that have (nationkey = 20 and acctbal &gt; 1000) or rows that have nationkey = 11\nSELECT * FROM customer WHERE (nationkey = 20 AND acctbal &gt; 1000) OR nationkey = 11 LIMIT 10;\nWe can combine multiple filter clauses, as seen above. We have seen examples of equals (=) and greater than (&gt;) conditional operators. There are 6 conditional operators, they are\n\n&lt; Less than\n&gt; Greater than\n&lt;= Less than or equal to\n&gt;= Greater than or equal to\n= Equal\n&lt;&gt; and != both represent Not equal (some DBs only support one of these)\n\nAdditionally, for string types, we can make pattern matching with like condition. In a like condition, a _ means any single character, and % means zero or more characters, for example.\nUSE tpch.tiny;\n\n-- all customer rows where the name has a 381 in it\nSELECT * FROM customer WHERE name LIKE '%381%';\n\n-- all customer rows where the name ends with a 381\nSELECT * FROM customer WHERE name LIKE '%381';\n\n-- all customer rows where the name starts with a 381\nSELECT * FROM customer WHERE name LIKE '381%';\n\n-- all customer rows where the name has a combination of any character and 9 and 1\nSELECT * FROM customer WHERE name LIKE '%_91%';\nWe can also filter for more than one value using IN and NOT IN.\nUSE tpch.tiny;\n\n-- all customer rows which have nationkey = 10 or nationkey = 20\nSELECT * FROM customer WHERE nationkey IN (10,20);\n\n-- all customer rows which have do not have nationkey as 10 or 20\nSELECT * FROM customer WHERE nationkey NOT IN (10,20);\nWe can get the number of rows in a table using count(*) as shown below.\nUSE tpch.tiny;\n\nSELECT COUNT(*) FROM customer; -- 1500\nSELECT COUNT(*) FROM lineitem; -- 60175\nIf we want to get the rows sorted by values in a specific column, we use ORDER BY, for example.\nUSE tpch.tiny;\n\n-- Will show the first ten customer records with the lowest custkey\n-- rows are ordered in ASC order by default\nSELECT * FROM orders ORDER BY custkey LIMIT 10; \n\n-- Will show the first ten customer's records with the highest custkey\nSELECT * FROM orders ORDER BY custkey DESC LIMIT 10;",
    "crumbs": [
      "Use SQL to transform data",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Read data, Combine tables, & aggregate numbers to understand business performance</span>"
    ]
  },
  {
    "objectID": "sql_basics.html#combine-data-from-multiple-tables-using-joins-there-are-different-types-of-joins",
    "href": "sql_basics.html#combine-data-from-multiple-tables-using-joins-there-are-different-types-of-joins",
    "title": "1  Read data, Combine tables, & aggregate numbers to understand business performance",
    "section": "1.3 Combine data from multiple tables using JOINs (there are different types of JOINS)",
    "text": "1.3 Combine data from multiple tables using JOINs (there are different types of JOINS)\nWe can combine data from multiple tables using joins. When we write a join query, we have a format as shown below.\nSELECT\n    a.*\nFROM\n    table_a a -- LEFT table a\n    JOIN table_b b -- RIGHT table b\n    ON a.id = b.id\nThe table specified first (table_a) is the left table, whereas the table established second is the right table. When we have multiple tables joined, we consider the joined dataset from the first two tables as the left table and the third table as the right table (The DB optimizes our join for performance).\nSELECT\n    a.*\nFROM\n    table_a a -- LEFT table a\n    JOIN table_b b -- RIGHT table b\n    ON a.id = b.id\n    JOIN table_c c -- LEFT table is the joined data from table_a & table_b, right table is table_c\n    ON a.c_id = c.id\nThere are five main types of joins, they are:\n\n1.3.1 1. Inner join (default): Get only rows in both tables\nUSE tpch.tiny;\n\nSELECT\n    o.orderkey,\n    l.orderkey\nFROM\n    orders o\n    JOIN lineitem l ON o.orderkey = l.orderkey\n    AND o.orderdate BETWEEN l.shipdate - INTERVAL '5' DAY\n    AND l.shipdate + INTERVAL '5' DAY\nLIMIT\n    100;\n\nSELECT\n    COUNT(o.orderkey) AS order_rows_count,\n    COUNT(l.orderkey) AS lineitem_rows_count\nFROM\n    orders o\n    JOIN lineitem l ON o.orderkey = l.orderkey\n    AND o.orderdate BETWEEN l.shipdate - INTERVAL '5' DAY\n    AND l.shipdate + INTERVAL '5' DAY;\n-- 2477, 2477\nNote: JOIN defaults to INNER JOIN`.\nThe output will have rows from orders and lineitem that found at least one matching row from the other table with the specified join condition (same orderkey and orderdate within ship date +/- 5 days).\nWe can also see that 2,477 rows from orders and lineitem tables matched.\n\n\n1.3.2 2. Left outer join (aka left join): Get all rows from the left table and only matching rows from the right table.\nUSE tpch.tiny;\n\nSELECT\n    o.orderkey,\n    l.orderkey\nFROM\n    orders o\n    LEFT JOIN lineitem l ON o.orderkey = l.orderkey\n    AND o.orderdate BETWEEN l.shipdate - INTERVAL '5' DAY\n    AND l.shipdate + INTERVAL '5' DAY\nLIMIT\n    100;\n\nSELECT\n    COUNT(o.orderkey) AS order_rows_count,\n    COUNT(l.orderkey) AS lineitem_rows_count\nFROM\n    orders o\n    LEFT JOIN lineitem l ON o.orderkey = l.orderkey\n    AND o.orderdate BETWEEN l.shipdate - INTERVAL '5' DAY\n    AND l.shipdate + INTERVAL '5' DAY;\n-- 15197, 2477\nThe output will have all the rows from orders and the rows from lineitem that were able to find at least one matching row from the orders table with the specified join condition (same orderkey and orderdate within ship date +/- 5 days).\nWe can also see that the number of rows from the orders table is 15,197 & from the lineitem table is 2,477. The number of rows in orders is 15000, but the join condition produces 15197 since some orders match with multiple lineitems.\n\n\n1.3.3 3. Right outer join (aka right join): Get matching rows from the left and all rows from the right table.\nUSE tpch.tiny;\n\nSELECT\n    o.orderkey,\n    l.orderkey\nFROM\n    orders o\n    RIGHT JOIN lineitem l ON o.orderkey = l.orderkey\n    AND o.orderdate BETWEEN l.shipdate - INTERVAL '5' DAY\n    AND l.shipdate + INTERVAL '5' DAY\nLIMIT\n    100;\n\nSELECT\n    COUNT(o.orderkey) AS order_rows_count,\n    COUNT(l.orderkey) AS lineitem_rows_count\nFROM\n    orders o\n    RIGHT JOIN lineitem l ON o.orderkey = l.orderkey\n    AND o.orderdate BETWEEN l.shipdate - INTERVAL '5' DAY\n    AND l.shipdate + INTERVAL '5' DAY;\n-- 2477, 60175\nThe output will have the rows from orders that found at least one matching row from the lineitem table with the specified join condition (same orderkey and orderdate within ship date +/- 5 days) and all the rows from the lineitem table.\nWe can also see that the number of rows from the orders table is 15,197 & from the lineitem table is 2,477.\n\n\n1.3.4 4. Full outer join: Get all rows from both the left and right tables.\nUSE tpch.tiny;\n\nSELECT\n    o.orderkey,\n    l.orderkey\nFROM\n    orders o\n    FULL OUTER JOIN lineitem l ON o.orderkey = l.orderkey\n    AND o.orderdate BETWEEN l.shipdate - INTERVAL '5' DAY\n    AND l.shipdate + INTERVAL '5' DAY\nLIMIT\n    100;\n\nSELECT\n    COUNT(o.orderkey) AS order_rows_count,\n    COUNT(l.orderkey) AS lineitem_rows_count\nFROM\n    orders o\n    FULL OUTER JOIN lineitem l ON o.orderkey = l.orderkey\n    AND o.orderdate BETWEEN l.shipdate - INTERVAL '5' DAY\n    AND l.shipdate + INTERVAL '5' DAY;\n-- 15197, 60175\nThe output will have all the rows from orders that found at least one matching row from the lineitem table with the specified join condition (same orderkey and orderdate within ship date +/- 5 days) and all the rows from the lineitem table.\nWe can also see that the number of rows from the orders table is 15,197 & from the lineitem table is 2,477.\n\n\n1.3.5 5. Cross join: Get the cartesian product of all rows\nUSE tpch.tiny;\n\nSELECT\n    n.name AS nation_name,\n    r.name AS region_name\nFROM\n    nation n\n    CROSS JOIN region r;\nThe output will have every row of the nation joined with every row of the region. There are 25 nations and five regions, leading to 125 rows in our result from the cross-join.\n\n\nThere are cases where we will need to join a table with itself, called a SELF-join. Let’s consider an example.\n\nFor every customer order, get the order placed earlier in the same week (Sunday - Saturday, not the previous seven days). Only show customer orders that have at least one such order.\n\nUSE tpch.tiny;\n\nSELECT\n    o1.custkey,\n    o1.totalprice,\n    o1.orderdate,\n    o2.totalprice,\n    o2.orderdate\nFROM\n    orders o1\n    JOIN orders o2 ON o1.custkey = o2.custkey\n    AND year(o1.orderdate) = year(o2.orderdate)\n    AND week(o1.orderdate) = week(o2.orderdate)\nWHERE\n    o1.orderkey != o2.orderkey\nLIMIT\n    10;",
    "crumbs": [
      "Use SQL to transform data",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Read data, Combine tables, & aggregate numbers to understand business performance</span>"
    ]
  },
  {
    "objectID": "sql_basics.html#generate-metrics-for-your-dimensions-using-group-by",
    "href": "sql_basics.html#generate-metrics-for-your-dimensions-using-group-by",
    "title": "1  Read data, Combine tables, & aggregate numbers to understand business performance",
    "section": "1.4 Generate metrics for your dimension(s) using GROUP BY",
    "text": "1.4 Generate metrics for your dimension(s) using GROUP BY\nMost analytical queries require calculating metrics that involve combining data from multiple rows. GROUP BY allows us to perform aggregate calculations on data from a set of rows recognized by values of specified column(s). For example:\n\nCreate a report that shows the number of orders per orderpriority segment.\n\nUSE tpch.tiny;\n\nSELECT\n    orderpriority,\n    count(*) AS num_orders\nFROM\n    orders\nGROUP BY\n    orderpriority;\nIn the above query, we group the data by orderpriority, and the calculation count(*) will be applied to the rows having a specific orderpriority value.\nThe calculations allowed are typically SUM/MIN/MAX/AVG/COUNT. However, some databases have more complex aggregate functions; check your DB documentation.",
    "crumbs": [
      "Use SQL to transform data",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Read data, Combine tables, & aggregate numbers to understand business performance</span>"
    ]
  },
  {
    "objectID": "sql_basics.html#use-the-result-of-a-query-within-a-query-using-sub-queries",
    "href": "sql_basics.html#use-the-result-of-a-query-within-a-query-using-sub-queries",
    "title": "1  Read data, Combine tables, & aggregate numbers to understand business performance",
    "section": "1.5 Use the result of a query within a query using sub-queries",
    "text": "1.5 Use the result of a query within a query using sub-queries\nWhen we want to use the result of a query as a table in another query, we use subqueries. Let’s consider an example:\n\nCreate a report that shows the nation, how many items it supplied (by suppliers in that nation), and how many items it purchased (by customers in that nation).\n\nUSE tpch.tiny;\n\nSELECT\n    n.name AS nation_name,\n    s.quantity AS supplied_items_quantity,\n    c.quantity AS purchased_items_quantity\nFROM\n    nation n\n    LEFT JOIN (\n        SELECT\n            n.nationkey,\n            sum(l.quantity) AS quantity\n        FROM\n            lineitem l\n            JOIN supplier s ON l.suppkey = s.suppkey\n            JOIN nation n ON s.nationkey = n.nationkey\n        GROUP BY\n            n.nationkey\n    ) s ON n.nationkey = s.nationkey\n    LEFT JOIN (\n        SELECT\n            n.nationkey,\n            sum(l.quantity) AS quantity\n        FROM\n            lineitem l\n            JOIN orders o ON l.orderkey = o.orderkey\n            JOIN customer c ON o.custkey = c.custkey\n            JOIN nation n ON c.nationkey = n.nationkey\n        GROUP BY\n            n.nationkey\n    ) c ON n.nationkey = c.nationkey;\nIn the above query, we can see that there are two sub-queries, one to calculate the quantity supplied by a nation and the other to calculate the quantity purchased by the customers of a nation.",
    "crumbs": [
      "Use SQL to transform data",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Read data, Combine tables, & aggregate numbers to understand business performance</span>"
    ]
  },
  {
    "objectID": "sql_basics.html#change-data-types-cast-and-handle-nulls-coalesce",
    "href": "sql_basics.html#change-data-types-cast-and-handle-nulls-coalesce",
    "title": "2  Read data, Combine tables, & aggregate numbers to understand business performance",
    "section": "2.8 Change data types (CAST) and handle NULLS (COALESCE)",
    "text": "2.8 Change data types (CAST) and handle NULLS (COALESCE)\nEvery column in a table has a specific data type. The data types fall under one of the following categories.\n\nNumerical: Data types used to store numbers.\n\nInteger: Positive and negative numbers. Different types of Integer, such as tinyint, int, and bigint, allow storage of different ranges of values. Integers cannot have decimal digits.\nFloating: These can have decimal digits but stores an approximate value.\nDecimal: These can have decimal digits and store the exact value. The decimal type allows you to specify the scale and precision. Where scale denotes the count of numbers allowed as a whole & precision denotes the count of numbers allowed after the decimal point. E.g., DECIMAL(8,3) allows eight numbers in total, with three allowed after the decimal point.\n\nBoolean: Data types used to store True or False values.\nString: Data types used to store alphanumeric characters.\n\nVarchar(n): Data type allows storage of variable character string, with a permitted max length n.\nChar(n): Data type allows storage of fixed character string. A column of char(n) type adds (length(string) - n) empty spaces to a string that does not have n characters.\n\nDate & time: Data types used to store dates, time, & timestamps(date + time).\nObjects (JSON, ARRAY): Data types used to store JSON and ARRAY data.\n\nSome databases have data types that are unique to them as well. We should check the database documents to understand the data types offered.\nFunctions such as DATE_DIFF and ROUND are specific to a data type. It is best practice to use the appropriate data type for your columns. We can convert data types using the CAST function, as shown below.\nUSE tpch.tiny;\n\nSELECT\n    DATE_DIFF('day', '2022-10-01', '2022-10-05'); -- will fail due to in correct data type\n\nSELECT\n    DATE_DIFF(\n        'day',\n        CAST('2022-10-01' AS DATE),\n        CAST('2022-10-05' AS DATE)\n    );\nA NULL will be used for that field when a value is not present. In cases where we want to use the first non-NULL value from a list of columns, we use COALESCE as shown below.\nLet’s consider an example as shown below. We can see how when l.orderkey is NULL; the DB uses 999999 as the output.\nUSE tpch.tiny;\n\nSELECT\n    o.orderkey,\n    o.orderdate,\n    COALESCE(l.orderkey, 9999999) AS lineitem_orderkey,\n    l.shipdate\nFROM\n    orders o\n    LEFT JOIN lineitem l ON o.orderkey = l.orderkey\n    AND o.orderdate BETWEEN l.shipdate - INTERVAL '5' DAY\n    AND l.shipdate + INTERVAL '5' DAY\nLIMIT\n    100;",
    "crumbs": [
      "Use SQL to transform data",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Read data, Combine tables, & aggregate numbers to understand business performance</span>"
    ]
  },
  {
    "objectID": "sql_basics.html#replicate-if.else-logic-with-case-statements",
    "href": "sql_basics.html#replicate-if.else-logic-with-case-statements",
    "title": "2  Read data, Combine tables, & aggregate numbers to understand business performance",
    "section": "2.5 Replicate IF.ELSE logic with CASE statements",
    "text": "2.5 Replicate IF.ELSE logic with CASE statements\nWe can do conditional logic in the SELECT ... FROM part of our query, as shown below.\nUSE tpch.tiny;\n\nSELECT\n    orderkey,\n    totalprice,\n    CASE\n        WHEN totalprice &gt; 100000 THEN 'high'\n        WHEN totalprice BETWEEN 25000\n        AND 100000 THEN 'medium'\n        ELSE 'low'\n    END AS order_price_bucket\nFROM\n    orders;\nWe can see how we display different values depending on the totalprice column. We can also use multiple criteria as our conditional criteria (e.g., totalprice &gt; 100000 AND orderpriority = ‘2-HIGH’).",
    "crumbs": [
      "Use SQL to transform data",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Read data, Combine tables, & aggregate numbers to understand business performance</span>"
    ]
  },
  {
    "objectID": "sql_basics.html#stack-tables-on-top-of-each-other-with-union-and-union-all-subtract-tables-with-except",
    "href": "sql_basics.html#stack-tables-on-top-of-each-other-with-union-and-union-all-subtract-tables-with-except",
    "title": "2  Read data, Combine tables, & aggregate numbers to understand business performance",
    "section": "2.6 Stack tables on top of each other with UNION and UNION ALL, subtract tables with EXCEPT",
    "text": "2.6 Stack tables on top of each other with UNION and UNION ALL, subtract tables with EXCEPT\nWhen we want to combine data from tables by stacking them on top of each other, we use UNION or UNION ALL. UNION removes duplicate rows, and UNION ALL does not remove duplicate rows. Let’s look at an example.\nUSE tpch.tiny;\n\nSELECT custkey, name FROM customer WHERE name LIKE '%_91%' -- 25 rows\n\n-- UNION will remove duplicate rows; the below query will produce 25 rows\nSELECT custkey, name FROM customer WHERE name LIKE '%_91%'\nUNION\nSELECT custkey, name FROM customer WHERE name LIKE '%_91%'\nUNION\nSELECT custkey, name FROM customer WHERE name LIKE '%_91%';\n\n-- UNION ALL will not remove duplicate rows; the below query will produce 75 rows\nSELECT custkey, name FROM customer WHERE name LIKE '%_91%'\nUNION ALL\nSELECT custkey, name FROM customer WHERE name LIKE '%_91%'\nUNION ALL\nSELECT custkey, name FROM customer WHERE name LIKE '%_91%';\nWhen we want to get all the rows from the first dataset that are not in the second dataset, we can use EXCEPT.\nUSE tpch.tiny;\n\n-- EXCEPT will get the rows in the first query result that is not in the second query result, 0 rows\nSELECT custkey, name FROM customer WHERE name LIKE '%_91%'\nEXCEPT\nSELECT custkey, name FROM customer WHERE name LIKE '%_91%';\n\n\n-- The below query will result in 23 rows; the first query has 25 rows, and the second has two rows\nSELECT custkey, name FROM customer WHERE name LIKE '%_91%'\nEXCEPT\nSELECT custkey, name FROM customer WHERE name LIKE '%191%';",
    "crumbs": [
      "Use SQL to transform data",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Read data, Combine tables, & aggregate numbers to understand business performance</span>"
    ]
  },
  {
    "objectID": "sql_basics.html#save-queries-as-views-for-more-straightforward-reads",
    "href": "sql_basics.html#save-queries-as-views-for-more-straightforward-reads",
    "title": "2  Read data, Combine tables, & aggregate numbers to understand business performance",
    "section": "2.10 Save queries as views for more straightforward reads",
    "text": "2.10 Save queries as views for more straightforward reads\nWhen we have large/complex queries that we need to run often, we can save them as views. Views are DB objects that operate similarly to a table. The OLAP DB executes the underlying query when we query a view.\nUse views to hide query complexities and limit column access (by exposing only specific table columns) for end-users.\nFor example, we can create a view for the nation-level report from the above section, as shown below.\nUSE minio.warehouse;\n\nCREATE VIEW nation_supplied_purchased_quantity AS\nSELECT\n    n.name AS nation_name,\n    s.quantity AS supplied_items_quantity,\n    c.quantity AS purchased_items_quantity\nFROM\n    tpch.tiny.nation n\n    LEFT JOIN (\n        SELECT\n            n.nationkey,\n            sum(l.quantity) AS quantity\n        FROM\n            tpch.tiny.lineitem l\n            JOIN tpch.tiny.supplier s ON l.suppkey = s.suppkey\n            JOIN tpch.tiny.nation n ON s.nationkey = n.nationkey\n        GROUP BY\n            n.nationkey\n    ) s ON n.nationkey = s.nationkey\n    LEFT JOIN (\n        SELECT\n            n.nationkey,\n            sum(l.quantity) AS quantity\n        FROM\n            tpch.tiny.lineitem l\n            JOIN tpch.tiny.orders o ON l.orderkey = o.orderkey\n            JOIN tpch.tiny.customer c ON o.custkey = c.custkey\n            JOIN tpch.tiny.nation n ON c.nationkey = n.nationkey\n        GROUP BY\n            n.nationkey\n    ) c ON n.nationkey = c.nationkey;\n\nSELECT\n    *\nFROM\n    nation_supplied_purchased_quantity;\nNow the view nation_supplied_purchased_quantity will run the underlying query when used. Note here we use the minio.tpch schema because catalog tpch does not allow the creation of views. The tpch catalog comes with Trino and only allows read operations. Read more about connectors here.",
    "crumbs": [
      "Use SQL to transform data",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Read data, Combine tables, & aggregate numbers to understand business performance</span>"
    ]
  },
  {
    "objectID": "sql_basics.html#use-these-standard-inbuilt-db-functions-for-string-time-and-numeric-data-manipulation",
    "href": "sql_basics.html#use-these-standard-inbuilt-db-functions-for-string-time-and-numeric-data-manipulation",
    "title": "2  Read data, Combine tables, & aggregate numbers to understand business performance",
    "section": "2.9 Use these standard inbuilt DB functions for String, Time, and Numeric data manipulation",
    "text": "2.9 Use these standard inbuilt DB functions for String, Time, and Numeric data manipulation\nWhen processing data, more often than not, we will need to change values in columns; shown below are a few standard functions to be aware of:\n\nString functions\n\nLENGTH is used to calculate the length of a string. E.g., SELECT LENGTH('hi'); will output 2.\nCONCAT combines multiple string columns into one. E.g., SELECT CONCAT(clerk, '-', orderpriority) FROM ORDERS LIMIT 5; will concatenate clear and orderpriority columns with a dash in between them.\nSPLIT is used to split a value into an array based on a given delimiter. E.g., SELECT SPLIT(clerk, '#') FROM ORDERS LIMIT 5; will output a column with arrays formed by splitting clerk values on #.\nSUBSTRING is used to get a sub-string from a value, given the start and end character indices. E.g., SELECT clerk, SUBSTR(clerk, 1, 5) FROM orders LIMIT 5; will get the first five (1 - 5) characters of the clerk column. Note that the indexing starts from 1 in Trino.\nTRIM is used to remove empty spaces to the left and right of the value. E.g., SELECT TRIM(' hi '); will output hi without any spaces around it. LTRIM and RTRIM are similar but only remove spaces before and after the string, respectively.\n\nDate and Time functions\n\nAdding and subtracting dates: Is used to add and subtract periods; the format heavily depends on the DB. E.g., In Trino, the query\n  SELECT\n  date_diff('DAY', DATE '2022-10-01', DATE '2023-11-05') diff_in_days,\n  date_diff('MONTH', DATE '2022-10-01', DATE '2023-11-05') diff_in_months,\n  date_diff('YEAR', DATE '2022-10-01', DATE '2023-11-05') diff_in_years;\nIt will show the difference between the two dates in the specified period. We can also add/subtract an arbitrary period from a date/time column. E.g., SELECT DATE '2022-11-05' + INTERVAL '10' DAY; will show the output 2022-11-15.\nstring &lt;=&gt; date/time conversions: When we want to change the data type of a string to date/time, we can use the DATE 'YYYY-MM-DD' or TIMESTAMP 'YYYY-MM-DD HH:mm:SS functions. But when the data is in a different date/time format such as MM/DD/YYYY, we will need to specify the input structure; we do this using date_parse, E.g. SELECT date_parse('11-05-2023', '%m-%d-%Y');. We can convert a timestamp/date into a string with the required format using date_format. E.g., SELECT DATE_FORMAT(orderdate, '%Y-%m-01') AS first_month_date FROM orders LIMIT 5; will map every orderdate to the first of their month.\nTime frame functions (YEAR/MONTH/DAY): When we want to extract specific periods from a date/time column, we can use these functions. E.g., SELECT year(date '2023-11-05'); will return 2023. Similarly, we have month, day, hour, min, etc.\n\nNumeric\n\nROUND is used to specify the number of digits allowed after the decimal point. E.g. SELECT ROUND(100.102345, 2);",
    "crumbs": [
      "Use SQL to transform data",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Read data, Combine tables, & aggregate numbers to understand business performance</span>"
    ]
  },
  {
    "objectID": "sql_basics.html#create-a-table-and-temp-table",
    "href": "sql_basics.html#create-a-table-and-temp-table",
    "title": "1  Read data, Combine tables, & aggregate numbers to understand business performance",
    "section": "1.11 Create a table and temp table",
    "text": "1.11 Create a table and temp table\nWe can create a table using the CREATE TABLE statement. We need to specify the table name, column name, and data types, as shown below.\nCREATE SCHEMA IF NOT EXISTS minio.warehouse WITH (location = 's3a://warehouse/');\n\nUSE minio.warehouse;\n\nDROP TABLE IF EXISTS sample_table;\nCREATE TABLE sample_table (\n  sample_key bigint,\n  sample_status varchar\n);\n\nSELECT * FROM sample_table;\n\nINSERT INTO sample_table VALUES (1, 'hello');\n\nSELECT * FROM sample_table;\nWe can create a temporary table, which is a table that only exists for the duration of the SQL session (open-exit CLI or closing a connection). Unfortunately temp tables are not available in Trino as of writing this book (github issue for temp table support in Trino).",
    "crumbs": [
      "Use SQL to transform data",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Read data, Combine tables, & aggregate numbers to understand business performance</span>"
    ]
  },
  {
    "objectID": "cte.html",
    "href": "cte.html",
    "title": "2  CTE (Common Table Expression) improves code readability and reduces repetition",
    "section": "",
    "text": "2.1 Why to use a CTE\nA CTE is a named select statement that can be reused in a single query.\nComplex SQL queries often involve multiple sub-queries. Multiple sub-queries make the code hard to read.Use a Common Table Expression (CTE) to make your queries readable\nCTEs also make testing complex queries simpler",
    "crumbs": [
      "Use SQL to transform data",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>CTE (Common Table Expression) improves code readability and reduces repetition</span>"
    ]
  },
  {
    "objectID": "cte.html#how-to-define-a-cte",
    "href": "cte.html#how-to-define-a-cte",
    "title": "2  CTE (Common Table Expression) improves code readability and reduces repetition",
    "section": "2.2 How to define a CTE",
    "text": "2.2 How to define a CTE\nUse the WITH key word to start defining a CTE, the with key word is not necessary for consequetive CTE definitions.\n-- CTE definition\nWITH\n  supplier_nation_metrics AS ( -- CTE 1 defined using WITH keyword\n    SELECT\n      n.n_nationkey,\n      SUM(l.l_QUANTITY) AS num_supplied_parts\n    FROM\n      lineitem l\n      JOIN supplier s ON l.l_suppkey = s.s_suppkey\n      JOIN nation n ON s.s_nationkey = n.n_nationkey\n    GROUP BY\n      n.n_nationkey\n  ),\n\n  buyer_nation_metrics AS ( -- CTE 2 defined just as a name\n    SELECT\n      n.n_nationkey,\n      SUM(l.l_QUANTITY) AS num_purchased_parts\n    FROM\n      lineitem l\n      JOIN orders o ON l.l_orderkey = o.o_orderkey\n      JOIN customer c ON o.o_custkey = c.c_custkey\n      JOIN nation n ON c.c_nationkey = n.n_nationkey\n    GROUP BY\n      n.n_nationkey\n  )\n\nSELECT -- The final select will not have a comma before it\n  n.n_name AS nation_name,\n  s.num_supplied_parts,\n  b.num_purchased_parts\nFROM\n  nation n\n  LEFT JOIN supplier_nation_metrics s ON n.n_nationkey = s.n_nationkey\n  LEFT JOIN buyer_nation_metrics b ON n.n_nationkey = b.n_nationkey\nLIMIT 10;\nNote that the last CTE does not have a , after it.\nLet’s look another example: Calculate the money lost due to discounts. Use lineitem to get the price of items (without discounts) that are part of an order and compare it to the order.\nadd: details extn\nHint: Figure out the grain that the comparison need to be made in. Think in steps i.e. get the price of all the items in an order without discounts and then compare it to the orders data whose totalprice has been computed with discounts.\nWITH lineitem_agg AS (\n    SELECT \n        l_orderkey,\n        SUM(l_extendedprice) AS total_price_without_discount\n    FROM \n        lineitem\n    GROUP BY \n        l_orderkey\n)\nSELECT \n    o.o_orderkey,\n    o.o_totalprice, \n    l.total_price_without_discount - o.o_totalprice AS amount_lost_to_discount\nFROM \n    orders o\nJOIN \n    lineitem_agg l ON o.o_orderkey = l.l_orderkey\nORDER BY \n    o.o_orderkey;\nHere are the schemas of orders and lineitem tables. add: TPCH image",
    "crumbs": [
      "Use SQL to transform data",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>CTE (Common Table Expression) improves code readability and reduces repetition</span>"
    ]
  },
  {
    "objectID": "cte.html#why-just-because-you-can-doesnt-mean-you-should.-be-mindful-of-code-readability.",
    "href": "cte.html#why-just-because-you-can-doesnt-mean-you-should.-be-mindful-of-code-readability.",
    "title": "2  CTE (Common Table Expression) improves code readability and reduces repetition",
    "section": "2.2 [WHY] Just because you can doesn’t mean you should. Be mindful of code readability.",
    "text": "2.2 [WHY] Just because you can doesn’t mean you should. Be mindful of code readability.\nA sql query with multiple temporary tables is better than a 1000-line SQL query with numerous CTEs.\n\nKeep the number of CTE per query small (depends on the size of the query, but typically &lt; 5)\nCasestudy:\nRead the query below and answer the question\nwith orders as (\nselect\n        order_id,\n        customer_id,\n        order_status,\n        order_purchase_timestamp::TIMESTAMP AS order_purchase_timestamp,\n        order_approved_at::TIMESTAMP AS order_approved_at,\n        order_delivered_carrier_date::TIMESTAMP AS order_delivered_carrier_date,\n        order_delivered_customer_date::TIMESTAMP AS order_delivered_customer_date,\n        order_estimated_delivery_date::TIMESTAMP AS order_estimated_delivery_date\n    from raw_layer.orders\n    ),\n stg_customers as (\n    select\n        customer_id,\n        zipcode,\n        city,\n        state_code,\n        datetime_created::TIMESTAMP as datetime_created,\n        datetime_updated::TIMESTAMP as datetime_updated,\n        dbt_valid_from,\n        dbt_valid_to\n    from customer_snapshot\n),\nstate as (\nselect\n        state_id::INT as state_id,\n        state_code::VARCHAR(2) as state_code,\n        state_name::VARCHAR(30) as state_name\n    from raw_layer.state\n    ),\ndim_customers as (\nselect\n    c.customer_id,\n    c.zipcode,\n    c.city,\n    c.state_code,\n    s.state_name,\n    c.datetime_created,\n    c.datetime_updated,\n    c.dbt_valid_from::TIMESTAMP as valid_from,\n    case\n        when c.dbt_valid_to is NULL then '9999-12-31'::TIMESTAMP\n        else c.dbt_valid_to::TIMESTAMP\n    end as valid_to\nfrom stg_customers as c\ninner join state as s on c.state_code = s.state_code\n)\nselect\n    o.order_id,\n    o.customer_id,\n    o.order_status,\n    o.order_purchase_timestamp,\n    o.order_approved_at,\n    o.order_delivered_carrier_date,\n    o.order_delivered_customer_date,\n    o.order_estimated_delivery_date,\n    c.zipcode as customer_zipcode,\n    c.city as customer_city,\n    c.state_code as customer_state_code,\n    c.state_name as customer_state_name\nfrom orders as o\ninner join dim_customers as c on\n    o.customer_id = c.customer_id\n    and o.order_purchase_timestamp &gt;= c.valid_from\n    and o.order_purchase_timestamp &lt;= c.valid_to;\n[Exercise]\nTime limit during live workshop: 10 min\nScenario: Assume you are building tables for your data team and creating this CTE.\nQuestion: From a team-wide table reusability perspective, what do you think is wrong with the above query?\nQuestion: How would you change this Code so that your colleagues can reuse your work? Recap\nCTEs help with the readability and reusability of your query\n\nCTEs are defined using the WITH keyword\n\nDon’t overuse CTE; be mindful of query size\n\nCTEs performance depends on the DB; check your query plan",
    "crumbs": [
      "Use SQL to transform data",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>CTE (Common Table Expression) improves code readability and reduces repetition</span>"
    ]
  },
  {
    "objectID": "windows.html",
    "href": "windows.html",
    "title": "3  Use window function when you need to use values from other rows to compute a value for the current row",
    "section": "",
    "text": "3.1 Window functions have four parts\nThe function SUM that we use in the above query is an aggregate function. Notice how the running_sum adds up (aka aggregates) the o_totalprice over all the rows. The rows themselves are ordered in ascending order by its orderdate.\nReference: The standard aggregate functions are MIN, MAX, AVG, SUM, & COUNT, modern data systems offer a variety of powerful aggregations functions. Check your database documentation for available aggreagate functions. e.g. list of agg functions available in TrinoDB\nWrite a query to calculate the daily running average of totalprice of every customer.\nHint: Figure out the PARTITION BY column first, then the ORDER BY column and finally the FUNCTION to use to compute running average.",
    "crumbs": [
      "Use SQL to transform data",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Use window function when you need to use values from other rows to compute a value for the current row</span>"
    ]
  },
  {
    "objectID": "windows.html#how-to-create-a-window-function",
    "href": "windows.html#how-to-create-a-window-function",
    "title": "3  Use window function when you need to use values from other rows to compute a value for the current row",
    "section": "",
    "text": "3.1.1 Window functions have four parts\n\nPartition: Defines a set of rows based on specified column(s) value. If no partition is specified, the entire table is considered a partition.\nOrder By: This optional clause specifies how to order the rows within a partition.\nFunction: The function to be applied on the current row. The function results in an additional column in the output.\nWindow frame: Within a partition, a window frame allows you to specify the rows to be considered in the function computation.\n\n\n\n\n\nCreate window function\n\n\nSELECT\n  o_custkey,\n  o_orderdate,\n  o_totalprice,\n  SUM(o_totalprice) -- FUNCTION \n  OVER (\n    PARTITION BY\n      o_custkey -- PARTITION\n    ORDER BY\n      o_orderdate -- ORDER BY; ASCENDING ORDER unless specified as DESC\n  ) AS running_sum\nFROM\n  orders\nWHERE\n  o_custkey = 4\nORDER BY\n  o_orderdate\nLIMIT\n  10;\nThe function SUM that we use in the above query is an aggregate function. Notice how the running_sum adds up (aka aggregates) the o_totalprice over all the rows. The rows themselves are ordered in ascending order by its orderdate.\nReference: While the standard aggregate functions are MIN, MAX, AVG, SUM, & COUNT, modern data systems offer a variety of powerful aggregations functions. Check your database documentation for available aggreagate functions. e.g. list of agg functions available in duckdb\n\n\n3.1.2 [Exercise]\nWrite a query to calculate the daily running average of totalprice of every customer.\nTime limit during live workshop: 5 min\nHint: Figure out the PARTITION BY column first, then the ORDER BY column and finally the FUNCTION to use to compute running average.\n%%sql – your code here\n\n\n3.1.3 A window frame defines a set of rows within a partition on which a function is to be applied\nWhile our functions operate on the rows in the partition a window frame provides more granular ways to operate on a select set of rows within a partition.\nWhen we need to operate one a set of rows within a parition (e.g. a sliding window) we can use the window frame to define these set of rows.\n\n\n\nThree order Sliding window average\n\n\nExample\nConsider a scenario where you have sales data, and you want to calculate a 3-day moving average of sales within each store:\nSELECT\n    store_id,\n    sale_date,\n    sales_amount,\n    AVG(sales_amount) OVER (\n        PARTITION BY store_id\n        ORDER BY sale_date\n        ROWS BETWEEN 2 PRECEDING AND CURRENT ROW\n    ) AS moving_avg_sales\nFROM\n    sales;\nIn this example:\n\nPARTITION BY store_id ensures the calculation is done separately for each store.\nORDER BY sale_date defines the order of rows within each partition.\nROWS BETWEEN 2 PRECEDING AND CURRENT ROW specifies the window frame, considering the current row and the two preceding rows to calculate the moving average. add: image Without defining the window frame, the function might not be able to provide the specific moving average calculation you need.\n\n\n3.1.3.1 Use ordering of rows to define your window frame with the ROWS clause\n\nROWS: Used to select a set of rows relative to the current row based on position.\n\nRow definition format ROWS BETWEEN start_point AND end_point.\nThe start_point and end_point can be any of the following three (in the proper order:\n\nn PRECEDING: n rows preceding the current row. UNBOUNDED PRECEDING indicates all rows before the current row.\nn FOLLOWING: n rows following the current row. UNBOUNDED FOLLOWING indicates all rows after the current row.\n\n\n\nLet’s see how relative row numbers can be used to define a window range.\nConsider this window function\nAVG(total_price) OVER ( -- FUNCTION: RUNNING AVERAGE\n    PARTITION BY o_custkey -- PARTITIONED BY customer\n    ORDER BY order_month \n    ROWS BETWEEN 1 PRECEDING AND 1 FOLLOWING -- WINDOW FRAME DEFINED AS 1 ROW PRECEDING to 1 ROW FOLLOWING\n    )\n\n\n\nWindow frame with ROWS\n\n\n\nWrite a query on the orders table that has the following output:\n\no_custkey\norder_month: In YYYY-MM format, use strftime(o_orderdate, ‘%Y-%m’) AS order_month\ntotal_price: Sum of o_totalprice for that month\nthree_mo_total_price_avg: The 3 month (previous, current & next) average of total_price for that customer\n\n\nSELECT\n  order_month,\n  o_custkey,\n  total_price,\n  ROUND(\n    AVG(total_price) OVER ( -- FUNCTION: RUNNING AVERAGE\n      PARTITION BY\n        o_custkey -- PARTITIONED BY customer\n      ORDER BY\n        order_month ROWS BETWEEN 1 PRECEDING\n        AND 1 FOLLOWING -- WINDOW FRAME DEFINED AS 1 ROW PRECEDING to 1 ROW FOLLOWING\n    ),\n    2\n  ) AS three_mo_total_price_avg\nFROM\n  (\n    SELECT\n      strftime (o_orderdate, '%Y-%m') AS order_month,\n      o_custkey,\n      SUM(o_totalprice) AS total_price\n    FROM\n      orders\n    GROUP BY\n      1,\n      2\n  )\nLIMIT\n  5;\n\n\n3.1.3.2 Use values of the columns to define window frame using RANGE clause\n\nRANGE: Used to select a set of rows relative to the current row based on the value of the columns specified in the ORDER BY clause.\n\nRange definition format RANGE BETWEEN start_point AND end_point.\nThe start_point and end_point can be any of the following:\n\nCURRENT ROW: The current row.\nn PRECEDING: All rows with values within the specified range that are less than or equal to n units preceding the value of the current row.\nn FOLLOWING: All rows with values within the specified range that are greater than or equal to n units following the value of the current row.\nUNBOUNDED PRECEDING: All rows before the current row within the partition.\nUNBOUNDED FOLLOWING: All rows after the current row within the partition.\n\nRANGE is particularly useful when dealing with numeric or date/time ranges, allowing for calculations like running totals, moving averages, or cumulative distributions.\n\n\nLet’s see how RANGE works with AVG(total price) OVER (PARTITION BY customer id ORDER BY date RANGE BETWEEN INTERVAL '1' DAY PRECEDING AND '1' DAY FOLLOWING) using the below visualization:\n\n\n\nRANGE\n\n\n\n\n\n3.1.4 [Exercise]\nNow that we have seen how to create a window frame with ROWS, let’ explore how to do this with RANGE.\n\nWrite a query on the orders table that has the following output:\n\norder_month,\no_custkey,\ntotal_price,\nthree_mo_total_price_avg\nconsecutive_three_mo_total_price_avg: The consecutive 3 month average of total_price for that customer. Note that this should only include months that are chronologically next to each other.\n\n\nTime limit during live workshop: 10 min\nHint: Use CAST(strftime(o_orderdate, '%Y-%m-01') AS DATE) to cast order_month to date format.\nHint: Use the INTERVAL format shown above to construct the window function to compute consecutive_three_mo_total_price_avg column.\n\nThe orders table schema is shown below:\n\n\n\n\nOrders table\n\n\n-- write your query here",
    "crumbs": [
      "Use SQL to transform data",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Use window function when you need to use values from other rows to compute a value for the current row</span>"
    ]
  },
  {
    "objectID": "windows.html#how-to-use-ranking-functions-to-get-topbottom-n-rows",
    "href": "windows.html#how-to-use-ranking-functions-to-get-topbottom-n-rows",
    "title": "3  Use window functions to do looping with SQL",
    "section": "4.2 [HOW] to use ranking functions to get top/bottom n rows",
    "text": "4.2 [HOW] to use ranking functions to get top/bottom n rows\nIf you are working on a problem to get the top/bottom n rows (as defined by some value) then use the row functions.\nLet’s look at an example of how to use a row function:\nFrom the orders table get the top 3 spending customers per day. The orders table schema is shown below:\n\n\n\nOrders table\n\n\n\n4.2.1 [Example]\nSELECT\n  *\nFROM\n  (\n    SELECT\n      o_orderdate,\n      o_totalprice,\n      o_custkey,\n      RANK() -- RANKING FUNCTION \n      OVER (\n        PARTITION BY\n          o_orderdate -- PARTITION BY order date\n        ORDER BY\n          o_totalprice DESC -- ORDER rows withing partition by totalprice\n      ) AS rnk\n    FROM\n      orders\n  )\nWHERE\n  rnk &lt;= 3\nORDER BY\n  o_orderdate\nLIMIT\n  5;\n\n\n4.2.2 [Exercise] From the orders table get the 3 lowest spending customers per day\nTime limit during live workshop: 5 min\nHint 1. Figure out the PARTITION BY column first, then the ORDER BY column and finally the FUNCTION to use to compute running average.\nThe orders table schema is shown below:\n\n\n\nOrders table\n\n\n-- your code here",
    "crumbs": [
      "Use SQL to transform data",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Use window function when you need to use values from other rows to compute a value for the current row</span>"
    ]
  },
  {
    "objectID": "windows.html#reference-standard-ranking-functions",
    "href": "windows.html#reference-standard-ranking-functions",
    "title": "3  Use window functions to do looping with SQL",
    "section": "4.3 [REFERENCE] standard RANKING functions:",
    "text": "4.3 [REFERENCE] standard RANKING functions:\n\nRANK: Ranks the rows starting from 1 to n within the window frame. Ranks the rows with the same value (defined by the “ORDER BY” clause) as the same and skips the ranking numbers that would have been present if the values were different.\nDENSE_RANK: Ranks the rows starting from 1 to n within the window frame. Ranks the rows with the same value (defined by the “ORDER BY” clause) as the same and does not skip any ranking numbers.\nROW_NUMBER: Adds a row number that starts from 1 to n within the window frame and does not create any repeating values.\n\n\n4.3.1 [Example]\n-- Let's look at an example showing the difference between RANK, DENSE_RANK and ROW_NUMBER\nSELECT \n    order_date,\n    order_id,\n    total_price,\n    ROW_NUMBER() OVER (PARTITION BY order_date ORDER BY total_price) AS row_number,\n    RANK() OVER (PARTITION BY order_date ORDER BY total_price) AS rank,\n    DENSE_RANK() OVER (PARTITION BY order_date ORDER BY total_price) AS dense_rank\nFROM (\n    SELECT \n        '2024-07-08' AS order_date, 'order_1' AS order_id, 100 AS total_price UNION ALL\n    SELECT \n        '2024-07-08', 'order_2', 200 UNION ALL\n    SELECT \n        '2024-07-08', 'order_3', 150 UNION ALL\n    SELECT \n        '2024-07-08', 'order_4', 90 UNION ALL\n    SELECT \n        '2024-07-08', 'order_5', 100 UNION ALL\n    SELECT \n        '2024-07-08', 'order_6', 90 UNION ALL\n    SELECT \n        '2024-07-08', 'order_7', 100 UNION ALL\n    SELECT \n        '2024-07-10', 'order_8', 100 UNION ALL\n    SELECT \n        '2024-07-10', 'order_9', 100 UNION ALL\n    SELECT \n        '2024-07-10', 'order_10', 100 UNION ALL\n    SELECT \n        '2024-07-11', 'order_11', 100\n) AS orders\nORDER BY order_date, row_number;\nNow that we have see how to define a window function and how to use ranking and aggregation functions, let’s take it a step further by practicing value functions.\nRemember that value functions are used to access other row’s values while operating on the current row\nLet’s take a look at LEAD and LAG functions:\n\n\n\nLAG AND LEAD\n\n\n\n\n4.3.2 [Exercise] Write a SQL query using the orders table that calculates the following columns:\n1. o_orderdate: From orders table\n2. o_custkey: From orders table\n3. o_totalprice: From orders table\n4. totalprice_diff: The customers current day's o_totalprice - that same customers most recent previous purchase's o_totalprice\n\nTime limit during live workshop: 5 min\nHint:\n\nStart by figuring out what the PARTITION BY column should be, then what the ORDER BY column should be, and then finally the function to use.\nUse the LAG(column_name) ranking function to identify the prior day’s revenue.\n\nThe orders table schema is shown below:\n\n\n\n\nOrders table\n\n\n-- write your query here",
    "crumbs": [
      "Use SQL to transform data",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Use window function when you need to use values from other rows to compute a value for the current row</span>"
    ]
  },
  {
    "objectID": "windows.html#reference-here-is-a-quick-definition-of-the-standard-value-functions",
    "href": "windows.html#reference-here-is-a-quick-definition-of-the-standard-value-functions",
    "title": "3  Use window functions to do looping with SQL",
    "section": "4.4 [REFERENCE] Here is a quick definition of the standard VALUE functions:",
    "text": "4.4 [REFERENCE] Here is a quick definition of the standard VALUE functions:\n\nNTILE(n): Divides the rows in the window frame into n approximately equal groups, and assigns a number to each row indicating which group it belongs to.\nFIRST_VALUE(): Returns the first value in the window frame.\nLAST_VALUE(): Returns the last value in the window frame.\nLAG(): Accesses data from a previous row within the window frame.\nLEAD(): Accesses data from a subsequent row within the window frame.",
    "crumbs": [
      "Use SQL to transform data",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Use window function when you need to use values from other rows to compute a value for the current row</span>"
    ]
  },
  {
    "objectID": "windows.html#how-to-define-a-window-frame-with-rows",
    "href": "windows.html#how-to-define-a-window-frame-with-rows",
    "title": "3  Use window functions to do looping with SQL",
    "section": "5.1 [HOW] to define a window frame with ROWS",
    "text": "5.1 [HOW] to define a window frame with ROWS\n\nROWS: Used to select a set of rows relative to the current row based on position.\n\nRow definition format ROWS BETWEEN start_point AND end_point.\nThe start_point and end_point can be any of the following three (in the proper order:\n\nn PRECEDING: n rows preceding the current row. UNBOUNDED PRECEDING indicates all rows before the current row.\nn FOLLOWING: n rows following the current row. UNBOUNDED FOLLOWING indicates all rows after the current row.\n\n\n\nLet’s see how relative row numbers can be used to define a window range.\nConsider this window function\nAVG(total_price) OVER ( -- FUNCTION: RUNNING AVERAGE\n    PARTITION BY o_custkey -- PARTITIONED BY customer\n    ORDER BY order_month \n    ROWS BETWEEN 1 PRECEDING AND 1 FOLLOWING -- WINDOW FRAME DEFINED AS 1 ROW PRECEDING to 1 ROW FOLLOWING\n    )\n\n\n\nWindow frame with ROWS\n\n\n\n5.1.1 [Example]\n\nWrite a query on the orders table that has the following output:\n\no_custkey\norder_month: In YYYY-MM format, use strftime(o_orderdate, ‘%Y-%m’) AS order_month\ntotal_price: Sum of o_totalprice for that month\nthree_mo_total_price_avg: The 3 month (previous, current & next) average of total_price for that customer\n\n\nSELECT\n  order_month,\n  o_custkey,\n  total_price,\n  ROUND(\n    AVG(total_price) OVER ( -- FUNCTION: RUNNING AVERAGE\n      PARTITION BY\n        o_custkey -- PARTITIONED BY customer\n      ORDER BY\n        order_month ROWS BETWEEN 1 PRECEDING\n        AND 1 FOLLOWING -- WINDOW FRAME DEFINED AS 1 ROW PRECEDING to 1 ROW FOLLOWING\n    ),\n    2\n  ) AS three_mo_total_price_avg\nFROM\n  (\n    SELECT\n      strftime (o_orderdate, '%Y-%m') AS order_month,\n      o_custkey,\n      SUM(o_totalprice) AS total_price\n    FROM\n      orders\n    GROUP BY\n      1,\n      2\n  )\nLIMIT\n  5;",
    "crumbs": [
      "Use SQL to transform data",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Use window function when you need to use values from other rows to compute a value for the current row</span>"
    ]
  },
  {
    "objectID": "windows.html#how-to-define-a-window-frame-with-range",
    "href": "windows.html#how-to-define-a-window-frame-with-range",
    "title": "3  Use window functions to do looping with SQL",
    "section": "5.2 [HOW] to define a window frame with RANGE",
    "text": "5.2 [HOW] to define a window frame with RANGE\n\nRANGE: Used to select a set of rows relative to the current row based on the value of the columns specified in the ORDER BY clause.\n\nRange definition format RANGE BETWEEN start_point AND end_point.\nThe start_point and end_point can be any of the following:\n\nCURRENT ROW: The current row.\nn PRECEDING: All rows with values within the specified range that are less than or equal to n units preceding the value of the current row.\nn FOLLOWING: All rows with values within the specified range that are greater than or equal to n units following the value of the current row.\nUNBOUNDED PRECEDING: All rows before the current row within the partition.\nUNBOUNDED FOLLOWING: All rows after the current row within the partition.\n\nRANGE is particularly useful when dealing with numeric or date/time ranges, allowing for calculations like running totals, moving averages, or cumulative distributions.\n\n\nLet’s see how RANGE works with AVG(total price) OVER (PARTITION BY customer id ORDER BY date RANGE BETWEEN INTERVAL '1' DAY PRECEDING AND '1' DAY FOLLOWING) using the below visualization:\n\n\n\nRANGE\n\n\n\n5.2.1 [Exercise]\nNow that we have seen how to create a window frame with ROWS, let’ explore how to do this with RANGE.\n\nWrite a query on the orders table that has the following output:\n\norder_month,\no_custkey,\ntotal_price,\nthree_mo_total_price_avg\nconsecutive_three_mo_total_price_avg: The consecutive 3 month average of total_price for that customer. Note that this should only include months that are chronologically next to each other.\n\n\nTime limit during live workshop: 10 min\nHint: Use CAST(strftime(o_orderdate, '%Y-%m-01') AS DATE) to cast order_month to date format.\nHint: Use the INTERVAL format shown above to construct the window function to compute consecutive_three_mo_total_price_avg column.\n\nThe orders table schema is shown below:\n\n\n\n\nOrders table\n\n\n-- write your query here",
    "crumbs": [
      "Use SQL to transform data",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Use window function when you need to use values from other rows to compute a value for the current row</span>"
    ]
  },
  {
    "objectID": "python.html",
    "href": "python.html",
    "title": "Python connects the different part of your data pipeline",
    "section": "",
    "text": "Data is stored on disk and processed in memory\nProcessing data is tricky in Python, since Python is not the most optimal language for large scale data manipulation. You would often use Python to tell a data processing engine what to do. For this reason its critical to know how what the difference between disk and memory are.\nWhen we run a Python (or any language) script, it is run as a process. Each process will use a part of your computer’s memory (RAM). Understanding the difference between RAM and Disk will enable you to write efficient data pipelines; let’s go over them:\nRAM is expensive, while disk (HDD, SSD) is cheaper. One issue with data processing is that the memory available to use is often less than the size of the data to be processed. This is when we use distributed systems like Spark or systems like DuckDB, which enable us to process larger-than-memory data.\nAs we will see in the transformation sections, when we use systems like Spark, Snowflake or Duckdb, Python is just the interface the real data processing (and Memory and disk usage) depends on the data processing engine.",
    "crumbs": [
      "Python connects the different part of your data pipeline"
    ]
  },
  {
    "objectID": "python.html#data-is-stored-on-disk-and-processed-in-memory",
    "href": "python.html#data-is-stored-on-disk-and-processed-in-memory",
    "title": "Python connects the different part of your data pipeline",
    "section": "",
    "text": "Disk and Memory\n\n\n\n\nMemory is the space used by a running process to store any information that it may need for its operation. The computer’s RAM is used for this purpose. This is where any variables you define, Pandas dataframe you use will be stored.\nDisk is used to store data. When we process data from disk (read data from csv, etc) it means that our process is reading data from disk into memory and then processing it. Computers generally use HDD or SSD to store your files.",
    "crumbs": [
      "Python connects the different part of your data pipeline"
    ]
  },
  {
    "objectID": "py_basics.html",
    "href": "py_basics.html",
    "title": "4  Manipulate data with standard libraries and co-locate code with classes and functions",
    "section": "",
    "text": "4.0.1 Use the appropriate data structure based on how data will be used\nLet’s go over some basics of the Python language:",
    "crumbs": [
      "Python connects the different part of your data pipeline",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Manipulate data with standard libraries and co-locate code with classes and functions</span>"
    ]
  },
  {
    "objectID": "py_el.html",
    "href": "py_el.html",
    "title": "5  Python has libraries to read and write data to (almost) any system",
    "section": "",
    "text": "5.1 Exercises",
    "crumbs": [
      "Python connects the different part of your data pipeline",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Python has libraries to read and write data to (almost) any system</span>"
    ]
  },
  {
    "objectID": "py_transform.html",
    "href": "py_transform.html",
    "title": "6  Python has libraries to tell the data processing engine (Spark, Trino, Duckdb, Polars, etc) what to do",
    "section": "",
    "text": "6.1 Exercises",
    "crumbs": [
      "Python connects the different part of your data pipeline",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Python has libraries to tell the data processing engine (Spark, Trino, Duckdb, Polars, etc) what to do</span>"
    ]
  },
  {
    "objectID": "dw.html",
    "href": "dw.html",
    "title": "7  Data warehouse contains historical data and is used to analyze business performance",
    "section": "",
    "text": "7.1 OLTP vs OLAP based data warehouses\nThere are two main types of databases: OLTP and OLAP. Their differences are shown below.\nNote Apache Spark started as a pure data processing system and over time with the increased need for structure introduced capabilities to manage tables.",
    "crumbs": [
      "Data modeling is the process of getting data ready for analytics",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Data warehouse contains historical data and is used to analyze business performance</span>"
    ]
  },
  {
    "objectID": "dw.html#what-is-a-data-warehouse",
    "href": "dw.html#what-is-a-data-warehouse",
    "title": "7  Data warehouse contains historical data and is used to analyze business performance",
    "section": "",
    "text": "basic Data Warehouse\n\n\n\n\nDimensional modeling - Kimball\nData vault - Linstedt\nData mart\nFlat table",
    "crumbs": [
      "Data modeling is the process of getting data ready for analytics",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Data warehouse contains historical data and is used to analyze business performance</span>"
    ]
  },
  {
    "objectID": "dw.html#oltp-vs-olap-based-data-warehouses",
    "href": "dw.html#oltp-vs-olap-based-data-warehouses",
    "title": "7  Data warehouse contains historical data and is used to analyze business performance",
    "section": "",
    "text": "OLTP\nOLAP\n\n\n\n\nStands for\nOnline transaction processing\nOnline analytical processing\n\n\nUsage pattern\nOptimized for fast CRUD(create, read, update, delete) of a small number of rows\nOptimized for running select c1, c2, sum(c3),.. where .. group by on a large number of rows (aka analytical queries), and ingesting large amounts of data via bulk import or event stream\n\n\nStorage type\nRow oriented\nColumn-oriented\n\n\nData modeling\nData modeling is based on normalization\nData modeling is based on denormalization. Some popular ones are dimensional modeling and data vaults\n\n\nData state\nRepresents current state of the data\nContains historical events that have already happened\n\n\nData size\nGigabytes to Terabytes\nTerabytes and above\n\n\nExample database\nMySQL, Postgres, etc\nClickhouse, AWS Redshift, Snowflake, GCP Bigquery, etc",
    "crumbs": [
      "Data modeling is the process of getting data ready for analytics",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Data warehouse contains historical data and is used to analyze business performance</span>"
    ]
  },
  {
    "objectID": "dw_tables.html",
    "href": "dw_tables.html",
    "title": "8  Data warehouse modeling (Kimball) is based off of 2 types of tables: Fact and dimensions",
    "section": "",
    "text": "8.1 Facts represents events that occured & dimensions the entities to which events occur to\nA data warehouse is a database that stores your company’s historical data. The tables in a data warehouse are usually of two types, as shown below.\nA fact table’s grain (aka granularity, level) refers to what a row in a fact table represents. E.g., In our checkout process, we can have two fact tables, one for the order and another for the individual items in the order. The items table will have one row per item purchased, whereas the order table will have one row per order made.\nNote: If you notice the slight difference in the decimal digits, it’s due to using a double datatype which is an inexact data type.\nWe can see how the lineitem table can be “rolled up” to get the data in the orders table. But having just the orders table is not sufficient since the lineitem table will provide us with individual item details such as discount and quantity details.",
    "crumbs": [
      "Data modeling is the process of getting data ready for analytics",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Data warehouse modeling (Kimball) is based off of 2 types of tables: Fact and dimensions</span>"
    ]
  },
  {
    "objectID": "dw_tables.html#model-your-data-define-data-models-for-historical-analytics",
    "href": "dw_tables.html#model-your-data-define-data-models-for-historical-analytics",
    "title": "8  Data warehouse modeling (Kimball) is based off of 2 types of tables: Fact and dimensions",
    "section": "8.6 [Model your data] Define data models for historical analytics",
    "text": "8.6 [Model your data] Define data models for historical analytics\nWith knowledge of your data and the data that end users want, you can begin to design your pipeline. Before jumping into the tech, you should clearly define the tables that your pipeline will produce.\nHere is a list of steps that you can follow to define your data model:\n\nUse the 3-hop architecture. Most companies use dbt and/or Spark for their pipelines—both dbt flow and medallion (databricks recommended architecture) for the 3-hop pattern.\nStart by defining your silver tables: The facts and dimension tables.\nDefine the join keys for the silver tables.\nIf the dimensional data can change over time, use SCD2.\nIn order to keep the metric definition in your code base, use OBT and aggregated tables.\nSpecify the different layers:\n\nraw: raw data from upstream sources, as is dumped into a cloud store/raw table.\nbronze: raw -&gt; data types and naming conventions applied\nsilver: bronze -&gt; transformed to facts and dimensions\ngold: facts and dimensions -&gt; OBT (optionally) -&gt; aggregate tables with metrics for direct use by end users or BI layer\n\nIf the fact table is large (usually the case), use a partition to store the data in separate folders. The columns to partition by typically are the time/day when the data event occurred and also by the most commonly filtered column. Mention that you are considering partitioning in this stage; you can expand on this later in the data storage section.\n\nOutcomes:\n\nData flow architecture: raw -&gt; bronze -&gt; silver -&gt; gold.\nDefined facts, dimensions, OBT, and aggregate tables with their data schemas.\nData partitions for large fact and OBT tables.",
    "crumbs": [
      "Data modeling is the process of getting data ready for analytics",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Data warehouse modeling (Kimball) is based off of 2 types of tables: Fact and dimensions</span>"
    ]
  },
  {
    "objectID": "dw_tables.html#relate-the-data-model-to-the-business-by-understanding-facts-and-dimensions",
    "href": "dw_tables.html#relate-the-data-model-to-the-business-by-understanding-facts-and-dimensions",
    "title": "8  Data warehouse modeling (Kimball) is based off of 2 types of tables: Fact and dimensions",
    "section": "",
    "text": "Dimension: Each row in a dimension table represents a business entity that is important to the business. For example, An car parts seller’s data warehouse will have a customer dimension table, where each row will represent an individual customer. Other examples of dimension tables in a car parts seller’s data warehouse would be supplier & part tables.\nFacts: Each row in a fact table represents a business process that occurred. E.g., In our data warehouse, each row in the orders fact table will represent an individual order, and each row in the lineitem fact table will represent an item sold as part of an order. Each fact row will have a unique identifier; in our case, it’s orderkey for orders and a combination of orderkey & linenumber for lineitem.\n\n\n\n\n-- calculating the totalprice of an order (with orderkey = 1) from it's individual items\nSELECT\n    orderkey,\n    round( sum(extendedprice * (1 - discount) * (1 + tax)),\n        2\n    ) AS totalprice\nFROM\n    lineitem\nWHERE\n    orderkey = 1\nGROUP BY\n    orderkey;\n\n/*\n orderkey | totalprice\n----------+------------\n        1 |  172799.56\n*/\n\n-- The totalprice of an order (with orderkey = 1)\nSELECT\n    orderkey,\n    totalprice\nFROM\n    orders\nWHERE\n    orderkey = 1;\n\n/*\n orderkey | totalprice\n----------+------------\n        1 |  172799.49\n*/",
    "crumbs": [
      "Data modeling is the process of getting data ready for analytics",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Data warehouse modeling (Kimball) is based off of 2 types of tables: Fact and dimensions</span>"
    ]
  },
  {
    "objectID": "dw_tables.html#modifying-granularity-aka-roll-upgroup-by-is-the-basis-of-analytical-data-processing",
    "href": "dw_tables.html#modifying-granularity-aka-roll-upgroup-by-is-the-basis-of-analytical-data-processing",
    "title": "8  Data warehouse modeling (Kimball) is based off of 2 types of tables: Fact and dimensions",
    "section": "8.4 Modifying granularity (aka roll up/Group by) is the basis of analytical data processing",
    "text": "8.4 Modifying granularity (aka roll up/Group by) is the basis of analytical data processing\nThe term analytical querying usually refers to aggregating numerical (spend, count, sum, avg) data from the fact table for specific dimension attribute(s) (e.g., name, Nation, date, month) from the dimension tables. Some examples of analytical queries are\n\nWho are the top 10 suppliers (by totalprice) in the past year?\nWhat are the average sales per Nation per year?\nHow do customer market segments perform (sales) month-over-month?\n\nThe questions above ask about historically aggregating data from the fact tables for one or more business entities(dimensions). Consider the example analytical question below and notice the facts and dimensions.\n\nWhen we dissect the above analytical query, we see that it involves:\n\nJoining the fact data with dimension table(s) to get the dimension attributes such as name, region, & brand. We join the orders fact table with the customer dimension table in our example.\nModifying granularity (aka rollup, Group by) of the joined table to the dimension(s) in question. In our example, this refers to GROUP BY custkey, YEAR(orderdate).\n\n\nWhen rolling up, we must pay attention to the type of data stored in the column. E.g., In our lineitem table, we can sum up the price but not the discount percentage since percentages cannot be rolled up (don’t roll up fractional calculate numbers, distinct counts). Additive facts are the columns in the fact table that can be aggregated and still have a meaningful value, while the ones that cannot (e.g., percentages, distinct counts) are called non-additive facts.\nWe can see how understanding the granularity and which columns are additive or not will help you answer a business question.",
    "crumbs": [
      "Data modeling is the process of getting data ready for analytics",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Data warehouse modeling (Kimball) is based off of 2 types of tables: Fact and dimensions</span>"
    ]
  },
  {
    "objectID": "dw_tables.html#popular-dimension-types-snapshot-scd2",
    "href": "dw_tables.html#popular-dimension-types-snapshot-scd2",
    "title": "8  Data warehouse modeling (Kimball) is based off of 2 types of tables: Fact and dimensions",
    "section": "8.5 Popular dimension types: Snapshot & SCD2",
    "text": "8.5 Popular dimension types: Snapshot & SCD2",
    "crumbs": [
      "Data modeling is the process of getting data ready for analytics",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Data warehouse modeling (Kimball) is based off of 2 types of tables: Fact and dimensions</span>"
    ]
  },
  {
    "objectID": "dw_data_flow.html",
    "href": "dw_data_flow.html",
    "title": "9  Most companies use the multi-hop architecture",
    "section": "",
    "text": "Following an established way of processing data accounts for handling common potential issues, and you have plenty of resources to reference. Most industry-standard patterns follow a 3-hop (or layered) architecture. They are\n\nRaw layer stores data from upstream sources as is. This layer sometimes involves changing data types and standardizing column names.\nTransformed layer: Data from the raw layer is transformed based on a chosen modeling principle to form a set of tables. Common modeling principles are Dimensional modeling (Kimball), Data Vault model, entity-relationship data model, etc.\nConsumption layer: Data from the transformed layer are combined to form datasets that directly map to the end-user use case. The consumption layer typically involves joining and aggregating transformed layer tables to enable end-users to analyze historical performance. Business-specific metrics are often defined at this layer. We should ensure that a metric is only defined in one place.\nInterface layer [Optional]: Data in the consumption layer often confirms the data warehouse naming/data types, etc. However, the data presented to the end user should be easy to use and understand. An interface layer is often a view that acts as an interface between the warehouse table and the consumers of this table.\n\nMost frameworks/tools propose their version of the 3-hop architecture: 1. Apache Spark: Medallion architecture 2. dbt: Project Structure.\nShown below is our project’s 3-hop architecture:\n\n\n\nData architecture\n\n\nThe bronze, silver, gold, and interface layers correspond to the abovementioned raw, transformed, consumption, and interface layers. We have used dimensional modeling (with SCD2 for dim_customers) for our silver layer.\nFor a pipeline/transformation function/table, inputs are called upstream, and output users are called downstream consumers.\nAt larger companies, multiple teams work on different layers. A data ingestion team may bring the data into the bronze layer, and other teams may build their own silver and gold tables as necessary.",
    "crumbs": [
      "Data modeling is the process of getting data ready for analytics",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Most companies use the multi-hop architecture</span>"
    ]
  },
  {
    "objectID": "docker.html",
    "href": "docker.html",
    "title": "10  Docker recreates the same environment for your code in any machine",
    "section": "",
    "text": "10.1 Docker image is a blueprint for your container\nAn image is a blueprint to create your docker container. You can define the modules to install, variables to set, etc. Let’s consider our example:\nadd: screenshot of https://github.com/databricks/docker-spark-iceberg/blob/main/spark/Dockerfile\nThe commands in the docker image are run in order. Let’s go over the key commands:\nNote that this code is on github and maintained by the Iceverg/Spark community. This repo is used to build and host official spark-iceberg images on docker hub.\nA docker hub is an online repository where one can store and retrieve docker images and it is where most official systems docker images are stored. https://hub.docker.com/",
    "crumbs": [
      "Working in a team",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Docker recreates the same environment for your code in any machine</span>"
    ]
  },
  {
    "objectID": "docker.html#docker-concepts",
    "href": "docker.html#docker-concepts",
    "title": "10  Docker replicates the same code running environment in any machine",
    "section": "",
    "text": "Docker overview\n\n\n\n10.1.1 GitHub Repo Link\ndocker_for_data_engineers\nTo run the code in this post, you’ll need to install the following:\n\ngit version &gt;= 2.37.1\nDocker version &gt;= 20.10.17 and Docker compose v2 version &gt;= v2.10.2.\n\nWindows users: please setup WSL and a local Ubuntu Virtual machine following the instructions here. Install the above prerequisites on your Ubuntu terminal; if you have trouble installing docker, follow the steps here (only Step 1 is necessary).\nAll the commands shown below are to be run via the terminal (use the Ubuntu terminal for WSL users).\ngit clone https://github.com/josephmachado/docker_for_data_engineers.git\ncd efficient_data_processing_spark\n# Build our custom image based off of our local Dockerfile\ndocker compose build spark-master\n# start containers\ndocker compose up --build -d --scale spark-worker=2\ndocker ps # See list of running docker containers and their settings\n# stop containers\ndocker compose down\n\n\n10.1.2 2.1. Define the OS and its configurations with an image\nAn image is a blueprint to create your docker container. You can define the modules to install, variables to set, etc. Let’s consider our example:\n\n\n\nDocker image\n\n\nThe commands in the docker image (usually called Dockerfile) are run in order. Let’s go over the key commands:\n\nFROM: We need a base operating system on which to set our configurations. We can also use existing Docker images available at the Docker Hub and add our config on top of them. In our example, we use the official Delta Lake Docker image.\nCOPY: Copy is used to copy files or folders from our local filesystem to the image. The copy command is usually used when building the docker image to copy settings, static files, etc. In our example, we copy over the tpch-dbgen folder, which contains the logic to create tpch data. We also copy over our requirements.txt file and our entrypoint.sh file.\nRUN: Run is used to run a command in the shell terminal of your image. It is typically used to install libraries, create folders, etc.\nENV: This command sets the image’s environment variables. In our example, we set Spark environment variables.\nENTRYPOINT: The entrypoint command executes a script when the image starts. In our example, we use a script file (entrypoint.sh) to start spark master and worker nodes depending on the inputs given to the docker cli when starting a container from this image.\n\n\n\n10.1.3 2.2. Use the image to run containers\nContainers are the actual running virtual machines. We use images(Dockerfile) to create docker containers. We can spin up one or more containers from a given image.\n\n10.1.3.1 2.2.1. Communicate between containers and local OS\nTypically, with data infra, multiple containers run. When we want the container to communicate with them and the local operating system, we need to enable it via ports for http-based interactions. In our example, we ensure that Spark clusters can talk with each other and with the local operating system by defining ports in our docker-compose.yml file.\nUsing mounted volumes, we can also ensure that files are synced between the containers and the local operating system. In addition to syncing local files, we can also create docker volumes to sync files between our containers.\n\n\n\ndocker port\n\n\n\n\n10.1.3.2 2.2.2. Start containers with docker CLI or compose\nWe have seen how images are blueprints for containers. We need to use the docker clito start a container. With the docker cli, we can define the image to use, the container’s name, volume mounts, open ports, etc. For example, to start our Spark master container, we can use the following:\ndocker run -d \\\n  --name spark-master \\\n  --entrypoint ./entrypoint.sh \\\n  -p 4040:4040 \\\n  -p 9090:8080 \\\n  -p 7077:7077 \\\n  -v \"$(pwd)/capstone:/opt/spark/work-dir/capstone\" \\\n  -v \"$(pwd)/data-processing-spark:/opt/spark/work-dir/data-processing-spark\" \\\n  -v spark-logs:/opt/spark/spark-events \\\n  -v tpch-data:/opt/spark/tpch-dbgen \\\n  --env-file .env.spark \\\n  spark-image master\nHowever, with most data systems, we will need to ensure multiple systems are running. While we can use docker cli to do this, a better option is to use docker compose to orchestrate the different containers required. With docker compose, we can define all our settings in one file and ensure that they are started in the order we prefer.\nOur docker compose is defined here. With our docker compose defined, starting our containers is a simple command, as shown below:\ndocker compose up --build -d --scale spark-worker=2\nThe command will, by default, look for a file called docker-compose.yml in the directory in which it is run. We use the --build command to tell docker to use the image to create containers (without build, docker will use any previously created image, which may not reflect the changes you make to the Dockerfile). We also use --scale to ask docker to create two spark-worker containers. Note that we define our container names in the docker-compose.yml file.\nIn our setup, we have four systems specified in the docker compose yaml file; they are\n\nPostgres database: We use a Postgres database to simulate an upstream application database.\nSpark cluster: We create a spark cluster with a master and two workers, where the data is processed. The spark cluster also includes a history server, which displays the logs and resource utilization (Spark UI) for completed/failed spark applications.\nMinio: Minio is open-source software that has a fully compatible API with the AWS S3 cloud storage system. We use minio to replicate S3 locally.\ncreatebuckets: This is a short-lived docker container that creates the necessary folder structure in our minio storage system before turning off.\n\n\n\n\ndocker infra\n\n\n\n\n10.1.3.3 2.2.3. Executing commands in your docker container\nTypically, docker containers are meant to live for the duration of the command to be executed on them. In our use case, since we need our infrastructure to be always on, we use the entrypoint.sh to start Spark master and worker servers, which keeps the containers running.\nUsing the exec command, you can submit commands to be run in a specific container. For example, we can use the following to open a bash terminal in our spark-master container:\ndocker exec -ti spark-master bash\n# You will be in the master container bash shell\nexit # exit the container\nNote that the -ti indicates that this will be run in an interactive mode. As shown below, we can run a command without interactive mode and get an output.\ndocker exec spark-master echo hello\n# prints hello",
    "crumbs": [
      "Working in a team",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Docker recreates the same environment for your code in any machine</span>"
    ]
  },
  {
    "objectID": "git.html",
    "href": "git.html",
    "title": "11  [git] is a tool to help multiple engineers work with the same code",
    "section": "",
    "text": "11.1 Repo",
    "crumbs": [
      "Working in a team",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>[git] is a tool to help multiple engineers work with the same code</span>"
    ]
  },
  {
    "objectID": "git.html#clone",
    "href": "git.html#clone",
    "title": "11  [git] is a tool to help multiple engineers work with the same code",
    "section": "11.2 Clone",
    "text": "11.2 Clone",
    "crumbs": [
      "Working in a team",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>[git] is a tool to help multiple engineers work with the same code</span>"
    ]
  },
  {
    "objectID": "git.html#pull",
    "href": "git.html#pull",
    "title": "11  [git] is a tool to help multiple engineers work with the same code",
    "section": "11.3 Pull",
    "text": "11.3 Pull",
    "crumbs": [
      "Working in a team",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>[git] is a tool to help multiple engineers work with the same code</span>"
    ]
  },
  {
    "objectID": "git.html#checkout--b",
    "href": "git.html#checkout--b",
    "title": "11  [git] is a tool to help multiple engineers work with the same code",
    "section": "11.4 Checkout -b",
    "text": "11.4 Checkout -b",
    "crumbs": [
      "Working in a team",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>[git] is a tool to help multiple engineers work with the same code</span>"
    ]
  },
  {
    "objectID": "git.html#rebase",
    "href": "git.html#rebase",
    "title": "11  [git] is a tool to help multiple engineers work with the same code",
    "section": "11.5 rebase",
    "text": "11.5 rebase",
    "crumbs": [
      "Working in a team",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>[git] is a tool to help multiple engineers work with the same code</span>"
    ]
  },
  {
    "objectID": "git.html#merge-conflict",
    "href": "git.html#merge-conflict",
    "title": "11  [git] is a tool to help multiple engineers work with the same code",
    "section": "11.6 Merge conflict",
    "text": "11.6 Merge conflict",
    "crumbs": [
      "Working in a team",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>[git] is a tool to help multiple engineers work with the same code</span>"
    ]
  },
  {
    "objectID": "git.html#pull-request",
    "href": "git.html#pull-request",
    "title": "11  [git] is a tool to help multiple engineers work with the same code",
    "section": "11.7 Pull request",
    "text": "11.7 Pull request",
    "crumbs": [
      "Working in a team",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>[git] is a tool to help multiple engineers work with the same code</span>"
    ]
  },
  {
    "objectID": "working_in_a_team.html",
    "href": "working_in_a_team.html",
    "title": "Working in a team",
    "section": "",
    "text": "Data engineering has multiple components and its crucial that you have the same environment that runs in production is also aavailable to the dta engineers to test and develop in.\nIn this section we will see how docker is used to simulate the same running environment across multiple systems.",
    "crumbs": [
      "Working in a team"
    ]
  },
  {
    "objectID": "data_pipeline.html#schedulers-to-run-data-pipelines-at-specified-frequency",
    "href": "data_pipeline.html#schedulers-to-run-data-pipelines-at-specified-frequency",
    "title": "[Scheduler & Orchestrator] to define when and how to run your data pipelines",
    "section": "2.1. Schedulers to run data pipelines at specified frequency",
    "text": "2.1. Schedulers to run data pipelines at specified frequency\nBatch data pipelines will need to be run at specific intervals. The frequency required may be as simple as hourly, daily, weekly, monthly, etc., or complex, e.g., 2nd Tuesday of every month.\nAirflow uses a process called Scheduler that checks our DAGs(data pipeline) every minute to see if it needs to be started.\nIn addition to defining a schedule in cron format, Airflow also enables you to create custom timetables that you can reuse across your data pipelines.\nIn our code, we define our pipeline to run every day(ref link).\nwith DAG(\n    \"user_analytics_dag\",\n    description=\"A DAG to Pull user data and movie review data \\\n        to analyze their behaviour\",\n    schedule_interval=timedelta(days=1),\n    start_date=datetime(2023, 1, 1),\n    catchup=False,\n) as dag:",
    "crumbs": [
      "[Scheduler & Orchestrator] to define when and how to run your data pipelines"
    ]
  },
  {
    "objectID": "data_pipeline.html#orchestrators-to-define-the-order-of-execution-of-your-pipeline-tasks",
    "href": "data_pipeline.html#orchestrators-to-define-the-order-of-execution-of-your-pipeline-tasks",
    "title": "[Scheduler & Orchestrator] to define when and how to run your data pipelines",
    "section": "2.2. Orchestrators to define the order of execution of your pipeline tasks",
    "text": "2.2. Orchestrators to define the order of execution of your pipeline tasks\nWith complex data pipelines, we want parts of our pipelines to run in a specific order. For example, if your pipeline is pulling data from multiple independent sources, we would want them to run in parallel. Apache Airflow enables us to chain parts of our code to run in parallel or sequentially as needed.\n\n2.2.1. Define the order of execution of pipeline tasks with a DAG\nData pipelines are DAGs, i.e., they consist of a series of tasks that need to be run in a specified order without any cyclic dependencies.\nA Directed Acyclic Graph (DAG) is a directed graph with no directed cycles. It consists of vertices and edges, with each edge directed from one vertex to another, so following those directions will never form a closed loop. (ref: wiki)\n\n\n\nDAG\n\n\nIn data pipelines, we use the following terminology: 1. DAG: Represents an entire data pipeline. 2. Task: Individual node in a DAG. The tasks usually correspond to some data task. 3. Dependency: The edges between nodes represent the dependency between tasks. 1. Upstream: All the tasks that run before the task under consideration. 2. Downstream: All the tasks that run after the task under consideration.\nIn our example, if we consider the movie_classifier task, we can see its upstream and downstream tasks as shown below.\n\n\n\nDAG Dependency\n\n\nWith DAG, we can define dependencies, i.e., we can define when a task runs depending on upstream tasks (ref: trigger rules)\nWe can also set individual task-level settings, such as the number of retries per task and branch logic, where you can define logic to choose one or more tasks out of multiple tasks. We can dynamically create tasks based on your logic.\nIn our code we define our DAG using the &gt;&gt; syntax (ref link).\n# Define the tasks \ncreate_s3_bucket &gt;&gt; [user_purchase_to_s3, movie_review_to_s3]\n\nuser_purchase_to_s3 &gt;&gt; get_user_purchase_to_warehouse\n\nmovie_review_to_s3 &gt;&gt; movie_classifier &gt;&gt; get_movie_review_to_warehouse\n\n(\n    [get_user_purchase_to_warehouse, get_movie_review_to_warehouse]\n    &gt;&gt; get_user_behaviour_metric\n    &gt;&gt; gen_dashboard\n)\n\n\n2.2.2. Define where to run your code\nWhen we run our DAG, each task will be run individually. Airflow enables us to run our tasks in multiple ways:\n\nRun code in the same machine as your scheduler process with Local and sequential executor\nRun code in a task queue (i.e. a system that will run tasks in individual machines) with a celery executor.\nRun code as k8s pods with Kubernetes executor.\nWrite custom logic to run your tasks.\n\nSee this link for more details about running your tasks. In our project, we use the default SequentialExecutor, which is set up by default.\n\n\n2.2.3. Use operators to connect to popular services\nMost data pipelines involve talking to an external system, such as a cloud storage system (S3), data warehouse (Snowflake), or data processing system (Apache Spark).\nWhile we can write code to work with these external systems, Apache Airflow provides a robust set of Operators for most services we can reuse.\nIn our code, we use multiple Airflow operators to reduce the amount of code we have to write (ref code):\ncreate_s3_bucket = S3CreateBucketOperator(\n  task_id=\"create_s3_bucket\", bucket_name=user_analytics_bucket\n)\nmovie_review_to_s3 = LocalFilesystemToS3Operator(\n  task_id=\"movie_review_to_s3\",\n  filename=\"/opt/airflow/data/movie_review.csv\",\n  dest_key=\"raw/movie_review.csv\",\n  dest_bucket=user_analytics_bucket,\n  replace=True,\n)\n\nuser_purchase_to_s3 = SqlToS3Operator(\n  task_id=\"user_purchase_to_s3\",\n  sql_conn_id=\"postgres_default\",\n  query=\"select * from retail.user_purchase\",\n  s3_bucket=user_analytics_bucket,\n  s3_key=\"raw/user_purchase/user_purchase.csv\",\n  replace=True,\n)\nThe above code shows how we use Airflow operators to create S3 buckets and copy data from local files and a Postgres db into an S3 bucket. See here for a list of Airflow operators.",
    "crumbs": [
      "[Scheduler & Orchestrator] to define when and how to run your data pipelines"
    ]
  },
  {
    "objectID": "data_pipeline.html#observability-to-see-how-your-pipelines-are-running",
    "href": "data_pipeline.html#observability-to-see-how-your-pipelines-are-running",
    "title": "[Scheduler & Orchestrator] to define when and how to run your data pipelines",
    "section": "2.3. Observability to see how your pipelines are running",
    "text": "2.3. Observability to see how your pipelines are running\nWhen a data pipeline runs, all information is stored in a metadata db and as logs. The historical information allows us to observe your data pipelines’ current state and historical state.\nHere is a list of the tables used to store metadata about our pipelines:\n\n\n\nMetadata DB\n\n\nApache Airflow enables us to store logs in multiple locations (ref docs). In our project we store them locally in the file system, which can be accessed by clicking on a specific task -&gt; Logs as shown below.\n\n\n\nSpark logs\n\n\n\n2.3.1. See progress & historical information on UI\nWhen we run data pipelines, we can use a nice web UI to see the progress, failures, and other details. Powered by the metadata db and the logs, we can also see individual task logs and the inputs to a specific task, among others.\nThe web UI provides good visibility into our pipelines’ current and historical state.\n\n\n\nDAG logs\n\n\n\n\n\nTask inputs\n\n\n\n\n2.3.2. Analyze data pipeline performance with Web UI\nWe can see how pipelines have performed over time, inspect task run time, and see how long a task had to wait to get started. The performance metrics provide us with the necessary insights to optimize our systems.\n\n\n\nPerformance\n\n\n\n\n2.3.3. Re-run data pipelines via UI\nIn addition to seeing how our pipelines are running, we can manually trigger DAGs with custom inputs as necessary. The ability to trigger/re-run DAGs helps us quickly resolve one-off issues. See this link for information on triggering dags with UI and CLI.\n\n\n\nTriggers\n\n\n\n\n2.3.4. Reuse variables and connections across your pipelines\nApache Airflow also allows us to create and store variables and connection settings that can be reused across our data pipelines. In our code, we create variables using Airflow CLI here.\nOnce the connection/variables are set, we can see them in our UI:\n\n\n\nConnection\n\n\n\n\n2.3.5. Define who can view/edit your data pipelines with access control\nWhen managing Airflow used by multiple people, it can be beneficial to have some people have limited access to the data pipeline. For example, you want to avoid a stakeholder being able to stop or delete a DAG accidentally.\nSee this link for details on access control.",
    "crumbs": [
      "[Scheduler & Orchestrator] to define when and how to run your data pipelines"
    ]
  },
  {
    "objectID": "so.html",
    "href": "so.html",
    "title": "11  Airflow is both a scheduler and an orchestrator",
    "section": "",
    "text": "11.1 Schedulers run data pipelines at specified frequency\nBatch data pipelines will need to be run at specific intervals. The frequency required may be as simple as hourly, daily, weekly, monthly, etc., or complex, e.g., 2nd Tuesday of every month.\nAirflow uses a process called Scheduler that checks our DAGs(data pipeline) every minute to see if it needs to be started.\nIn addition to defining a schedule in cron format, Airflow also enables you to create custom timetables that you can reuse across your data pipelines.\nIn our code, we define our pipeline to run every day(ref link).",
    "crumbs": [
      "Scheduler defines when & Orchestrator defines how to, run your data pipelines",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Airflow is both a scheduler and an orchestrator</span>"
    ]
  },
  {
    "objectID": "so.html#schedulers-to-run-data-pipelines-at-specified-frequency",
    "href": "so.html#schedulers-to-run-data-pipelines-at-specified-frequency",
    "title": "11  Airflow is both a scheduler and an orchestrator",
    "section": "",
    "text": "with DAG(\n    \"user_analytics_dag\",\n    description=\"A DAG to Pull user data and movie review data \\\n        to analyze their behaviour\",\n    schedule_interval=timedelta(days=1),\n    start_date=datetime(2023, 1, 1),\n    catchup=False,\n) as dag:",
    "crumbs": [
      "Scheduler defines when & Orchestrator defines how to, run your data pipelines",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Airflow is both a scheduler and an orchestrator</span>"
    ]
  },
  {
    "objectID": "so.html#orchestrators-to-define-the-order-of-execution-of-your-pipeline-tasks",
    "href": "so.html#orchestrators-to-define-the-order-of-execution-of-your-pipeline-tasks",
    "title": "11  Airflow is both a scheduler and an orchestrator",
    "section": "11.2 Orchestrators to define the order of execution of your pipeline tasks",
    "text": "11.2 Orchestrators to define the order of execution of your pipeline tasks\nWith complex data pipelines, we want parts of our pipelines to run in a specific order. For example, if your pipeline is pulling data from multiple independent sources, we would want them to run in parallel. Apache Airflow enables us to chain parts of our code to run in parallel or sequentially as needed.\n\n11.2.1 Define the order of execution of pipeline tasks with a DAG\nData pipelines are DAGs, i.e., they consist of a series of tasks that need to be run in a specified order without any cyclic dependencies.\nA Directed Acyclic Graph (DAG) is a directed graph with no directed cycles. It consists of vertices and edges, with each edge directed from one vertex to another, so following those directions will never form a closed loop. (ref: wiki)\n\n\n\nDAG\n\n\nIn data pipelines, we use the following terminology: 1. DAG: Represents an entire data pipeline. 2. Task: Individual node in a DAG. The tasks usually correspond to some data task. 3. Dependency: The edges between nodes represent the dependency between tasks. 1. Upstream: All the tasks that run before the task under consideration. 2. Downstream: All the tasks that run after the task under consideration.\nIn our example, if we consider the movie_classifier task, we can see its upstream and downstream tasks as shown below.\n\n\n\nDAG Dependency\n\n\nWith DAG, we can define dependencies, i.e., we can define when a task runs depending on upstream tasks (ref: trigger rules)\nWe can also set individual task-level settings, such as the number of retries per task and branch logic, where you can define logic to choose one or more tasks out of multiple tasks. We can dynamically create tasks based on your logic.\nIn our code we define our DAG using the &gt;&gt; syntax (ref link).\n# Define the tasks \ncreate_s3_bucket &gt;&gt; [user_purchase_to_s3, movie_review_to_s3]\n\nuser_purchase_to_s3 &gt;&gt; get_user_purchase_to_warehouse\n\nmovie_review_to_s3 &gt;&gt; movie_classifier &gt;&gt; get_movie_review_to_warehouse\n\n(\n    [get_user_purchase_to_warehouse, get_movie_review_to_warehouse]\n    &gt;&gt; get_user_behaviour_metric\n    &gt;&gt; gen_dashboard\n)\n\n\n11.2.2 Define where to run your code\nWhen we run our DAG, each task will be run individually. Airflow enables us to run our tasks in multiple ways:\n\nRun code in the same machine as your scheduler process with Local and sequential executor\nRun code in a task queue (i.e. a system that will run tasks in individual machines) with a celery executor.\nRun code as k8s pods with Kubernetes executor.\nWrite custom logic to run your tasks.\n\nSee this link for more details about running your tasks. In our project, we use the default SequentialExecutor, which is set up by default.\n\n\n11.2.3 Use operators to connect to popular services\nMost data pipelines involve talking to an external system, such as a cloud storage system (S3), data warehouse (Snowflake), or data processing system (Apache Spark).\nWhile we can write code to work with these external systems, Apache Airflow provides a robust set of Operators for most services we can reuse.\nIn our code, we use multiple Airflow operators to reduce the amount of code we have to write (ref code):\ncreate_s3_bucket = S3CreateBucketOperator(\n  task_id=\"create_s3_bucket\", bucket_name=user_analytics_bucket\n)\nmovie_review_to_s3 = LocalFilesystemToS3Operator(\n  task_id=\"movie_review_to_s3\",\n  filename=\"/opt/airflow/data/movie_review.csv\",\n  dest_key=\"raw/movie_review.csv\",\n  dest_bucket=user_analytics_bucket,\n  replace=True,\n)\n\nuser_purchase_to_s3 = SqlToS3Operator(\n  task_id=\"user_purchase_to_s3\",\n  sql_conn_id=\"postgres_default\",\n  query=\"select * from retail.user_purchase\",\n  s3_bucket=user_analytics_bucket,\n  s3_key=\"raw/user_purchase/user_purchase.csv\",\n  replace=True,\n)\nThe above code shows how we use Airflow operators to create S3 buckets and copy data from local files and a Postgres db into an S3 bucket. See here for a list of Airflow operators.",
    "crumbs": [
      "Scheduler defines when & Orchestrator defines how to, run your data pipelines",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Airflow is both a scheduler and an orchestrator</span>"
    ]
  },
  {
    "objectID": "so.html#observability-to-see-how-your-pipelines-are-running",
    "href": "so.html#observability-to-see-how-your-pipelines-are-running",
    "title": "11  Airflow is both a scheduler and an orchestrator",
    "section": "11.3 Observability to see how your pipelines are running",
    "text": "11.3 Observability to see how your pipelines are running\nWhen a data pipeline runs, all information is stored in a metadata db and as logs. The historical information allows us to observe your data pipelines’ current state and historical state.\nHere is a list of the tables used to store metadata about our pipelines:\n\n\n\nMetadata DB\n\n\nApache Airflow enables us to store logs in multiple locations (ref docs). In our project we store them locally in the file system, which can be accessed by clicking on a specific task -&gt; Logs as shown below.\n\n\n\nSpark logs\n\n\n\n11.3.1 See progress & historical information on UI\nWhen we run data pipelines, we can use a nice web UI to see the progress, failures, and other details. Powered by the metadata db and the logs, we can also see individual task logs and the inputs to a specific task, among others.\nThe web UI provides good visibility into our pipelines’ current and historical state.\n\n\n\nDAG logs\n\n\n\n\n\nTask inputs\n\n\n\n\n11.3.2 Analyze data pipeline performance with Web UI\nWe can see how pipelines have performed over time, inspect task run time, and see how long a task had to wait to get started. The performance metrics provide us with the necessary insights to optimize our systems.\n\n\n\nPerformance\n\n\n\n\n11.3.3 Re-run data pipelines via UI\nIn addition to seeing how our pipelines are running, we can manually trigger DAGs with custom inputs as necessary. The ability to trigger/re-run DAGs helps us quickly resolve one-off issues. See this link for information on triggering dags with UI and CLI.\n\n\n\nTriggers\n\n\n\n\n11.3.4 Reuse variables and connections across your pipelines\nApache Airflow also allows us to create and store variables and connection settings that can be reused across our data pipelines. In our code, we create variables using Airflow CLI here.\nOnce the connection/variables are set, we can see them in our UI:\n\n\n\nConnection\n\n\n\n\n11.3.5 Define who can view/edit your data pipelines with access control\nWhen managing Airflow used by multiple people, it can be beneficial to have some people have limited access to the data pipeline. For example, you want to avoid a stakeholder being able to stop or delete a DAG accidentally.\nSee this link for details on access control.",
    "crumbs": [
      "Scheduler defines when & Orchestrator defines how to, run your data pipelines",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Airflow is both a scheduler and an orchestrator</span>"
    ]
  },
  {
    "objectID": "dbt.html",
    "href": "dbt.html",
    "title": "12  dbt-core is an orchestrator that makes managing pipelines simpler",
    "section": "",
    "text": "12.1 Create tables with select sql files\nIn dbt every .sql file has a select statement and is created as a data model (usually a table or a view). The select statement defines the data schema of the data model. The name of the .sql file defines the name of the data model.\nLet’s take a look at one of our silver tables (add: link)\nWe can see how the final select query is created as a data model. Note the ref function refers to another table that is defined by the folder path and file name (which is also its data model name).\nThe setting that defines which data models should be tables/views/materialized views, etc will be defined in the dbt_project.yml file (add: link).\nWhen you run the dbt run command all your models will be created as tables.",
    "crumbs": [
      "Scheduler defines when & Orchestrator defines how to, run your data pipelines",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>dbt-core is an orchestrator that makes managing pipelines simpler</span>"
    ]
  },
  {
    "objectID": "dbt.html#project",
    "href": "dbt.html#project",
    "title": "12  dbt-core is an orchestrator that makes managing pipelines simpler",
    "section": "",
    "text": "Data Flow\n\n\n\n\n12.1.1 3.1. Project Demo\nHere is a demo of how to run this on CodeSpaces(click on the image below to open video on youtube): \n\n12.1.1.1 Prerequisites\n\npython ^3.11\ngit\n\nClone the git repo as shown below:\ngit clone https://github.com/josephmachado/simple_dbt_project.git\ncd simple_dbt_project\nSetup python virtual environment as shown below:\nrm -rf myenv\n# set up venv and run dbt\npython -m venv myenv\nsource myenv/bin/activate\npip install -r requirements.txt\nRun dbt commands as shown below:\ndbt clean\ndbt deps\ndbt snapshot\ndbt run \ndbt test\ndbt docs generate\ndbt docs serve\nGo to http://localhost:8080 to see the dbt documentation. If you are running this on GitHub CodeSpaces the port 8080 will be open automatically. Press Ctrl + c to stop the document server.\nIn our project folder you will see the following folders.\n.\n├── analysis\n├── data\n├── macros\n├── models\n│   ├── marts\n│   │   ├── core\n│   │   └── marketing\n│   └── staging\n├── snapshots\n└── tests\n\nanalysis: Any .sql files found in this folder will be compiled to raw sql when you run dbt compile. They will not be run by dbt but can be copied into any tool of choice.\ndata: We can store raw data that we want to be loaded into our data warehouse. This is typically used to store small mapping data.\nmacros: Dbt allows users to create macros, which are sql based functions. These macros can be reused across our project.\n\nWe will go over the models, snapshots, and tests folders in the below sections.\nNote: The project repository has advanced features which are explained in the uplevel dbt workflow article. It is recommended to read this tutorial first before diving into the advanced features specified in the uplevel dbt workflow article article.\n\n\n\n12.1.2 3.2. Configurations and connections\nLet’s set the warehouse connections and project settings.\n\n12.1.2.1 3.2.1. profiles.yml\nDbt requires a profiles.yml file to contain data warehouse connection details. We have defined the warehouse connection details at ./profiles.yml.\nThe target variable defines the environment. The default is dev. We can have multiple targets, which can be specified when running dbt commands.\nThe profile is sde_dbt_tutorial. The profiles.yml file can contain multiple profiles for when you have more than one dbt project.\n\n\n12.1.2.2 3.2.2. dbt_project.yml\nIn this file, you can define the profile to be used and the paths for different types of files (see *-paths).\nMaterialization is a variable that controls how dbt creates a model. By default, every model will be a view. This can be overridden in dbt_project.yml. We have set the models under models/marts/core/ to materialize as tables.\n# Configuring models\nmodels:\n    sde_dbt_tutorial:\n        # Applies to all files under models/marts/core/\n        marts:\n            core:\n                materialized: table\n\n\n\n12.1.3 3.3 Data flow\nWe will see how the customer_orders table is created from the source tables. These transformations follow warehouse and dbt best practices.\n\n12.1.3.1 3.3.1. Source\nSource tables refer to tables loaded into the warehouse by an EL process. Since dbt did not create them, we have to define them. This definition enables referring to the source tables using the source function. For e.g. { source('warehouse', 'orders') } refers to the warehouse.orders table. We can also define tests to ensure that the source data is clean.\n\nSource definition: /models/staging/src_eltool.yml\nTest definitions: /models/staging/src_eltool.yml\n\n\n\n12.1.3.2 3.3.2. Snapshots\nA business entity’s attributes change over time. These changes should be captured in our data warehouse. E.g. a user may move to a new address. This is called slowly changing dimensions, in data warehouse modeling.\nRead this article to understand the importance of storing historical data changes, and what slowly changing dimensions are.\nDbt allows us to easily create these slowly changing dimension tables (type 2) using the snapshot feature. When creating a snapshot, we need to define the database, schema, strategy, and columns to identify row updates.\ndbt snapshot\n# alternative run the command 'just snapshot'\ndbt creates a snapshot table on the first run, and on consecutive runs will check for changed values and update older rows. We simulate this as shown below\n# Remove header from ./raw_data/customers_new.csv\n# and append it to ./raw_data/customers.csv\necho \"\" &gt;&gt; ./raw_data/customers.csv\ntail -n +2 ./raw_data/customer_new.csv &gt;&gt; ./raw_data/customers.csv\nRun the snapshot command again\ndbt snapshot\nRaw data (raw_layer.customer) & Snapshot table (snapshots.customers_snapshot) \nThe row with zipcode 59655 had its dbt_valid_to column updated. The dbt from and to columns represent the time range when the data in that row is representative of customer 82.\n\nModel definition: /snapshots/customers.sql\n\n\n\n12.1.3.3 3.3.3. Staging\nThe staging area is where raw data is cast into correct data types, given consistent column names, and prepared to be transformed into models used by end-users.\nYou might have noticed the eltool in the staging model names. If we use Fivetran to EL data, our models will be named stg_fivetran__orders and the YAML file will be stg_fivetran.yml.\nIn stg_eltool__customers.sql we use the ref function instead of the source function because this model is derived from the snapshot model. In dbt, we can use the ref function to refer to any models created by dbt.\n\nTest definitions: /models/staging/stg_eltool.yml\nModel definitions: /models/staging/stg_eltool__customers.sql,stg_eltool__orders.sql,stg_eltool__state.sql\n\n\n\n12.1.3.4 3.3.4. Marts\nMarts consist of the core tables for end-users and business vertical-specific tables. In our example, we have a marketing department-specific folder to defined the model requested by marketing.\n\n12.1.3.4.1 3.3.4.1. Core\nThe core defines the fact and dimension models to be used by end-users. The fact and dimension models are materialized as tables, for performance on frequent use. The fact and dimension models are based on kimball dimensional model.\n\nTest definitions: /models/marts/core/core.yml\nModel definitions: /models/staging/dim_customers,fct_orders.sql\n\nDbt offers four generic tests, unique, not_null, accepted_values, and relationships. We can create one-off (aka bespoke) tests under the Tests folder. Let’s create a sql test script that checks if any of the customer rows were duplicated or missed. If the query returns one or more records, the tests will fail. Understanding this script is left as an exercise for the reader.\n\nOne-off test: /tests/assert_customer_dimension_has_no_row_loss.sql\n\n\n\n12.1.3.4.2 3.3.4.2. Marketing\nIn this section, we define the models for marketing end users. A project can have multiple business verticals. Having one folder per business vertical provides an easy way to organize the models.\n\nTest definitions: /models/marts/marketing/marketing.yml\nModel definitions: /models/marts/marketing/customer_orders.sql\n\n\n\n\n\n12.1.4 3.4. dbt run\nWe have the necessary model definitions in place. Let’s create the models.\ndbt snapshot # just snapshot\ndbt run \n# Finished running 5 view models, 2 table models, 2 hooks in 0 hours 0 minutes and 3.22 seconds (3.22s).\nThe stg_eltool__customers model requires snapshots.customers_snapshot model. But snapshots are not created on dbt run ,so we run dbt snapshot first.\nOur staging and marketing models are as materialized views, and the two core models are materialized as tables.\nThe snapshot command should be executed independently from the run command to keep snapshot tables up to date. If snapshot tables are stale, the models will be incorrect. There is snapshot freshness monitoring in dbt cloud UI.\n\n\n12.1.5 3.5. dbt test\nWith the models defined, we can run tests on them. Note that, unlike standard testing, these tests run after the data has been processed. You can run tests as shown below.\ndbt test # or run \"just test\"\n# Finished running 14 tests...\n\n\n12.1.6 3.6. dbt docs\nOne of the powerful features of dbt is its docs. To generate documentation and serve them, run the following commands:\ndbt docs generate\ndbt docs serve\nYou can visit http://localhost:8080 to see the documentation. Navigate to customer_orders within the sde_dbt_tutorial project in the left pane. Click on the view lineage graph icon on the lower right side. The lineage graph shows the dependencies of a model. You can also see the tests defined, descriptions (set in the corresponding YAML file), and the compiled sql statements.\n\n\n\nour project structure\n\n\n\n\n12.1.7 3.7. Scheduling\nWe have seen how to create snapshots, models, run tests and generate documentation. These are all commands run via the cli. Dbt compiles the models into sql queries under the target folder (not part of git repo) and executes them on the data warehouse.\nTo schedule dbt runs, snapshots, and tests we need to use a scheduler. Dbt cloud is a great option to do easy scheduling. Checkout this article to learn how to schedule jobs with dbt cloud. The dbt commands can be run by other popular schedulers like cron, Airflow, Dagster, etc.",
    "crumbs": [
      "Scheduler defines when & Orchestrator defines how to, run your data pipelines",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>dbt-core is an orchestrator that makes managing pipelines simpler</span>"
    ]
  },
  {
    "objectID": "sql_basics.html#the-hierarchy-of-data-organization-is-a-database-schema-table-columns-and-data-types",
    "href": "sql_basics.html#the-hierarchy-of-data-organization-is-a-database-schema-table-columns-and-data-types",
    "title": "1  Read data, Combine tables, & aggregate numbers to understand business performance",
    "section": "",
    "text": "SHOW catalogs;\n\nSHOW schemas IN tpch;\n\nSHOW TABLES IN tpch.tiny;\n\nDESCRIBE tpch.tiny.lineitem;\n\nUSE tpch.tiny;\n\nDESCRIBE lineitem;",
    "crumbs": [
      "Use SQL to transform data",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Read data, Combine tables, & aggregate numbers to understand business performance</span>"
    ]
  },
  {
    "objectID": "sql_basics.html#in-spark-a-catalog-can-have-multiple-schemas-schemas-can-have-multiple-tables",
    "href": "sql_basics.html#in-spark-a-catalog-can-have-multiple-schemas-schemas-can-have-multiple-tables",
    "title": "1  Read data, Combine tables, & aggregate numbers to understand business performance",
    "section": "",
    "text": "SHOW catalogs;\n\nSHOW schemas IN tpch;\n\nSHOW TABLES IN tpch.tiny;\n\nDESCRIBE tpch.tiny.lineitem;\n\nUSE tpch.tiny;\n\nDESCRIBE lineitem;",
    "crumbs": [
      "Use SQL to transform data",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Read data, Combine tables, & aggregate numbers to understand business performance</span>"
    ]
  },
  {
    "objectID": "sql_basics.html#use-selectfrom-limit-where-order-by-to-read-the-required-data",
    "href": "sql_basics.html#use-selectfrom-limit-where-order-by-to-read-the-required-data",
    "title": "2  Read data, Combine tables, & aggregate numbers to understand business performance",
    "section": "2.2 Use SELECT…FROM, LIMIT, WHERE, & ORDER BY to read the required data",
    "text": "2.2 Use SELECT…FROM, LIMIT, WHERE, & ORDER BY to read the required data\nThe most common use for querying is to read data in our tables. We can do this using a SELECT ... FROM statement, as shown below.\nuse prod.db;\n\n-- use * to specify all columns\nSELECT\n  *\nFROM\n  orders\nLIMIT\n  4\n\n-- use column names to only read data from those columns\nSELECT\n  o_orderkey,\n  o_totalprice\nFROM\n  orders\nLIMIT\n  4\nHowever, running a SELECT ... FROM statement can cause issues when the data set is extensive. If you want to look at the data, use LIMIT n to tell Trino only to get n number of rows.\nWe can use the’ WHERE’ clause if we want to get the rows that match specific criteria. We can specify one or more filters within the’ WHERE’ clause. The WHERE clause with more than one filter can use combinations of AND and OR criteria to combine the filter criteria, as shown below.\n\n-- all customer rows that have c_nationkey = 20\nSELECT\n  *\nFROM\n  customer\nWHERE\n  c_nationkey = 20\nLIMIT\n  10;\n\n\n-- all customer rows that have c_nationkey = 20 and c_acctbal &gt; 1000\nSELECT\n  *\nFROM\n  customer\nWHERE\n  c_nationkey = 20\n  AND c_acctbal &gt; 1000\nLIMIT\n  10;\n\n\n-- all customer rows that have c_nationkey = 20 or c_acctbal &gt; 1000\nSELECT\n  *\nFROM\n  customer\nWHERE\n  c_nationkey = 20\n  OR c_acctbal &gt; 1000\nLIMIT\n  10;\n\n\n-- all customer rows that have (c_nationkey = 20 and c_acctbal &gt; 1000) or rows that have c_nationkey = 11\nSELECT\n  *\nFROM\n  customer\nWHERE\n  (\n    c_nationkey = 20\n    AND c_acctbal &gt; 1000\n  )\n  OR c_nationkey = 11\nLIMIT\n  10;\nWe can combine multiple filter clauses, as seen above. We have seen examples of equals (=) and greater than (&gt;) conditional operators. There are 6 conditional operators, they are\n\n&lt; Less than\n&gt; Greater than\n&lt;= Less than or equal to\n&gt;= Greater than or equal to\n= Equal\n&lt;&gt; and != both represent Not equal (some DBs only support one of these)\n\nAdditionally, for string types, we can make pattern matching with like condition. In a like condition, a _ means any single character, and % means zero or more characters, for example.\nUSE tpch.tiny;\n\n-- all customer rows where the name has a 381 in it\nSELECT * FROM customer WHERE name LIKE '%381%';\n\n-- all customer rows where the name ends with a 381\nSELECT * FROM customer WHERE name LIKE '%381';\n\n-- all customer rows where the name starts with a 381\nSELECT * FROM customer WHERE name LIKE '381%';\n\n-- all customer rows where the name has a combination of any character and 9 and 1\nSELECT * FROM customer WHERE name LIKE '%_91%';\nWe can also filter for more than one value using IN and NOT IN.\nUSE tpch.tiny;\n\n-- all customer rows which have nationkey = 10 or nationkey = 20\nSELECT * FROM customer WHERE nationkey IN (10,20);\n\n-- all customer rows which have do not have nationkey as 10 or 20\nSELECT * FROM customer WHERE nationkey NOT IN (10,20);\nWe can get the number of rows in a table using count(*) as shown below.\nUSE tpch.tiny;\n\nSELECT COUNT(*) FROM customer; -- 1500\nSELECT COUNT(*) FROM lineitem; -- 60175\nIf we want to get the rows sorted by values in a specific column, we use ORDER BY, for example.\nUSE tpch.tiny;\n\n-- Will show the first ten customer records with the lowest custkey\n-- rows are ordered in ASC order by default\nSELECT * FROM orders ORDER BY custkey LIMIT 10; \n\n-- Will show the first ten customer's records with the highest custkey\nSELECT * FROM orders ORDER BY custkey DESC LIMIT 10;",
    "crumbs": [
      "Use SQL to transform data",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Read data, Combine tables, & aggregate numbers to understand business performance</span>"
    ]
  },
  {
    "objectID": "sql_basics.html#combine-data-from-multiple-tables-using-joins",
    "href": "sql_basics.html#combine-data-from-multiple-tables-using-joins",
    "title": "2  Read data, Combine tables, & aggregate numbers to understand business performance",
    "section": "2.3 Combine data from multiple tables using JOINs",
    "text": "2.3 Combine data from multiple tables using JOINs\nWe can combine data from multiple tables using joins. When we write a join query, we have a format as shown below.\nSELECT\n    a.*\nFROM\n    table_a a -- LEFT table a\n    JOIN table_b b -- RIGHT table b\n    ON a.id = b.id\nThe table specified first (table_a) is the left table, whereas the table established second is the right table. When we have multiple tables joined, we consider the joined dataset from the first two tables as the left table and the third table as the right table (The DB optimizes our join for performance).\nSELECT\n    a.*\nFROM\n    table_a a -- LEFT table a\n    JOIN table_b b -- RIGHT table b\n    ON a.id = b.id\n    JOIN table_c c -- LEFT table is the joined data from table_a & table_b, right table is table_c\n    ON a.c_id = c.id\nThere are five main types of joins, they are:\n\n2.3.1 1. Inner join (default): Get rows with same join keys from both tables\nUSE tpch.tiny;\n\nSELECT\n    o.orderkey,\n    l.orderkey\nFROM\n    orders o\n    JOIN lineitem l ON o.orderkey = l.orderkey\n    AND o.orderdate BETWEEN l.shipdate - INTERVAL '5' DAY\n    AND l.shipdate + INTERVAL '5' DAY\nLIMIT\n    100;\n\nSELECT\n    COUNT(o.orderkey) AS order_rows_count,\n    COUNT(l.orderkey) AS lineitem_rows_count\nFROM\n    orders o\n    JOIN lineitem l ON o.orderkey = l.orderkey\n    AND o.orderdate BETWEEN l.shipdate - INTERVAL '5' DAY\n    AND l.shipdate + INTERVAL '5' DAY;\n-- 2477, 2477\nNote: JOIN defaults to INNER JOIN`.\nThe output will have rows from orders and lineitem that found at least one matching row from the other table with the specified join condition (same orderkey and orderdate within ship date +/- 5 days).\nWe can also see that 2,477 rows from orders and lineitem tables matched.\n\n\n2.3.2 2. Left outer join (aka left join): Get all rows from the left table and only matching rows from the right table.\nUSE tpch.tiny;\n\nSELECT\n    o.orderkey,\n    l.orderkey\nFROM\n    orders o\n    LEFT JOIN lineitem l ON o.orderkey = l.orderkey\n    AND o.orderdate BETWEEN l.shipdate - INTERVAL '5' DAY\n    AND l.shipdate + INTERVAL '5' DAY\nLIMIT\n    100;\n\nSELECT\n    COUNT(o.orderkey) AS order_rows_count,\n    COUNT(l.orderkey) AS lineitem_rows_count\nFROM\n    orders o\n    LEFT JOIN lineitem l ON o.orderkey = l.orderkey\n    AND o.orderdate BETWEEN l.shipdate - INTERVAL '5' DAY\n    AND l.shipdate + INTERVAL '5' DAY;\n-- 15197, 2477\nThe output will have all the rows from orders and the rows from lineitem that were able to find at least one matching row from the orders table with the specified join condition (same orderkey and orderdate within ship date +/- 5 days).\nWe can also see that the number of rows from the orders table is 15,197 & from the lineitem table is 2,477. The number of rows in orders is 15000, but the join condition produces 15197 since some orders match with multiple lineitems.\n\n\n2.3.3 3. Right outer join (aka right join): Get matching rows from the left and all rows from the right table.\nUSE tpch.tiny;\n\nSELECT\n    o.orderkey,\n    l.orderkey\nFROM\n    orders o\n    RIGHT JOIN lineitem l ON o.orderkey = l.orderkey\n    AND o.orderdate BETWEEN l.shipdate - INTERVAL '5' DAY\n    AND l.shipdate + INTERVAL '5' DAY\nLIMIT\n    100;\n\nSELECT\n    COUNT(o.orderkey) AS order_rows_count,\n    COUNT(l.orderkey) AS lineitem_rows_count\nFROM\n    orders o\n    RIGHT JOIN lineitem l ON o.orderkey = l.orderkey\n    AND o.orderdate BETWEEN l.shipdate - INTERVAL '5' DAY\n    AND l.shipdate + INTERVAL '5' DAY;\n-- 2477, 60175\nThe output will have the rows from orders that found at least one matching row from the lineitem table with the specified join condition (same orderkey and orderdate within ship date +/- 5 days) and all the rows from the lineitem table.\nWe can also see that the number of rows from the orders table is 15,197 & from the lineitem table is 2,477.\n\n\n2.3.4 4. Full outer join: Get matched and un-matched rows from both the tables.\nUSE tpch.tiny;\n\nSELECT\n    o.orderkey,\n    l.orderkey\nFROM\n    orders o\n    FULL OUTER JOIN lineitem l ON o.orderkey = l.orderkey\n    AND o.orderdate BETWEEN l.shipdate - INTERVAL '5' DAY\n    AND l.shipdate + INTERVAL '5' DAY\nLIMIT\n    100;\n\nSELECT\n    COUNT(o.orderkey) AS order_rows_count,\n    COUNT(l.orderkey) AS lineitem_rows_count\nFROM\n    orders o\n    FULL OUTER JOIN lineitem l ON o.orderkey = l.orderkey\n    AND o.orderdate BETWEEN l.shipdate - INTERVAL '5' DAY\n    AND l.shipdate + INTERVAL '5' DAY;\n-- 15197, 60175\nThe output will have all the rows from orders that found at least one matching row from the lineitem table with the specified join condition (same orderkey and orderdate within ship date +/- 5 days) and all the rows from the lineitem table.\nWe can also see that the number of rows from the orders table is 15,197 & from the lineitem table is 2,477.\n\n\n2.3.5 5. Cross join: Join every row in left table with every row in the right table\nUSE tpch.tiny;\n\nSELECT\n    n.name AS nation_name,\n    r.name AS region_name\nFROM\n    nation n\n    CROSS JOIN region r;\nThe output will have every row of the nation joined with every row of the region. There are 25 nations and five regions, leading to 125 rows in our result from the cross-join.\n\n\nThere are cases where we will need to join a table with itself, called a SELF-join. Let’s consider an example.\n\nFor every customer order, get the order placed earlier in the same week (Sunday - Saturday, not the previous seven days). Only show customer orders that have at least one such order.\n\nUSE tpch.tiny;\n\nSELECT\n    o1.custkey,\n    o1.totalprice,\n    o1.orderdate,\n    o2.totalprice,\n    o2.orderdate\nFROM\n    orders o1\n    JOIN orders o2 ON o1.custkey = o2.custkey\n    AND year(o1.orderdate) = year(o2.orderdate)\n    AND week(o1.orderdate) = week(o2.orderdate)\nWHERE\n    o1.orderkey != o2.orderkey\nLIMIT\n    10;",
    "crumbs": [
      "Use SQL to transform data",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Read data, Combine tables, & aggregate numbers to understand business performance</span>"
    ]
  },
  {
    "objectID": "sql_basics.html#use-group-by-to-aggregate-numbers",
    "href": "sql_basics.html#use-group-by-to-aggregate-numbers",
    "title": "1  Read data, Combine tables, & aggregate numbers to understand business performance",
    "section": "1.4 Use GROUP BY to aggregate numbers",
    "text": "1.4 Use GROUP BY to aggregate numbers\nMost analytical queries require calculating metrics that involve combining data from multiple rows. GROUP BY allows us to perform aggregate calculations on data from a set of rows recognized by values of specified column(s). For example:\n\nCreate a report that shows the number of orders per orderpriority segment.\n\nUSE tpch.tiny;\n\nSELECT\n    orderpriority,\n    count(*) AS num_orders\nFROM\n    orders\nGROUP BY\n    orderpriority;\nIn the above query, we group the data by orderpriority, and the calculation count(*) will be applied to the rows having a specific orderpriority value.\nThe calculations allowed are typically SUM/MIN/MAX/AVG/COUNT. However, some databases have more complex aggregate functions; check your DB documentation.",
    "crumbs": [
      "Use SQL to transform data",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Read data, Combine tables, & aggregate numbers to understand business performance</span>"
    ]
  },
  {
    "objectID": "sql_basics.html#sub-query-use-query-instead-of-a-table",
    "href": "sql_basics.html#sub-query-use-query-instead-of-a-table",
    "title": "2  Read data, Combine tables, & aggregate numbers to understand business performance",
    "section": "2.7 Sub-query: Use query instead of a table",
    "text": "2.7 Sub-query: Use query instead of a table\nWhen we want to use the result of a query as a table in another query, we use subqueries. Let’s consider an example:\n\nCreate a report that shows the nation, how many items it supplied (by suppliers in that nation), and how many items it purchased (by customers in that nation).\n\nUSE tpch.tiny;\n\nSELECT\n    n.name AS nation_name,\n    s.quantity AS supplied_items_quantity,\n    c.quantity AS purchased_items_quantity\nFROM\n    nation n\n    LEFT JOIN (\n        SELECT\n            n.nationkey,\n            sum(l.quantity) AS quantity\n        FROM\n            lineitem l\n            JOIN supplier s ON l.suppkey = s.suppkey\n            JOIN nation n ON s.nationkey = n.nationkey\n        GROUP BY\n            n.nationkey\n    ) s ON n.nationkey = s.nationkey\n    LEFT JOIN (\n        SELECT\n            n.nationkey,\n            sum(l.quantity) AS quantity\n        FROM\n            lineitem l\n            JOIN orders o ON l.orderkey = o.orderkey\n            JOIN customer c ON o.custkey = c.custkey\n            JOIN nation n ON c.nationkey = n.nationkey\n        GROUP BY\n            n.nationkey\n    ) c ON n.nationkey = c.nationkey;\nIn the above query, we can see that there are two sub-queries, one to calculate the quantity supplied by a nation and the other to calculate the quantity purchased by the customers of a nation.",
    "crumbs": [
      "Use SQL to transform data",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Read data, Combine tables, & aggregate numbers to understand business performance</span>"
    ]
  },
  {
    "objectID": "sql_basics.html#a-spark-catalog-can-have-multiple-schemas-schemas-can-have-multiple-tables",
    "href": "sql_basics.html#a-spark-catalog-can-have-multiple-schemas-schemas-can-have-multiple-tables",
    "title": "2  Read data, Combine tables, & aggregate numbers to understand business performance",
    "section": "",
    "text": "show catalogs;\nshow schemas IN demo; -- Catalog -&gt; schema\nshow schemas IN prod; -- schema -&gt; namespace\nshow tables IN prod.db -- namespace -&gt; Table\n\nDESCRIBE lineitem;\nDESCRIBE EXTENDED lineitem;",
    "crumbs": [
      "Use SQL to transform data",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Read data, Combine tables, & aggregate numbers to understand business performance</span>"
    ]
  },
  {
    "objectID": "sql_basics.html#combine-data-from-multiple-rows-into-one-using-group-by",
    "href": "sql_basics.html#combine-data-from-multiple-rows-into-one-using-group-by",
    "title": "2  Read data, Combine tables, & aggregate numbers to understand business performance",
    "section": "2.4 Combine data from multiple rows into one using GROUP BY",
    "text": "2.4 Combine data from multiple rows into one using GROUP BY\nMost analytical queries require calculating metrics that involve combining data from multiple rows. GROUP BY allows us to perform aggregate calculations on data from a set of rows recognized by values of specified column(s). For example:\n\nCreate a report that shows the number of orders per orderpriority segment.\n\nUSE tpch.tiny;\n\nSELECT\n    orderpriority,\n    count(*) AS num_orders\nFROM\n    orders\nGROUP BY\n    orderpriority;\nIn the above query, we group the data by orderpriority, and the calculation count(*) will be applied to the rows having a specific orderpriority value.\nThe calculations allowed are typically SUM/MIN/MAX/AVG/COUNT. However, some databases have more complex aggregate functions; check your DB documentation.\n\n2.4.1 Use HAVING to filter based on the aggregates created by GROUP BY",
    "crumbs": [
      "Use SQL to transform data",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Read data, Combine tables, & aggregate numbers to understand business performance</span>"
    ]
  },
  {
    "objectID": "cte.html#recreating-similar-cte-is-a-sign-that-it-should-be-a-table",
    "href": "cte.html#recreating-similar-cte-is-a-sign-that-it-should-be-a-table",
    "title": "2  CTE (Common Table Expression) improves code readability and reduces repetition",
    "section": "2.3 Recreating similar CTE is a sign that it should be a table",
    "text": "2.3 Recreating similar CTE is a sign that it should be a table\nA sql query with multiple temporary tables is better than a 1000-line SQL query with numerous CTEs.\nKeep the number of CTE per query small (depends on the size of the query, but typically &lt; 5)\nCasestudy:\nRead the query below and answer the question\nwith orders as (\nselect\n        order_id,\n        customer_id,\n        order_status,\n        order_purchase_timestamp::TIMESTAMP AS order_purchase_timestamp,\n        order_approved_at::TIMESTAMP AS order_approved_at,\n        order_delivered_carrier_date::TIMESTAMP AS order_delivered_carrier_date,\n        order_delivered_customer_date::TIMESTAMP AS order_delivered_customer_date,\n        order_estimated_delivery_date::TIMESTAMP AS order_estimated_delivery_date\n    from raw_layer.orders\n    ),\n stg_customers as (\n    select\n        customer_id,\n        zipcode,\n        city,\n        state_code,\n        datetime_created::TIMESTAMP as datetime_created,\n        datetime_updated::TIMESTAMP as datetime_updated,\n        dbt_valid_from,\n        dbt_valid_to\n    from customer_snapshot\n),\nstate as (\nselect\n        state_id::INT as state_id,\n        state_code::VARCHAR(2) as state_code,\n        state_name::VARCHAR(30) as state_name\n    from raw_layer.state\n    ),\ndim_customers as (\nselect\n    c.customer_id,\n    c.zipcode,\n    c.city,\n    c.state_code,\n    s.state_name,\n    c.datetime_created,\n    c.datetime_updated,\n    c.dbt_valid_from::TIMESTAMP as valid_from,\n    case\n        when c.dbt_valid_to is NULL then '9999-12-31'::TIMESTAMP\n        else c.dbt_valid_to::TIMESTAMP\n    end as valid_to\nfrom stg_customers as c\ninner join state as s on c.state_code = s.state_code\n)\nselect\n    o.order_id,\n    o.customer_id,\n    o.order_status,\n    o.order_purchase_timestamp,\n    o.order_approved_at,\n    o.order_delivered_carrier_date,\n    o.order_delivered_customer_date,\n    o.order_estimated_delivery_date,\n    c.zipcode as customer_zipcode,\n    c.city as customer_city,\n    c.state_code as customer_state_code,\n    c.state_name as customer_state_name\nfrom orders as o\ninner join dim_customers as c on\n    o.customer_id = c.customer_id\n    and o.order_purchase_timestamp &gt;= c.valid_from\n    and o.order_purchase_timestamp &lt;= c.valid_to;",
    "crumbs": [
      "Use SQL to transform data",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>CTE (Common Table Expression) improves code readability and reduces repetition</span>"
    ]
  },
  {
    "objectID": "windows.html#use-ranking-functions-to-get-topbottom-n-rows",
    "href": "windows.html#use-ranking-functions-to-get-topbottom-n-rows",
    "title": "3  Use window function when you need to use values from other rows to compute a value for the current row",
    "section": "3.2 Use ranking functions to get top/bottom n rows",
    "text": "3.2 Use ranking functions to get top/bottom n rows\nIf you are working on a problem to get the top/bottom n rows (as defined by some value) then use the row functions.\nLet’s look at an example of how to use a row function:\nFrom the orders table get the top 3 spending customers per day. The orders table schema is shown below:\n\n\n\nOrders table\n\n\n\n3.2.1 [Example]\nSELECT\n  *\nFROM\n  (\n    SELECT\n      o_orderdate,\n      o_totalprice,\n      o_custkey,\n      RANK() -- RANKING FUNCTION \n      OVER (\n        PARTITION BY\n          o_orderdate -- PARTITION BY order date\n        ORDER BY\n          o_totalprice DESC -- ORDER rows withing partition by totalprice\n      ) AS rnk\n    FROM\n      orders\n  )\nWHERE\n  rnk &lt;= 3\nORDER BY\n  o_orderdate\nLIMIT\n  5;\n\n\n3.2.2 [Exercise] From the orders table get the 3 lowest spending customers per day\nTime limit during live workshop: 5 min\nHint 1. Figure out the PARTITION BY column first, then the ORDER BY column and finally the FUNCTION to use to compute running average.\nThe orders table schema is shown below:\n\n\n\nOrders table\n\n\n-- your code here\n\n\n3.2.3 Standard RANKING functions:\n\nRANK: Ranks the rows starting from 1 to n within the window frame. Ranks the rows with the same value (defined by the “ORDER BY” clause) as the same and skips the ranking numbers that would have been present if the values were different.\nDENSE_RANK: Ranks the rows starting from 1 to n within the window frame. Ranks the rows with the same value (defined by the “ORDER BY” clause) as the same and does not skip any ranking numbers.\nROW_NUMBER: Adds a row number that starts from 1 to n within the window frame and does not create any repeating values.\n\n\n\n3.2.4 [Example]\n-- Let's look at an example showing the difference between RANK, DENSE_RANK and ROW_NUMBER\nSELECT \n    order_date,\n    order_id,\n    total_price,\n    ROW_NUMBER() OVER (PARTITION BY order_date ORDER BY total_price) AS row_number,\n    RANK() OVER (PARTITION BY order_date ORDER BY total_price) AS rank,\n    DENSE_RANK() OVER (PARTITION BY order_date ORDER BY total_price) AS dense_rank\nFROM (\n    SELECT \n        '2024-07-08' AS order_date, 'order_1' AS order_id, 100 AS total_price UNION ALL\n    SELECT \n        '2024-07-08', 'order_2', 200 UNION ALL\n    SELECT \n        '2024-07-08', 'order_3', 150 UNION ALL\n    SELECT \n        '2024-07-08', 'order_4', 90 UNION ALL\n    SELECT \n        '2024-07-08', 'order_5', 100 UNION ALL\n    SELECT \n        '2024-07-08', 'order_6', 90 UNION ALL\n    SELECT \n        '2024-07-08', 'order_7', 100 UNION ALL\n    SELECT \n        '2024-07-10', 'order_8', 100 UNION ALL\n    SELECT \n        '2024-07-10', 'order_9', 100 UNION ALL\n    SELECT \n        '2024-07-10', 'order_10', 100 UNION ALL\n    SELECT \n        '2024-07-11', 'order_11', 100\n) AS orders\nORDER BY order_date, row_number;\nNow that we have see how to define a window function and how to use ranking and aggregation functions, let’s take it a step further by practicing value functions.\nRemember that value functions are used to access other row’s values while operating on the current row\nLet’s take a look at LEAD and LAG functions:\n\n\n\nLAG AND LEAD\n\n\n\n\n3.2.5 [Exercise] Write a SQL query using the orders table that calculates the following columns:\n1. o_orderdate: From orders table\n2. o_custkey: From orders table\n3. o_totalprice: From orders table\n4. totalprice_diff: The customers current day's o_totalprice - that same customers most recent previous purchase's o_totalprice\n\nTime limit during live workshop: 5 min\nHint:\n\nStart by figuring out what the PARTITION BY column should be, then what the ORDER BY column should be, and then finally the function to use.\nUse the LAG(column_name) ranking function to identify the prior day’s revenue.\n\nThe orders table schema is shown below:\n\n\n\n\nOrders table\n\n\n-- write your query here",
    "crumbs": [
      "Use SQL to transform data",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Use window function when you need to use values from other rows to compute a value for the current row</span>"
    ]
  },
  {
    "objectID": "windows.html#standard-ranking-functions",
    "href": "windows.html#standard-ranking-functions",
    "title": "3  Use window function when you need to use values from other rows to compute a value for the current row",
    "section": "4.3 Standard RANKING functions:",
    "text": "4.3 Standard RANKING functions:\n\nRANK: Ranks the rows starting from 1 to n within the window frame. Ranks the rows with the same value (defined by the “ORDER BY” clause) as the same and skips the ranking numbers that would have been present if the values were different.\nDENSE_RANK: Ranks the rows starting from 1 to n within the window frame. Ranks the rows with the same value (defined by the “ORDER BY” clause) as the same and does not skip any ranking numbers.\nROW_NUMBER: Adds a row number that starts from 1 to n within the window frame and does not create any repeating values.\n\n\n4.3.1 [Example]\n-- Let's look at an example showing the difference between RANK, DENSE_RANK and ROW_NUMBER\nSELECT \n    order_date,\n    order_id,\n    total_price,\n    ROW_NUMBER() OVER (PARTITION BY order_date ORDER BY total_price) AS row_number,\n    RANK() OVER (PARTITION BY order_date ORDER BY total_price) AS rank,\n    DENSE_RANK() OVER (PARTITION BY order_date ORDER BY total_price) AS dense_rank\nFROM (\n    SELECT \n        '2024-07-08' AS order_date, 'order_1' AS order_id, 100 AS total_price UNION ALL\n    SELECT \n        '2024-07-08', 'order_2', 200 UNION ALL\n    SELECT \n        '2024-07-08', 'order_3', 150 UNION ALL\n    SELECT \n        '2024-07-08', 'order_4', 90 UNION ALL\n    SELECT \n        '2024-07-08', 'order_5', 100 UNION ALL\n    SELECT \n        '2024-07-08', 'order_6', 90 UNION ALL\n    SELECT \n        '2024-07-08', 'order_7', 100 UNION ALL\n    SELECT \n        '2024-07-10', 'order_8', 100 UNION ALL\n    SELECT \n        '2024-07-10', 'order_9', 100 UNION ALL\n    SELECT \n        '2024-07-10', 'order_10', 100 UNION ALL\n    SELECT \n        '2024-07-11', 'order_11', 100\n) AS orders\nORDER BY order_date, row_number;\nNow that we have see how to define a window function and how to use ranking and aggregation functions, let’s take it a step further by practicing value functions.\nRemember that value functions are used to access other row’s values while operating on the current row\nLet’s take a look at LEAD and LAG functions:\n\n\n\nLAG AND LEAD\n\n\n\n\n4.3.2 [Exercise] Write a SQL query using the orders table that calculates the following columns:\n1. o_orderdate: From orders table\n2. o_custkey: From orders table\n3. o_totalprice: From orders table\n4. totalprice_diff: The customers current day's o_totalprice - that same customers most recent previous purchase's o_totalprice\n\nTime limit during live workshop: 5 min\nHint:\n\nStart by figuring out what the PARTITION BY column should be, then what the ORDER BY column should be, and then finally the function to use.\nUse the LAG(column_name) ranking function to identify the prior day’s revenue.\n\nThe orders table schema is shown below:\n\n\n\n\nOrders table\n\n\n-- write your query here",
    "crumbs": [
      "Use SQL to transform data",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Use window function when you need to use values from other rows to compute a value for the current row</span>"
    ]
  },
  {
    "objectID": "windows.html#standard-value-functions",
    "href": "windows.html#standard-value-functions",
    "title": "3  Use window function when you need to use values from other rows to compute a value for the current row",
    "section": "4.4 Standard VALUE functions:",
    "text": "4.4 Standard VALUE functions:\n\nNTILE(n): Divides the rows in the window frame into n approximately equal groups, and assigns a number to each row indicating which group it belongs to.\nFIRST_VALUE(): Returns the first value in the window frame.\nLAST_VALUE(): Returns the last value in the window frame.\nLAG(): Accesses data from a previous row within the window frame.\nLEAD(): Accesses data from a subsequent row within the window frame.",
    "crumbs": [
      "Use SQL to transform data",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Use window function when you need to use values from other rows to compute a value for the current row</span>"
    ]
  },
  {
    "objectID": "windows.html#use-ordering-of-rows-to-define-your-window-frame-using-the-rows-clause",
    "href": "windows.html#use-ordering-of-rows-to-define-your-window-frame-using-the-rows-clause",
    "title": "3  Use window function when you need to use values from other rows to compute a value for the current row",
    "section": "5.1 Use ordering of rows to define your window frame using the ROWS clause",
    "text": "5.1 Use ordering of rows to define your window frame using the ROWS clause\n\nROWS: Used to select a set of rows relative to the current row based on position.\n\nRow definition format ROWS BETWEEN start_point AND end_point.\nThe start_point and end_point can be any of the following three (in the proper order:\n\nn PRECEDING: n rows preceding the current row. UNBOUNDED PRECEDING indicates all rows before the current row.\nn FOLLOWING: n rows following the current row. UNBOUNDED FOLLOWING indicates all rows after the current row.\n\n\n\nLet’s see how relative row numbers can be used to define a window range.\nConsider this window function\nAVG(total_price) OVER ( -- FUNCTION: RUNNING AVERAGE\n    PARTITION BY o_custkey -- PARTITIONED BY customer\n    ORDER BY order_month \n    ROWS BETWEEN 1 PRECEDING AND 1 FOLLOWING -- WINDOW FRAME DEFINED AS 1 ROW PRECEDING to 1 ROW FOLLOWING\n    )\n\n\n\nWindow frame with ROWS\n\n\n\n5.1.1 [Example]\n\nWrite a query on the orders table that has the following output:\n\no_custkey\norder_month: In YYYY-MM format, use strftime(o_orderdate, ‘%Y-%m’) AS order_month\ntotal_price: Sum of o_totalprice for that month\nthree_mo_total_price_avg: The 3 month (previous, current & next) average of total_price for that customer\n\n\nSELECT\n  order_month,\n  o_custkey,\n  total_price,\n  ROUND(\n    AVG(total_price) OVER ( -- FUNCTION: RUNNING AVERAGE\n      PARTITION BY\n        o_custkey -- PARTITIONED BY customer\n      ORDER BY\n        order_month ROWS BETWEEN 1 PRECEDING\n        AND 1 FOLLOWING -- WINDOW FRAME DEFINED AS 1 ROW PRECEDING to 1 ROW FOLLOWING\n    ),\n    2\n  ) AS three_mo_total_price_avg\nFROM\n  (\n    SELECT\n      strftime (o_orderdate, '%Y-%m') AS order_month,\n      o_custkey,\n      SUM(o_totalprice) AS total_price\n    FROM\n      orders\n    GROUP BY\n      1,\n      2\n  )\nLIMIT\n  5;",
    "crumbs": [
      "Use SQL to transform data",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Use window function when you need to use values from other rows to compute a value for the current row</span>"
    ]
  },
  {
    "objectID": "windows.html#use-ordering-of-rows-to-define-your-window-frame-with-the-rows-clause",
    "href": "windows.html#use-ordering-of-rows-to-define-your-window-frame-with-the-rows-clause",
    "title": "3  Use window function when you need to use values from other rows to compute a value for the current row",
    "section": "3.4 Use ordering of rows to define your window frame with the ROWS clause",
    "text": "3.4 Use ordering of rows to define your window frame with the ROWS clause\n\nROWS: Used to select a set of rows relative to the current row based on position.\n\nRow definition format ROWS BETWEEN start_point AND end_point.\nThe start_point and end_point can be any of the following three (in the proper order:\n\nn PRECEDING: n rows preceding the current row. UNBOUNDED PRECEDING indicates all rows before the current row.\nn FOLLOWING: n rows following the current row. UNBOUNDED FOLLOWING indicates all rows after the current row.\n\n\n\nLet’s see how relative row numbers can be used to define a window range.\nConsider this window function\nAVG(total_price) OVER ( -- FUNCTION: RUNNING AVERAGE\n    PARTITION BY o_custkey -- PARTITIONED BY customer\n    ORDER BY order_month \n    ROWS BETWEEN 1 PRECEDING AND 1 FOLLOWING -- WINDOW FRAME DEFINED AS 1 ROW PRECEDING to 1 ROW FOLLOWING\n    )\n\n\n\nWindow frame with ROWS\n\n\n\n3.4.1 [Example]\n\nWrite a query on the orders table that has the following output:\n\no_custkey\norder_month: In YYYY-MM format, use strftime(o_orderdate, ‘%Y-%m’) AS order_month\ntotal_price: Sum of o_totalprice for that month\nthree_mo_total_price_avg: The 3 month (previous, current & next) average of total_price for that customer\n\n\nSELECT\n  order_month,\n  o_custkey,\n  total_price,\n  ROUND(\n    AVG(total_price) OVER ( -- FUNCTION: RUNNING AVERAGE\n      PARTITION BY\n        o_custkey -- PARTITIONED BY customer\n      ORDER BY\n        order_month ROWS BETWEEN 1 PRECEDING\n        AND 1 FOLLOWING -- WINDOW FRAME DEFINED AS 1 ROW PRECEDING to 1 ROW FOLLOWING\n    ),\n    2\n  ) AS three_mo_total_price_avg\nFROM\n  (\n    SELECT\n      strftime (o_orderdate, '%Y-%m') AS order_month,\n      o_custkey,\n      SUM(o_totalprice) AS total_price\n    FROM\n      orders\n    GROUP BY\n      1,\n      2\n  )\nLIMIT\n  5;",
    "crumbs": [
      "Use SQL to transform data",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Use window function when you need to use values from other rows to compute a value for the current row</span>"
    ]
  },
  {
    "objectID": "windows.html#use-values-of-the-columns-to-define-window-frame-using-range-clause",
    "href": "windows.html#use-values-of-the-columns-to-define-window-frame-using-range-clause",
    "title": "3  Use window function when you need to use values from other rows to compute a value for the current row",
    "section": "3.5 Use values of the columns to define window frame using RANGE clause",
    "text": "3.5 Use values of the columns to define window frame using RANGE clause\n\nRANGE: Used to select a set of rows relative to the current row based on the value of the columns specified in the ORDER BY clause.\n\nRange definition format RANGE BETWEEN start_point AND end_point.\nThe start_point and end_point can be any of the following:\n\nCURRENT ROW: The current row.\nn PRECEDING: All rows with values within the specified range that are less than or equal to n units preceding the value of the current row.\nn FOLLOWING: All rows with values within the specified range that are greater than or equal to n units following the value of the current row.\nUNBOUNDED PRECEDING: All rows before the current row within the partition.\nUNBOUNDED FOLLOWING: All rows after the current row within the partition.\n\nRANGE is particularly useful when dealing with numeric or date/time ranges, allowing for calculations like running totals, moving averages, or cumulative distributions.\n\n\nLet’s see how RANGE works with AVG(total price) OVER (PARTITION BY customer id ORDER BY date RANGE BETWEEN INTERVAL '1' DAY PRECEDING AND '1' DAY FOLLOWING) using the below visualization:\n\n\n\nRANGE\n\n\n\n3.5.1 [Exercise]\nNow that we have seen how to create a window frame with ROWS, let’ explore how to do this with RANGE.\n\nWrite a query on the orders table that has the following output:\n\norder_month,\no_custkey,\ntotal_price,\nthree_mo_total_price_avg\nconsecutive_three_mo_total_price_avg: The consecutive 3 month average of total_price for that customer. Note that this should only include months that are chronologically next to each other.\n\n\nTime limit during live workshop: 10 min\nHint: Use CAST(strftime(o_orderdate, '%Y-%m-01') AS DATE) to cast order_month to date format.\nHint: Use the INTERVAL format shown above to construct the window function to compute consecutive_three_mo_total_price_avg column.\n\nThe orders table schema is shown below:\n\n\n\n\nOrders table\n\n\n-- write your query here",
    "crumbs": [
      "Use SQL to transform data",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Use window function when you need to use values from other rows to compute a value for the current row</span>"
    ]
  },
  {
    "objectID": "windows.html#value-functions-are-used-to-access-other-rows-values",
    "href": "windows.html#value-functions-are-used-to-access-other-rows-values",
    "title": "3  Use window function when you need to use values from other rows to compute a value for the current row",
    "section": "3.3 Value functions are used to access other rows values",
    "text": "3.3 Value functions are used to access other rows values\nStandard VALUE functions:\n\nNTILE(n): Divides the rows in the window frame into n approximately equal groups, and assigns a number to each row indicating which group it belongs to.\nFIRST_VALUE(): Returns the first value in the window frame.\nLAST_VALUE(): Returns the last value in the window frame.\nLAG(): Accesses data from a previous row within the window frame.\nLEAD(): Accesses data from a subsequent row within the window frame.",
    "crumbs": [
      "Use SQL to transform data",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Use window function when you need to use values from other rows to compute a value for the current row</span>"
    ]
  },
  {
    "objectID": "windows.html#aggregate-functions-enable-you-to-perform-running-metrics-computation",
    "href": "windows.html#aggregate-functions-enable-you-to-perform-running-metrics-computation",
    "title": "3  Use window function when you need to use values from other rows to compute a value for the current row",
    "section": "4.4 Aggregate functions enable you to perform running metrics computation",
    "text": "4.4 Aggregate functions enable you to perform running metrics computation",
    "crumbs": [
      "Use SQL to transform data",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Use window function when you need to use values from other rows to compute a value for the current row</span>"
    ]
  },
  {
    "objectID": "windows.html#aggregate-functions-enable-you-to-compute-running-metrics",
    "href": "windows.html#aggregate-functions-enable-you-to-compute-running-metrics",
    "title": "3  Use window function when you need to use values from other rows to compute a value for the current row",
    "section": "3.4 Aggregate functions enable you to compute running metrics",
    "text": "3.4 Aggregate functions enable you to compute running metrics\nThe standard aggregate functions are MIN, MAX, AVG, SUM, & COUNT. In addition to these make sure to check your DB engine documentation, in our case Spark Aggregate functions.\nWhen you need a running sum/min/max/avg, its almost always a use case for aggregate functions with windows.",
    "crumbs": [
      "Use SQL to transform data",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Use window function when you need to use values from other rows to compute a value for the current row</span>"
    ]
  },
  {
    "objectID": "dw.html#column-encoding-in-olap-dbs-enable-efficient-processing-of-aggeregating-a-small-number-of-columns-from-a-large-wide-table",
    "href": "dw.html#column-encoding-in-olap-dbs-enable-efficient-processing-of-aggeregating-a-small-number-of-columns-from-a-large-wide-table",
    "title": "7  Data warehouse contains historical data and is used to analyze business performance",
    "section": "7.2 Column encoding in OLAP DBs enable efficient processing of aggeregating a small number of columns from a large wide table",
    "text": "7.2 Column encoding in OLAP DBs enable efficient processing of aggeregating a small number of columns from a large wide table\nThe major improvement in analytical queries on OLAP is due to its column store technique. Let’s consider a table items, with the data shown below.\n\n\n\n\n\n\n\n\n\n\n\nitem_id\nitem_name\nitem_type\nitem_price\ndatetime_created\ndatetime_updated\n\n\n\n\n1\nitem_1\ngaming\n10\n‘2021-10-02 00:00:00’\n‘2021-11-02 13:00:00’\n\n\n2\nitem_2\ngaming\n20\n‘2021-10-02 01:00:00’\n‘2021-11-02 14:00:00’\n\n\n3\nitem_3\nbiking\n30\n‘2021-10-02 02:00:00’\n‘2021-11-02 15:00:00’\n\n\n4\nitem_4\nsurfing\n40\n‘2021-10-02 03:00:00’\n‘2021-11-02 16:00:00’\n\n\n5\nitem_5\nbiking\n50\n‘2021-10-02 04:00:00’\n‘2021-11-02 17:00:00’\n\n\n\nLet’s see how this table will be stored in a row and column-oriented storage. Data will be stored as pages (group of records) on the disk.\nRow oriented storage:\nLet’s assume that there is one row per page.\nPage 1: [1,item_1,gaming,10,'2021-10-02 00:00:00','2021-11-02 13:00:00'],\nPage 2: [2,item_2,gaming,20,'2021-10-02 01:00:00','2021-11-02 14:00:00']\nPage 3: [3,item_3,biking,30, '2021-10-02 02:00:00','2021-11-02 15:00:00'],\nPage 4: [4,item_4,surfing,40, '2021-10-02 03:00:00','2021-11-02 16:00:00'],\nPage 5: [5,item_5,biking,50, '2021-10-02 04:00:00','2021-11-02 17:00:00']\nColumn-oriented storage:\nLet’s assume that there is one column per page.\nPage 1: [1,2,3,4,5],\nPage 2: [item_1,item_2,item_3,item_4,item_5],\nPage 3: [gaming,gaming,biking,surfing,biking],\nPage 4: [10,20,30,40,50],\nPage 5: ['2021-10-02 00:00:00','2021-10-02 01:00:00','2021-10-02 02:00:00','2021-10-02 03:00:00','2021-10-02 04:00:00'],\nPage 6: ['2021-11-02 13:00:00','2021-11-02 14:00:00','2021-11-02 15:00:00','2021-11-02 16:00:00','2021-11-02 17:00:00']\nLet’s see how a simple analytical query will be executed.\nSELECT item_type,\n    SUM(price) total_price\nFROM items\nGROUP BY item_type;\nIn a row-oriented database\n\nAll the pages will need to be loaded into memory\nSum price column for same item_type values\n\nIn a column-oriented database\n\nOnly pages 3 and 4 will need to be loaded into memory\nSum price column for same item_type values\n\nAs you can see from this approach, we only need to read 2 pages in a column-oriented database vs 5 pages in a row-oriented database. In addition to this, a column-oriented database also provides\n\nBetter compression, as similar data types are next to each other and can be compressed more efficiently.\nVectorized processing\n\nAll of these features make a column-oriented database a great choice for storing and analyzing large amounts of data.",
    "crumbs": [
      "Data modeling is the process of getting data ready for analytics",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Data warehouse contains historical data and is used to analyze business performance</span>"
    ]
  },
  {
    "objectID": "dw.html#column-encoding-enables-efficient-processing-of-a-small-number-of-columns-from-a-large-wide-table",
    "href": "dw.html#column-encoding-enables-efficient-processing-of-a-small-number-of-columns-from-a-large-wide-table",
    "title": "7  Data warehouse contains historical data and is used to analyze business performance",
    "section": "7.2 Column encoding enables efficient processing of a small number of columns from a large wide table",
    "text": "7.2 Column encoding enables efficient processing of a small number of columns from a large wide table\nThe major improvement in analytical queries on OLAP is due to its column store technique. Let’s consider a table items, with the data shown below.\n\n\n\n\n\n\n\n\n\n\n\nitem_id\nitem_name\nitem_type\nitem_price\ndatetime_created\ndatetime_updated\n\n\n\n\n1\nitem_1\ngaming\n10\n‘2021-10-02 00:00:00’\n‘2021-11-02 13:00:00’\n\n\n2\nitem_2\ngaming\n20\n‘2021-10-02 01:00:00’\n‘2021-11-02 14:00:00’\n\n\n3\nitem_3\nbiking\n30\n‘2021-10-02 02:00:00’\n‘2021-11-02 15:00:00’\n\n\n4\nitem_4\nsurfing\n40\n‘2021-10-02 03:00:00’\n‘2021-11-02 16:00:00’\n\n\n5\nitem_5\nbiking\n50\n‘2021-10-02 04:00:00’\n‘2021-11-02 17:00:00’\n\n\n\nLet’s see how this table will be stored in a row and column-oriented storage. Data is stored as continuous pages (group of records) on the disk.\nRow oriented storage:\nLet’s assume that there is one row per page.\nPage 1: [1,item_1,gaming,10,'2021-10-02 00:00:00','2021-11-02 13:00:00'],\nPage 2: [2,item_2,gaming,20,'2021-10-02 01:00:00','2021-11-02 14:00:00']\nPage 3: [3,item_3,biking,30, '2021-10-02 02:00:00','2021-11-02 15:00:00'],\nPage 4: [4,item_4,surfing,40, '2021-10-02 03:00:00','2021-11-02 16:00:00'],\nPage 5: [5,item_5,biking,50, '2021-10-02 04:00:00','2021-11-02 17:00:00']\nColumn-oriented storage:\nLet’s assume that there is one column per page.\nPage 1: [1,2,3,4,5],\nPage 2: [item_1,item_2,item_3,item_4,item_5],\nPage 3: [gaming,gaming,biking,surfing,biking],\nPage 4: [10,20,30,40,50],\nPage 5: ['2021-10-02 00:00:00','2021-10-02 01:00:00','2021-10-02 02:00:00','2021-10-02 03:00:00','2021-10-02 04:00:00'],\nPage 6: ['2021-11-02 13:00:00','2021-11-02 14:00:00','2021-11-02 15:00:00','2021-11-02 16:00:00','2021-11-02 17:00:00']\nLet’s see how a simple analytical query will be executed.\nSELECT item_type,\n    SUM(price) total_price\nFROM items\nGROUP BY item_type;\nIn a row-oriented database\n\nAll the pages will need to be loaded into memory\nSum price column for same item_type values\n\nIn a column-oriented database\n\nOnly pages 3 and 4 will need to be loaded into memory. The information mapping page 3 and 4 to item_type and total_price will be encoded in the column oriented file and also stored in an OLTP called metadata db.\nSum price column for same item_type values\n\nAs you can see from this approach, we only need to read 2 pages in a column-oriented database vs 5 pages in a row-oriented database. In addition to this, a column-oriented database also provides\n\nBetter compression, as similar data types are next to each other and can be compressed more efficiently.\nVectorized processing\n\nAll of these features make a column-oriented database a great choice for storing and analyzing large amounts of data.",
    "crumbs": [
      "Data modeling is the process of getting data ready for analytics",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Data warehouse contains historical data and is used to analyze business performance</span>"
    ]
  },
  {
    "objectID": "dw_tables.html#facts-represents-events-that-occured-dimensions-are-entities-to-which-said-events-can-occur-to",
    "href": "dw_tables.html#facts-represents-events-that-occured-dimensions-are-entities-to-which-said-events-can-occur-to",
    "title": "8  Data warehouse modeling (Kimball) is based off of 2 types of tables: Fact and dimensions",
    "section": "",
    "text": "Dimension: Each row in a dimension table represents a business entity that is important to the business. For example, An car parts seller’s data warehouse will have a customer dimension table, where each row will represent an individual customer. Other examples of dimension tables in a car parts seller’s data warehouse would be supplier & part tables.\nFacts: Each row in a fact table represents a business process that occurred. E.g., In our data warehouse, each row in the orders fact table will represent an individual order, and each row in the lineitem fact table will represent an item sold as part of an order. Each fact row will have a unique identifier; in our case, it’s orderkey for orders and a combination of orderkey & linenumber for lineitem.\n\n\n\n\n-- calculating the totalprice of an order (with orderkey = 1) from it's individual items\nSELECT\n    orderkey,\n    round( sum(extendedprice * (1 - discount) * (1 + tax)),\n        2\n    ) AS totalprice\nFROM\n    lineitem\nWHERE\n    orderkey = 1\nGROUP BY\n    orderkey;\n\n/*\n orderkey | totalprice\n----------+------------\n        1 |  172799.56\n*/\n\n-- The totalprice of an order (with orderkey = 1)\nSELECT\n    orderkey,\n    totalprice\nFROM\n    orders\nWHERE\n    orderkey = 1;\n\n/*\n orderkey | totalprice\n----------+------------\n        1 |  172799.49\n*/",
    "crumbs": [
      "Data modeling is the process of getting data ready for analytics",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Data warehouse modeling (Kimball) is based off of 2 types of tables: Fact and dimensions</span>"
    ]
  },
  {
    "objectID": "dw_tables.html#facts-represents-events-that-occured-dimensions-are-entities-to-which-events-can-occur-to",
    "href": "dw_tables.html#facts-represents-events-that-occured-dimensions-are-entities-to-which-events-can-occur-to",
    "title": "8  Data warehouse modeling (Kimball) is based off of 2 types of tables: Fact and dimensions",
    "section": "",
    "text": "Dimension: Each row in a dimension table represents a business entity that is important to the business. For example, An car parts seller’s data warehouse will have a customer dimension table, where each row will represent an individual customer. Other examples of dimension tables in a car parts seller’s data warehouse would be supplier & part tables.\nFacts: Each row in a fact table represents a business process that occurred. E.g., In our data warehouse, each row in the orders fact table will represent an individual order, and each row in the lineitem fact table will represent an item sold as part of an order. Each fact row will have a unique identifier; in our case, it’s orderkey for orders and a combination of orderkey & linenumber for lineitem.\n\n\n\n\n-- calculating the totalprice of an order (with orderkey = 1) from it's individual items\nSELECT\n    orderkey,\n    round( sum(extendedprice * (1 - discount) * (1 + tax)),\n        2\n    ) AS totalprice\nFROM\n    lineitem\nWHERE\n    orderkey = 1\nGROUP BY\n    orderkey;\n\n/*\n orderkey | totalprice\n----------+------------\n        1 |  172799.56\n*/\n\n-- The totalprice of an order (with orderkey = 1)\nSELECT\n    orderkey,\n    totalprice\nFROM\n    orders\nWHERE\n    orderkey = 1;\n\n/*\n orderkey | totalprice\n----------+------------\n        1 |  172799.49\n*/\n\n\n\n8.1.1 Popular dimension types: Snapshot & SCD2",
    "crumbs": [
      "Data modeling is the process of getting data ready for analytics",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Data warehouse modeling (Kimball) is based off of 2 types of tables: Fact and dimensions</span>"
    ]
  },
  {
    "objectID": "dw_tables.html#one-big-table-obt-a-fact-table-left-joined-with-all-its-dimensions",
    "href": "dw_tables.html#one-big-table-obt-a-fact-table-left-joined-with-all-its-dimensions",
    "title": "8  Data warehouse modeling (Kimball) is based off of 2 types of tables: Fact and dimensions",
    "section": "8.2 One Big Table (OBT) = A fact table left joined with all its dimensions",
    "text": "8.2 One Big Table (OBT) = A fact table left joined with all its dimensions",
    "crumbs": [
      "Data modeling is the process of getting data ready for analytics",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Data warehouse modeling (Kimball) is based off of 2 types of tables: Fact and dimensions</span>"
    ]
  },
  {
    "objectID": "dw_tables.html#one-big-table-obt-is-a-fact-table-left-joined-with-all-its-dimensions",
    "href": "dw_tables.html#one-big-table-obt-is-a-fact-table-left-joined-with-all-its-dimensions",
    "title": "8  Data warehouse modeling (Kimball) is based off of 2 types of tables: Fact and dimensions",
    "section": "8.2 One Big Table (OBT) is a fact table left joined with all its dimensions",
    "text": "8.2 One Big Table (OBT) is a fact table left joined with all its dimensions",
    "crumbs": [
      "Data modeling is the process of getting data ready for analytics",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Data warehouse modeling (Kimball) is based off of 2 types of tables: Fact and dimensions</span>"
    ]
  },
  {
    "objectID": "dw_tables.html#summary-or-pre-aggregated-tables-are-stakeholder-team-specific-tables-built-for-reporting",
    "href": "dw_tables.html#summary-or-pre-aggregated-tables-are-stakeholder-team-specific-tables-built-for-reporting",
    "title": "8  Data warehouse modeling (Kimball) is based off of 2 types of tables: Fact and dimensions",
    "section": "8.3 Summary or pre-aggregated tables are stakeholder-team specific tables built for reporting",
    "text": "8.3 Summary or pre-aggregated tables are stakeholder-team specific tables built for reporting\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n–&gt;         –&gt;",
    "crumbs": [
      "Data modeling is the process of getting data ready for analytics",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Data warehouse modeling (Kimball) is based off of 2 types of tables: Fact and dimensions</span>"
    ]
  },
  {
    "objectID": "docker.html#docker-image-is-a-blueprint-of-what-your-docker-container-should-be",
    "href": "docker.html#docker-image-is-a-blueprint-of-what-your-docker-container-should-be",
    "title": "10  Docker recreates the same environment for your code in any machine",
    "section": "",
    "text": "Docker image\n\n\n\n\nFROM: We need a base operating system on which to set our configurations. We can also use existing Docker images available at the Docker Hub and add our config on top of them. In our example, we use the official Delta Lake Docker image.\nCOPY: Copy is used to copy files or folders from our local filesystem to the image. The copy command is usually used when building the docker image to copy settings, static files, etc. In our example, we copy over the tpch-dbgen folder, which contains the logic to create tpch data. We also copy over our requirements.txt file and our entrypoint.sh file.\nRUN: Run is used to run a command in the shell terminal of your image. It is typically used to install libraries, create folders, etc.\nENV: This command sets the image’s environment variables. In our example, we set Spark environment variables.\nENTRYPOINT: The entrypoint command executes a script when the image starts. In our example, we use a script file (entrypoint.sh) to start spark master and worker nodes depending on the inputs given to the docker cli when starting a container from this image.",
    "crumbs": [
      "Working in a team",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Docker recreates the same environment for your code in any machine</span>"
    ]
  },
  {
    "objectID": "docker.html#start-any-number-of-containers-using-docker-image",
    "href": "docker.html#start-any-number-of-containers-using-docker-image",
    "title": "10  Docker recreates the same environment for your code in any machine",
    "section": "10.2 Start any number of containers using docker image",
    "text": "10.2 Start any number of containers using docker image\nContainers are the actual running virtual machines. We use images(Dockerfile) to create docker containers. We can spin up one or more containers from a given image.\n\n10.2.1 Communicate between containers and local OS\nTypically, with data infra, multiple containers run. When we want the container to communicate with them and the local operating system, we need to enable it via ports for http-based interactions. In our example, we ensure that Spark clusters can talk with each other and with the local operating system by defining ports in our docker-compose.yml file.\nUsing mounted volumes, we can also ensure that files are synced between the containers and the local operating system. In addition to syncing local files, we can also create docker volumes to sync files between our containers.\n\n\n\ndocker port\n\n\n\n\n10.2.2 Start containers with docker CLI or compose\nWe have seen how images are blueprints for containers. We need to use the docker clito start a container. With the docker cli, we can define the image to use, the container’s name, volume mounts, open ports, etc. For example, to start our Spark master container, we can use the following:\ndocker run -d \\\n  --name spark-master \\\n  --entrypoint ./entrypoint.sh \\\n  -p 4040:4040 \\\n  -p 9090:8080 \\\n  -p 7077:7077 \\\n  -v \"$(pwd)/capstone:/opt/spark/work-dir/capstone\" \\\n  -v \"$(pwd)/data-processing-spark:/opt/spark/work-dir/data-processing-spark\" \\\n  -v spark-logs:/opt/spark/spark-events \\\n  -v tpch-data:/opt/spark/tpch-dbgen \\\n  --env-file .env.spark \\\n  spark-image master\nHowever, with most data systems, we will need to ensure multiple systems are running. While we can use docker cli to do this, a better option is to use docker compose to orchestrate the different containers required. With docker compose, we can define all our settings in one file and ensure that they are started in the order we prefer.\nOur docker compose is defined here. With our docker compose defined, starting our containers is a simple command, as shown below:\ndocker compose up --build -d --scale spark-worker=2\nThe command will, by default, look for a file called docker-compose.yml in the directory in which it is run. We use the --build command to tell docker to use the image to create containers (without build, docker will use any previously created image, which may not reflect the changes you make to the Dockerfile). We also use --scale to ask docker to create two spark-worker containers. Note that we define our container names in the docker-compose.yml file.\nIn our setup, we have four systems specified in the docker compose yaml file; they are\n\nPostgres database: We use a Postgres database to simulate an upstream application database.\nSpark cluster: We create a spark cluster with a master and two workers, where the data is processed. The spark cluster also includes a history server, which displays the logs and resource utilization (Spark UI) for completed/failed spark applications.\nMinio: Minio is open-source software that has a fully compatible API with the AWS S3 cloud storage system. We use minio to replicate S3 locally.\ncreatebuckets: This is a short-lived docker container that creates the necessary folder structure in our minio storage system before turning off.\n\n\n\n\ndocker infra\n\n\n\n\n10.2.3 Containers can be always-running or short-lived\n\n\n10.2.4 Executing commands in your docker container\nTypically, docker containers are meant to live for the duration of the command to be executed on them. In our use case, since we need our infrastructure to be always on, we use the entrypoint.sh to start Spark master and worker servers, which keeps the containers running.\nUsing the exec command, you can submit commands to be run in a specific container. For example, we can use the following to open a bash terminal in our spark-master container:\ndocker exec -ti spark-master bash\n# You will be in the master container bash shell\nexit # exit the container\nNote that the -ti indicates that this will be run in an interactive mode. As shown below, we can run a command without interactive mode and get an output.\ndocker exec spark-master echo hello\n# prints hello",
    "crumbs": [
      "Working in a team",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Docker recreates the same environment for your code in any machine</span>"
    ]
  },
  {
    "objectID": "windows.html#a-window-frame-defines-a-set-of-rows-within-a-partition-on-which-a-function-is-to-be-applied",
    "href": "windows.html#a-window-frame-defines-a-set-of-rows-within-a-partition-on-which-a-function-is-to-be-applied",
    "title": "3  Use window function when you need to use values from other rows to compute a value for the current row",
    "section": "3.3 A window frame defines a set of rows within a partition on which a function is to be applied",
    "text": "3.3 A window frame defines a set of rows within a partition on which a function is to be applied\nWhile our functions operate on the rows in the partition a window frame provides more granular ways to operate on a select set of rows within a partition.\nWhen we need to operate one a set of rows within a parition (e.g. a sliding window) we can use the window frame to define these set of rows.\n\n\n\nThree order Sliding window average\n\n\nExample\nConsider a scenario where you have sales data, and you want to calculate a 3-day moving average of sales within each store:\nSELECT\n    store_id,\n    sale_date,\n    sales_amount,\n    AVG(sales_amount) OVER (\n        PARTITION BY store_id\n        ORDER BY sale_date\n        ROWS BETWEEN 2 PRECEDING AND CURRENT ROW\n    ) AS moving_avg_sales\nFROM\n    sales;\nIn this example:\n\nPARTITION BY store_id ensures the calculation is done separately for each store.\nORDER BY sale_date defines the order of rows within each partition.\nROWS BETWEEN 2 PRECEDING AND CURRENT ROW specifies the window frame, considering the current row and the two preceding rows to calculate the moving average. add: image Without defining the window frame, the function might not be able to provide the specific moving average calculation you need.",
    "crumbs": [
      "Use SQL to transform data",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Use window function when you need to use values from other rows to compute a value for the current row</span>"
    ]
  },
  {
    "objectID": "windows.html#common-scenarios-when-you-want-to-use-window-functions",
    "href": "windows.html#common-scenarios-when-you-want-to-use-window-functions",
    "title": "3  Use window function when you need to use values from other rows to compute a value for the current row",
    "section": "",
    "text": "Calculate running metrics/sliding window over rows in the table (aggregate functions)\nRanking rows based on values in column(s) (ranking functions)\nAccess other row’s values while operating on the current row (value functions)\nAny combination of the above",
    "crumbs": [
      "Use SQL to transform data",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Use window function when you need to use values from other rows to compute a value for the current row</span>"
    ]
  },
  {
    "objectID": "dw_tables.html#facts-represents-events-that-occured-dimensions-the-entities-to-which-events-can-occur-to",
    "href": "dw_tables.html#facts-represents-events-that-occured-dimensions-the-entities-to-which-events-can-occur-to",
    "title": "8  Data warehouse modeling (Kimball) is based off of 2 types of tables: Fact and dimensions",
    "section": "",
    "text": "Dimension: Each row in a dimension table represents a business entity that is important to the business. For example, An car parts seller’s data warehouse will have a customer dimension table, where each row will represent an individual customer. Other examples of dimension tables in a car parts seller’s data warehouse would be supplier & part tables.\nFacts: Each row in a fact table represents a business process that occurred. E.g., In our data warehouse, each row in the orders fact table will represent an individual order, and each row in the lineitem fact table will represent an item sold as part of an order. Each fact row will have a unique identifier; in our case, it’s orderkey for orders and a combination of orderkey & linenumber for lineitem.\n\n\n\n\n-- calculating the totalprice of an order (with orderkey = 1) from it's individual items\nSELECT\n    orderkey,\n    round( sum(extendedprice * (1 - discount) * (1 + tax)),\n        2\n    ) AS totalprice\nFROM\n    lineitem\nWHERE\n    orderkey = 1\nGROUP BY\n    orderkey;\n\n/*\n orderkey | totalprice\n----------+------------\n        1 |  172799.56\n*/\n\n-- The totalprice of an order (with orderkey = 1)\nSELECT\n    orderkey,\n    totalprice\nFROM\n    orders\nWHERE\n    orderkey = 1;\n\n/*\n orderkey | totalprice\n----------+------------\n        1 |  172799.49\n*/\n\n\n\n8.1.1 Popular dimension types: Snapshot & SCD2",
    "crumbs": [
      "Data modeling is the process of getting data ready for analytics",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Data warehouse modeling (Kimball) is based off of 2 types of tables: Fact and dimensions</span>"
    ]
  },
  {
    "objectID": "dw_tables.html#facts-represents-events-that-occured-dimensions-the-entities-to-which-events-occur-to",
    "href": "dw_tables.html#facts-represents-events-that-occured-dimensions-the-entities-to-which-events-occur-to",
    "title": "8  Data warehouse modeling (Kimball) is based off of 2 types of tables: Fact and dimensions",
    "section": "",
    "text": "Dimension: Each row in a dimension table represents a business entity that is important to the business. For example, An car parts seller’s data warehouse will have a customer dimension table, where each row will represent an individual customer. Other examples of dimension tables in a car parts seller’s data warehouse would be supplier & part tables.\nFacts: Each row in a fact table represents a business process that occurred. E.g., In our data warehouse, each row in the orders fact table will represent an individual order, and each row in the lineitem fact table will represent an item sold as part of an order. Each fact row will have a unique identifier; in our case, it’s orderkey for orders and a combination of orderkey & linenumber for lineitem.\n\n\n\n\n-- calculating the totalprice of an order (with orderkey = 1) from it's individual items\nSELECT\n    orderkey,\n    round( sum(extendedprice * (1 - discount) * (1 + tax)),\n        2\n    ) AS totalprice\nFROM\n    lineitem\nWHERE\n    orderkey = 1\nGROUP BY\n    orderkey;\n\n/*\n orderkey | totalprice\n----------+------------\n        1 |  172799.56\n*/\n\n-- The totalprice of an order (with orderkey = 1)\nSELECT\n    orderkey,\n    totalprice\nFROM\n    orders\nWHERE\n    orderkey = 1;\n\n/*\n orderkey | totalprice\n----------+------------\n        1 |  172799.49\n*/\n\n\n\n8.1.1 Popular dimension types: Snapshot & SCD2",
    "crumbs": [
      "Data modeling is the process of getting data ready for analytics",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Data warehouse modeling (Kimball) is based off of 2 types of tables: Fact and dimensions</span>"
    ]
  },
  {
    "objectID": "docker.html#docker-image-is-a-blueprint-of-what-your-container-should-be",
    "href": "docker.html#docker-image-is-a-blueprint-of-what-your-container-should-be",
    "title": "10  Docker recreates the same environment for your code in any machine",
    "section": "",
    "text": "Docker image\n\n\n\n\nFROM: We need a base operating system on which to set our configurations. We can also use existing Docker images available at the Docker Hub and add our config on top of them. In our example, we use the official Delta Lake Docker image.\nCOPY: Copy is used to copy files or folders from our local filesystem to the image. The copy command is usually used when building the docker image to copy settings, static files, etc. In our example, we copy over the tpch-dbgen folder, which contains the logic to create tpch data. We also copy over our requirements.txt file and our entrypoint.sh file.\nRUN: Run is used to run a command in the shell terminal of your image. It is typically used to install libraries, create folders, etc.\nENV: This command sets the image’s environment variables. In our example, we set Spark environment variables.\nENTRYPOINT: The entrypoint command executes a script when the image starts. In our example, we use a script file (entrypoint.sh) to start spark master and worker nodes depending on the inputs given to the docker cli when starting a container from this image.",
    "crumbs": [
      "Working in a team",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Docker recreates the same environment for your code in any machine</span>"
    ]
  },
  {
    "objectID": "docker.html#docker-image-is-a-blueprint-for-your-container",
    "href": "docker.html#docker-image-is-a-blueprint-for-your-container",
    "title": "10  Docker recreates the same environment for your code in any machine",
    "section": "",
    "text": "Docker image\n\n\n\n\n\nFROM: We need a base operating system on which to set our configurations. We can also use existing Docker images available at the Docker Hub and add our config on top of them. In our example, we use the official Python Docker image.\nCOPY: Copy is used to copy files or folders from our local filesystem to the image. The copy command is usually used when building the docker image to copy settings, static files, etc. In our example, spark configs, entrypoint.sh, and iceberg yaml settings.\nENV: This command sets the image’s environment variables. In our example, we set Python, and Spark Paths.\nENTRYPOINT: The entrypoint command executes a script when the image starts. In our example, we use a script file (entrypoint.sh) to start spark master and worker nodes add entrypoint.sh.",
    "crumbs": [
      "Working in a team",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Docker recreates the same environment for your code in any machine</span>"
    ]
  },
  {
    "objectID": "docker.html#start-containers-based-on-docker-image",
    "href": "docker.html#start-containers-based-on-docker-image",
    "title": "10  Docker recreates the same environment for your code in any machine",
    "section": "10.2 Start containers based on docker image",
    "text": "10.2 Start containers based on docker image\nWe use images to create docker containers. We can create one or more containers from a given image.\n\n10.2.1 Communicate between containers and local OS\nTypically, with data infra, we need multiple systems to run simultaneously. Most data systems also expose runtime information, documentation, etc via ports. We have to inform docker which ports to keep open so that they are accessible from the “outside”, in our case your local browser.\nWhen we are developing, we’d want to make changes to the code and see its impact immediately. While you can use COPY to copy over your code when building a docker image, it will not reflect changes in real time and you will have to rebuild your container each time you need change your code.\nIn cases where you want data/code to sync 2 ways between your local machine and the running docker container use mounted volumes. In addition to syncing local files, volumes also sync files between our containers.\n\n\n\ndocker port\n\n\n\n\n10.2.2 Start containers with docker CLI or compose\nWe can use the docker cli to start containers based on an image. Let’s look at an example. To start our Spark master container, we can use the following:\ndocker run -d \\\n  --name spark-master \\\n  --entrypoint ./entrypoint.sh \\\n  -p 4040:4040 \\\n  -p 9090:8080 \\\n  -p 7077:7077 \\\n  --env-file .env.spark \\\n  spark-image master\nHowever, with most data systems, we will need to ensure multiple systems are running. While we can use docker cli to do this, a better option is to use docker compose to orchestrate the different containers required. With docker compose, we can define all our settings in one file and ensure that they are started in the order we prefer.\nOur docker compose is defined here. With our docker compose defined, starting our containers is a simple command, as shown below:\ndocker compose up -d\nThe command will, by default, look for a file called docker-compose.yml in the directory in which it is run.\n\n\n10.2.3 Containers can be always-running or short-lived\nDepending on what you want your containers to do, they can either be short lived (start, run a process, stop) or long lived (start, start spark master and worker nodes, wait for data processing request). Most data infra is long lived.\n\n\n10.2.4 Executing commands in your docker container\nUsing the exec command, you can submit commands to be run in a specific container. For example, we can use the following to open a bash terminal in our spark-master container:\ndocker exec -ti spark-master bash\n# You will be in the master container bash shell\n# try some commands\npwd \nexit # exit the container\nNote that the -ti indicates that this will be run in an interactive mode. As shown below, we can run a command without interactive mode and get an output.\ndocker exec spark-master echo hello\n# prints hello",
    "crumbs": [
      "Working in a team",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Docker recreates the same environment for your code in any machine</span>"
    ]
  },
  {
    "objectID": "so.html#schedulers-run-data-pipelines-at-specified-frequency",
    "href": "so.html#schedulers-run-data-pipelines-at-specified-frequency",
    "title": "11  Airflow is both a scheduler and an orchestrator",
    "section": "",
    "text": "with DAG(\n    \"user_analytics_dag\",\n    description=\"A DAG to Pull user data and movie review data \\\n        to analyze their behaviour\",\n    schedule_interval=timedelta(days=1),\n    start_date=datetime(2023, 1, 1),\n    catchup=False,\n) as dag:",
    "crumbs": [
      "Scheduler defines when & Orchestrator defines how to, run your data pipelines",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Airflow is both a scheduler and an orchestrator</span>"
    ]
  },
  {
    "objectID": "so.html#orchestrators-define-the-order-of-execution-of-your-pipeline-tasks",
    "href": "so.html#orchestrators-define-the-order-of-execution-of-your-pipeline-tasks",
    "title": "11  Airflow is both a scheduler and an orchestrator",
    "section": "11.2 Orchestrators define the order of execution of your pipeline tasks",
    "text": "11.2 Orchestrators define the order of execution of your pipeline tasks\nWith complex data pipelines, we want parts of our pipelines to run in a specific order. For example, if your pipeline is pulling data from multiple independent sources, we would want them to run in parallel. Apache Airflow enables us to chain parts of our code to run in parallel or sequentially as needed.\n\n11.2.1 Define the order of execution of pipeline tasks with a DAG\nData pipelines are DAGs, i.e., they consist of a series of tasks that need to be run in a specified order without any cyclic dependencies.\nA Directed Acyclic Graph (DAG) is a directed graph with no directed cycles. It consists of vertices and edges, with each edge directed from one vertex to another, so following those directions will never form a closed loop. (ref: wiki)\n\n\n\nDAG\n\n\nIn data pipelines, we use the following terminology: 1. DAG: Represents an entire data pipeline. 2. Task: Individual node in a DAG. The tasks usually correspond to some data task. 3. Dependency: The edges between nodes represent the dependency between tasks. 1. Upstream: All the tasks that run before the task under consideration. 2. Downstream: All the tasks that run after the task under consideration.\nIn our example, if we consider the movie_classifier task, we can see its upstream and downstream tasks as shown below.\n\n\n\nDAG Dependency\n\n\nWith DAG, we can define dependencies, i.e., we can define when a task runs depending on upstream tasks (ref: trigger rules)\nWe can also set individual task-level settings, such as the number of retries per task and branch logic, where you can define logic to choose one or more tasks out of multiple tasks. We can dynamically create tasks based on your logic.\nIn our code we define our DAG using the &gt;&gt; syntax (ref link).\n# Define the tasks \ncreate_s3_bucket &gt;&gt; [user_purchase_to_s3, movie_review_to_s3]\n\nuser_purchase_to_s3 &gt;&gt; get_user_purchase_to_warehouse\n\nmovie_review_to_s3 &gt;&gt; movie_classifier &gt;&gt; get_movie_review_to_warehouse\n\n(\n    [get_user_purchase_to_warehouse, get_movie_review_to_warehouse]\n    &gt;&gt; get_user_behaviour_metric\n    &gt;&gt; gen_dashboard\n)\n\n\n11.2.2 Define where to run your code\nWhen we run our DAG, each task will be run individually. Airflow enables us to run our tasks in multiple ways:\n\nRun code in the same machine as your scheduler process with Local and sequential executor\nRun code in a task queue (i.e. a system that will run tasks in individual machines) with a celery executor.\nRun code as k8s pods with Kubernetes executor.\nWrite custom logic to run your tasks.\n\nSee this link for more details about running your tasks. In our project, we use the default SequentialExecutor, which is set up by default.\n\n\n11.2.3 Use operators to connect to popular services\nMost data pipelines involve talking to an external system, such as a cloud storage system (S3), data warehouse (Snowflake), or data processing system (Apache Spark).\nWhile we can write code to work with these external systems, Apache Airflow provides a robust set of Operators for most services we can reuse.\nIn our code, we use multiple Airflow operators to reduce the amount of code we have to write (ref code):\ncreate_s3_bucket = S3CreateBucketOperator(\n  task_id=\"create_s3_bucket\", bucket_name=user_analytics_bucket\n)\nmovie_review_to_s3 = LocalFilesystemToS3Operator(\n  task_id=\"movie_review_to_s3\",\n  filename=\"/opt/airflow/data/movie_review.csv\",\n  dest_key=\"raw/movie_review.csv\",\n  dest_bucket=user_analytics_bucket,\n  replace=True,\n)\n\nuser_purchase_to_s3 = SqlToS3Operator(\n  task_id=\"user_purchase_to_s3\",\n  sql_conn_id=\"postgres_default\",\n  query=\"select * from retail.user_purchase\",\n  s3_bucket=user_analytics_bucket,\n  s3_key=\"raw/user_purchase/user_purchase.csv\",\n  replace=True,\n)\nThe above code shows how we use Airflow operators to create S3 buckets and copy data from local files and a Postgres db into an S3 bucket. See here for a list of Airflow operators.",
    "crumbs": [
      "Scheduler defines when & Orchestrator defines how to, run your data pipelines",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Airflow is both a scheduler and an orchestrator</span>"
    ]
  },
  {
    "objectID": "airflow.html",
    "href": "airflow.html",
    "title": "11  Airflow is both a scheduler and an orchestrator",
    "section": "",
    "text": "11.1 Schedulers run data pipelines at specified frequency\nData pipelines will need to be run at specific intervals. The frequency required may be as simple as hourly, daily, weekly, monthly, etc., or complex, e.g., 2nd Tuesday of every month.\nAirflow uses a process called Scheduler that checks our DAGs(data pipeline) every minute (add: config setting) to see if it needs to be started.\nIn Airflow you can define when the pipeline should be run with a cron format or python timedelta. add:links\nIn our code, we define our pipeline to run every day(ref link).",
    "crumbs": [
      "Scheduler defines when & Orchestrator defines how to, run your data pipelines",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Airflow is both a scheduler and an orchestrator</span>"
    ]
  },
  {
    "objectID": "airflow.html#schedulers-run-data-pipelines-at-specified-frequency",
    "href": "airflow.html#schedulers-run-data-pipelines-at-specified-frequency",
    "title": "11  Airflow is both a scheduler and an orchestrator",
    "section": "",
    "text": "with DAG(\n    \"user_analytics_dag\",\n    description=\"A DAG to Pull user data and movie review data \\\n        to analyze their behaviour\",\n    schedule_interval=timedelta(days=1),\n    start_date=datetime(2023, 1, 1),\n    catchup=False,\n) as dag:",
    "crumbs": [
      "Scheduler defines when & Orchestrator defines how to, run your data pipelines",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Airflow is both a scheduler and an orchestrator</span>"
    ]
  },
  {
    "objectID": "airflow.html#orchestrators-define-the-order-of-execution-of-your-pipeline-tasks",
    "href": "airflow.html#orchestrators-define-the-order-of-execution-of-your-pipeline-tasks",
    "title": "11  Airflow is both a scheduler and an orchestrator",
    "section": "11.2 Orchestrators define the order of execution of your pipeline tasks",
    "text": "11.2 Orchestrators define the order of execution of your pipeline tasks\nWith complex data pipelines, we want parts of our pipelines to run in a specific order. For example, if your pipeline is pulling data from multiple independent sources, we may want to run them in parallel.\n\n11.2.1 Define the order of execution of pipeline tasks with a DAG\nDAG stands for Directed Acyclic Graph, i.e a series of steps that flow one way with a defined end condition. DAG is also Airflow code API that you can use to define your pipeline.\n\n\n\nDAG\n\n\nIn data pipelines, we use the following terminology: 1. DAG: Represents an entire data pipeline. 2. Task: Individual node in a DAG. The tasks usually correspond to some data task. 3. Dependency: The edges between nodes represent the dependency between tasks. 1. Upstream: All the tasks that run before the task under consideration. 2. Downstream: All the tasks that run after the task under consideration.\nIn our example, if we consider the movie_classifier task, we can see its upstream and downstream tasks as shown below.\n\n\n\nDAG Dependency\n\n\nWith DAG, we can define dependencies, i.e., we can define when a task runs depending on upstream tasks (ref: trigger rules)\nWe can also set individual task-level settings, such as the number of retries per task and branch logic, where you can define logic to choose one or more tasks out of multiple tasks. We can dynamically create tasks based on your logic.\nIn our code we define our DAG using the &gt;&gt; syntax (ref link).\n# Define the tasks \ncreate_s3_bucket &gt;&gt; [user_purchase_to_s3, movie_review_to_s3]\n\nuser_purchase_to_s3 &gt;&gt; get_user_purchase_to_warehouse\n\nmovie_review_to_s3 &gt;&gt; movie_classifier &gt;&gt; get_movie_review_to_warehouse\n\n(\n    [get_user_purchase_to_warehouse, get_movie_review_to_warehouse]\n    &gt;&gt; get_user_behaviour_metric\n    &gt;&gt; gen_dashboard\n)\n\n\n11.2.2 Define where to run your code\nWhen we run our DAG, each task will be run individually. Airflow enables us to run our tasks in multiple ways:\n\nRun code in the same machine as your scheduler process with Local and sequential executor\nRun code in a task queue (i.e. a system that will run tasks in individual machines) with a celery executor.\nRun code as k8s pods with Kubernetes executor.\nWrite custom logic to run your tasks.\n\nSee this link for more details about running your tasks. In our project, we use the default LocalExecutor, which is set up by default.\n\n\n11.2.3 Use operators to connect to popular services\nAs we saw in the Python section, data pipelines involve interacting with external system, such as a cloud storage system (S3), data warehouse (Snowflake), or data processing system (Apache Spark).\nWhile we can write code to work with these external systems, Apache Airflow provides a robust set of Operators for most services we can reuse.\nIn our code, we use multiple Airflow operators to reduce the amount of code we have to write (ref code):\ncreate_s3_bucket = S3CreateBucketOperator(\n  task_id=\"create_s3_bucket\", bucket_name=user_analytics_bucket\n)\nmovie_review_to_s3 = LocalFilesystemToS3Operator(\n  task_id=\"movie_review_to_s3\",\n  filename=\"/opt/airflow/data/movie_review.csv\",\n  dest_key=\"raw/movie_review.csv\",\n  dest_bucket=user_analytics_bucket,\n  replace=True,\n)\n\nuser_purchase_to_s3 = SqlToS3Operator(\n  task_id=\"user_purchase_to_s3\",\n  sql_conn_id=\"postgres_default\",\n  query=\"select * from retail.user_purchase\",\n  s3_bucket=user_analytics_bucket,\n  s3_key=\"raw/user_purchase/user_purchase.csv\",\n  replace=True,\n)\nThe above code shows how we use Airflow operators to create S3 buckets and copy data from local files and a Postgres db into an S3 bucket. See here for a list of Airflow operators.",
    "crumbs": [
      "Scheduler defines when & Orchestrator defines how to, run your data pipelines",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Airflow is both a scheduler and an orchestrator</span>"
    ]
  },
  {
    "objectID": "airflow.html#observability-to-see-how-your-pipelines-are-running",
    "href": "airflow.html#observability-to-see-how-your-pipelines-are-running",
    "title": "11  Airflow is both a scheduler and an orchestrator",
    "section": "11.3 Observability to see how your pipelines are running",
    "text": "11.3 Observability to see how your pipelines are running\nWhen a data pipeline runs, all information is stored in a metadata db and as logs. The historical information allows us to observe your data pipelines’ current state and historical state.\nHere is a list of the tables used to store metadata about our pipelines:\n\n\n\nMetadata DB\n\n\nApache Airflow enables us to store logs in multiple locations (ref docs). In our project we store them locally in the file system, which can be accessed by clicking on a specific task -&gt; Logs as shown below.\n\n\n\nSpark logs\n\n\n\n11.3.1 See progress & historical information on UI\nWhen we run data pipelines, we can use a nice web UI to see the progress, failures, and other details. Powered by the metadata db and the logs, we can also see individual task logs and the inputs to a specific task, among others.\nThe web UI provides good visibility into our pipelines’ current and historical state.\n\n\n\nDAG logs\n\n\n\n\n\nTask inputs\n\n\n\n\n11.3.2 Analyze data pipeline performance with Web UI\nWe can see how pipelines have performed over time, inspect task run time, and see how long a task had to wait to get started. The performance metrics provide us with the necessary insights to optimize our systems.\n\n\n\nPerformance\n\n\n\n\n11.3.3 Re-run data pipelines via UI\nIn addition to seeing how our pipelines are running, we can manually trigger DAGs with custom inputs as necessary. The ability to trigger/re-run DAGs helps us quickly resolve one-off issues. See this link for information on triggering dags with UI and CLI.\n\n\n\nTriggers\n\n\n\n\n11.3.4 Reuse variables and connections across your pipelines\nApache Airflow also allows us to create and store variables and connection settings that can be reused across our data pipelines. In our code, we create variables using Airflow CLI here.\nOnce the connection/variables are set, we can see them in our UI:\n\n\n\nConnection\n\n\n\n\n11.3.5 Define who can view/edit your data pipelines with access control\nWhen managing Airflow used by multiple people, it can be beneficial to have some people have limited access to the data pipeline. For example, you want to avoid a stakeholder being able to stop or delete a DAG accidentally.\nSee this link for details on access control.",
    "crumbs": [
      "Scheduler defines when & Orchestrator defines how to, run your data pipelines",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Airflow is both a scheduler and an orchestrator</span>"
    ]
  },
  {
    "objectID": "dw_table_types.html",
    "href": "dw_table_types.html",
    "title": "8  Data warehouse modeling (Kimball) is based off of 2 types of tables: Fact and dimensions",
    "section": "",
    "text": "8.1 Facts represents events that occured & dimensions the entities to which events occur to\nA data warehouse is a database that stores your company’s historical data. The main types of tables you need to create to power analytics are:\nA fact table’s grain (aka granularity, level) refers to what a row in a fact table represents. E.g., In our checkout process, we can have two fact tables, one for the order and another for the individual items in the order. The items table will have one row per item purchased, whereas the order table will have one row per order made.\nNote: If you notice the slight difference in the decimal digits, it’s due to using a double datatype which is an inexact data type.\nWe can see how the lineitem table can be “rolled up” to get the data in the orders table. But having just the orders table is not sufficient since the lineitem table will provide us with individual item details such as discount and quantity details.",
    "crumbs": [
      "Data modeling is the process of getting data ready for analytics",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Data warehouse modeling (Kimball) is based off of 2 types of tables: Fact and dimensions</span>"
    ]
  },
  {
    "objectID": "dw_table_types.html#facts-represents-events-that-occured-dimensions-the-entities-to-which-events-occur-to",
    "href": "dw_table_types.html#facts-represents-events-that-occured-dimensions-the-entities-to-which-events-occur-to",
    "title": "8  Data warehouse modeling (Kimball) is based off of 2 types of tables: Fact and dimensions",
    "section": "",
    "text": "Dimension: Each row in a dimension table represents a business entity that is important to the business. For example, An car parts seller’s data warehouse will have a customer dimension table, where each row will represent an individual customer. Other examples of dimension tables in a car parts seller’s data warehouse would be supplier & part tables.\nFacts: Each row in a fact table represents a business process that occurred. E.g., In our data warehouse, each row in the orders fact table will represent an individual order, and each row in the lineitem fact table will represent an item sold as part of an order. Each fact row will have a unique identifier; in our case, it’s orderkey for orders and a combination of orderkey & linenumber for lineitem.\n\n\n\n\n-- calculating the totalprice of an order (with orderkey = 1) from it's individual items\nSELECT\n    orderkey,\n    round( sum(extendedprice * (1 - discount) * (1 + tax)),\n        2\n    ) AS totalprice\nFROM\n    lineitem\nWHERE\n    orderkey = 1\nGROUP BY\n    orderkey;\n\n/*\n orderkey | totalprice\n----------+------------\n        1 |  172799.56\n*/\n\n-- The totalprice of an order (with orderkey = 1)\nSELECT\n    orderkey,\n    totalprice\nFROM\n    orders\nWHERE\n    orderkey = 1;\n\n/*\n orderkey | totalprice\n----------+------------\n        1 |  172799.49\n*/\n\n\n\n8.1.1 Popular dimension types: Full Snapshot & SCD2\n\nFull snapshot In this type of dimension, the entire dimension table is re-loaded each run. As the dimension tables are much smaller than the fact table this is usually an acceptable tradeoff. Typically each run would create a new copy while retaining older copy for a certain time period (say 6 months).\nSCD2 SCD2 stands for slowly changing dimension type 2. Any change to a column value will be tracked as a new row.\n\nIf your customer makes an address change in SCD2 it will be created as a new table. SCD2 has 3 key columns that allow us to see historical changes\n\nvalid_from\nvalid_to\nis_current\n\nadd: image showing snapshot dimension and SCD2 dimension model",
    "crumbs": [
      "Data modeling is the process of getting data ready for analytics",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Data warehouse modeling (Kimball) is based off of 2 types of tables: Fact and dimensions</span>"
    ]
  },
  {
    "objectID": "dw_table_types.html#one-big-table-obt-is-a-fact-table-left-joined-with-all-its-dimensions",
    "href": "dw_table_types.html#one-big-table-obt-is-a-fact-table-left-joined-with-all-its-dimensions",
    "title": "8  Data warehouse modeling (Kimball) is based off of 2 types of tables: Fact and dimensions",
    "section": "8.2 One Big Table (OBT) is a fact table left joined with all its dimensions",
    "text": "8.2 One Big Table (OBT) is a fact table left joined with all its dimensions\nAs the number of facts and dimensions grow you will notice that most of the queries that are run to get data for the end users use the same tables and the same joins.\nIn this scenario the expensive reprocessing of data can be avoided by creating an OBT. In an OBT you left join all the dimensions into a fact table. This big table can then be used to aggregate to different grains as needed for end user reproting.\nNote that the OBT should have the same grain as the fact table that it is based on or have the lower grain if you have to join multiple fact tables.\nIn our bike-part seller warehouse we can create an OBT by joining all the tables to the lineitem table\nadd: code",
    "crumbs": [
      "Data modeling is the process of getting data ready for analytics",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Data warehouse modeling (Kimball) is based off of 2 types of tables: Fact and dimensions</span>"
    ]
  },
  {
    "objectID": "dw_table_types.html#summary-or-pre-aggregated-tables-are-stakeholder-team-specific-tables-built-for-reporting",
    "href": "dw_table_types.html#summary-or-pre-aggregated-tables-are-stakeholder-team-specific-tables-built-for-reporting",
    "title": "8  Data warehouse modeling (Kimball) is based off of 2 types of tables: Fact and dimensions",
    "section": "8.3 Summary or pre-aggregated tables are stakeholder-team specific tables built for reporting",
    "text": "8.3 Summary or pre-aggregated tables are stakeholder-team specific tables built for reporting\nStakeholders often require data aggregated at various grains and similar metrics. Creating pre-aggregated or summary tables is creating these report for stakeholders so all they would have to do is select from the table without the need to recompute metrics. This has 2 benefits\n\nSame metric formula, as the data engineering will keep the metric definition in the code base, vs each stakeholder using a slightly different version and ending up with different numbers\nAvoid unnecessary recomputation as multiple stakeholders can now use the same table\n\nHowever the down side is that the data may not be as fresh as what a stakeholder would get if they just write a query.",
    "crumbs": [
      "Data modeling is the process of getting data ready for analytics",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Data warehouse modeling (Kimball) is based off of 2 types of tables: Fact and dimensions</span>"
    ]
  },
  {
    "objectID": "index.html#how-to-use-this-book",
    "href": "index.html#how-to-use-this-book",
    "title": "Data Engineering For Beginners",
    "section": "How to use this book",
    "text": "How to use this book\nThis book is written in order to guide you from not knowing much about data engineering to being proficient in the core ideas that underpins modern data engineering.\nI recommend you read the book in order and follow along with the code examples. Each chapter has exercises, the solutions for which you will receive via emails (sign up here add:link).",
    "crumbs": [
      "Start here"
    ]
  },
  {
    "objectID": "index.html#setup",
    "href": "index.html#setup",
    "title": "Data Engineering For Beginners",
    "section": "Setup",
    "text": "Setup\nFollow the steps here for setup.\nIn order to follow along with code, please make a copy of this starter notebook and follow along.",
    "crumbs": [
      "Start here"
    ]
  },
  {
    "objectID": "dbt.html#define-your-project-setting-at-dbt_project.yml",
    "href": "dbt.html#define-your-project-setting-at-dbt_project.yml",
    "title": "12  dbt-core is an orchestrator that makes managing pipelines simpler",
    "section": "12.4 Define your project setting at dbt_project.yml",
    "text": "12.4 Define your project setting at dbt_project.yml\nIn dbt_project.yml file you specify all the project specific settings that you want applied, such as\n\nThe folders to look for the .sql files\nThe folder to look for seeds, downloaded packages, SQL functions (aka macros), etc.\nHow to materialize a data model (ie. should a data model be created as a table/view/materialized view/temporal table, etc)\n\nMaterialization is a variable that controls how dbt creates a model. By default, every model will be a view. This can be overridden in dbt_project.yml. We have set the models under models/marts/core/ to materialize as tables.\n# Configuring models\nmodels:\n    sde_dbt_tutorial:\n        # Applies to all files under models/marts/core/\n        marts:\n            core:\n                materialized: table\nIf you need to define how your dbt project",
    "crumbs": [
      "Scheduler defines when & Orchestrator defines how to, run your data pipelines",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>dbt-core is an orchestrator that makes managing pipelines simpler</span>"
    ]
  },
  {
    "objectID": "dbt.html#dbt-recommends-the-3-hop-architecture-with-stage-core-data-marts",
    "href": "dbt.html#dbt-recommends-the-3-hop-architecture-with-stage-core-data-marts",
    "title": "12  dbt-core is an orchestrator that makes managing pipelines simpler",
    "section": "12.5 dbt recommends the 3-hop architecture with stage, core & data marts",
    "text": "12.5 dbt recommends the 3-hop architecture with stage, core & data marts\nWe will see how the customer_orders table is created from the source tables. These transformations follow warehouse and dbt best practices.\n\n12.5.1 Source\nSource tables refer to tables loaded into the warehouse by an EL process. In our case these are the base tpch tables, which are created by the extract step.\nWe need to define what tables are the sources in the src.yml file, this will be used by the stage tables with source function.\nsource add:\nadd: usage with source function\n\n\n12.5.2 Staging\nThe staging area is where raw data is cast into correct data types, given consistent column names, and prepared to be transformed into models used by end-users.\nYou can think of this stage as the first layer of transformations. We will place staging data models inside the staging folder, as shown below.\nadd: folder path\nTheir documentation and tests will be defined in a yaml file, as shown below.\nadd: staging yaml\n\n\n12.5.3 Marts\nMarts consist of the core tables for end-users and business vertical-specific tables.\n\n12.5.3.1 Core\nThe core defines the fact and dimension models to be used by end-users. We define our facts and tables under the marts/core folder. add: folder path.\nYou can see that we store the facts, dimensions and OBT under this folder.\n\n\n12.5.3.2 Stakeholder team specific\nIn this section, we define the models for marketing stakeholders, A project can have multiple business verticals. Having one folder per business vertical provides an easy way to organize the models.\nIn our example we store the metrics.sql in this location.",
    "crumbs": [
      "Scheduler defines when & Orchestrator defines how to, run your data pipelines",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>dbt-core is an orchestrator that makes managing pipelines simpler</span>"
    ]
  },
  {
    "objectID": "dbt.html#dbt-core-is-a-cli",
    "href": "dbt.html#dbt-core-is-a-cli",
    "title": "12  dbt-core is an orchestrator that makes managing pipelines simpler",
    "section": "12.6 dbt-core is a cli",
    "text": "12.6 dbt-core is a cli\nWith all our data model defined, we can use the dbt cli to run, test and create documentation. dbt command will look for the profiles.yml file in your $HOME directory by default so we either have to set the PROFILES_DIR environment variable (add: docekr file command) or use the --profiles-dir as part of the cli command.\n\n12.6.1 dbt run\nWe have the necessary model definitions in place. Let’s create the models.\ndbt run \n# Finished running 5 view models, 2 table models, 2 hooks in 0 hours 0 minutes and 3.22 seconds (3.22s).\nOur staging and marketing models are as materialized views, and the two core models are materialized as views as defined in ourlized as views as defined in our dbt_project.yml.\n\n\n12.6.2 dbt test\nWith the models defined, we can run tests on them. Note that, unlike standard testing, these tests run after the data has been processed. You can run tests as shown below.\ndbt test # or run \"just test\"\n# Finished running 14 tests...\n\n\n12.6.3 dbt docs\nOne of the powerful features of dbt is its docs. To generate documentation and serve them, run the following commands:\ndbt docs generate\ndbt docs serve\nThe generate command will create documentation in html format. The serve command will start a webserver that serves this html file.\nNavigate to customer_orders within the sde_dbt_tutorial project in the left pane. Click on the view lineage graph icon on the lower right side. The lineage graph shows the dependencies of a model. You can also see the tests defined, descriptions (set in the corresponding YAML file), and the compiled sql statements.\n\n\n\nour project structure",
    "crumbs": [
      "Scheduler defines when & Orchestrator defines how to, run your data pipelines",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>dbt-core is an orchestrator that makes managing pipelines simpler</span>"
    ]
  },
  {
    "objectID": "dbt.html#scheduling",
    "href": "dbt.html#scheduling",
    "title": "12  dbt-core is an orchestrator that makes managing pipelines simpler",
    "section": "12.7 Scheduling",
    "text": "12.7 Scheduling\nWe have seen how to create snapshots, models, run tests and generate documentation. These are all commands run via the cli. Dbt compiles the models into sql queries under the target folder (not part of git repo) and executes them on the data warehouse.\nTo schedule dbt runs, snapshots, and tests we need to use a scheduler. In the final capstone project we will use Airflow to schedule this dbt pipeline.",
    "crumbs": [
      "Scheduler defines when & Orchestrator defines how to, run your data pipelines",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>dbt-core is an orchestrator that makes managing pipelines simpler</span>"
    ]
  },
  {
    "objectID": "windows.html#window-functions-have-four-parts",
    "href": "windows.html#window-functions-have-four-parts",
    "title": "3  Use window function when you need to use values from other rows to compute a value for the current row",
    "section": "",
    "text": "Partition: Defines a set of rows based on specified column(s) value. If no partition is specified, the entire table is considered a partition.\nOrder By: This optional clause specifies how to order the rows within a partition. This is an optional clause, without this the rows inside a partition will not be ordered.\nFunction: The function to be applied on the current row.\nWindow frame: Within a partition, a window frame allows you to specify the rows to be considered in the function computation. This enables more options on how one can choose the rows to apply the function on.\n\n\n\n\n\nCreate window function\n\n\nSELECT\n  o_custkey,\n  o_orderdate,\n  o_totalprice,\n  SUM(o_totalprice) -- FUNCTION \n  OVER (\n    PARTITION BY\n      o_custkey -- PARTITION\n    ORDER BY\n      o_orderdate -- ORDER BY; ASCENDING ORDER unless specified as DESC\n  ) AS running_sum\nFROM\n  orders\nWHERE\n  o_custkey = 4\nORDER BY\n  o_orderdate\nLIMIT\n  10;\n\n\n\n\n\n\n3.1.1 Use window frames to define a set of rows to operate on\nWindow functions consider all the rows in a partition (depending on the type of function add:link) by default. However using a window frame one can select a set of rows withing a partition to operate on.\n\n\n\nThree order Sliding window average\n\n\nExample\nConsider a scenario where you have sales data, and you want to calculate a 3-day moving average of sales within each store:\nSELECT\n    store_id,\n    sale_date,\n    sales_amount,\n    AVG(sales_amount) OVER (\n        PARTITION BY store_id\n        ORDER BY sale_date\n        ROWS BETWEEN 2 PRECEDING AND CURRENT ROW\n    ) AS moving_avg_sales\nFROM\n    sales;\nIn this example:\n\nPARTITION BY store_id ensures the calculation is done separately for each store.\nORDER BY sale_date defines the order of rows within each partition.\nROWS BETWEEN 2 PRECEDING AND CURRENT ROW specifies the window frame, considering the current row and the two preceding rows to calculate the moving average. add: image Without defining the window frame, the function might not be able to provide the specific moving average calculation you need.\n\n\n3.1.1.1 Use ordering of rows to define your window frame with the ROWS clause\n\nROWS: Used to select a set of rows relative to the current row based on position.\n\nRow definition format ROWS BETWEEN start_point AND end_point.\nThe start_point and end_point can be any of the following three (in the proper order:\n\nn PRECEDING: n rows preceding the current row. UNBOUNDED PRECEDING indicates all rows before the current row.\nn FOLLOWING: n rows following the current row. UNBOUNDED FOLLOWING indicates all rows after the current row.\n\n\n\nLet’s see how relative row numbers can be used to define a window range.\nConsider this window function\nAVG(total_price) OVER ( -- FUNCTION: RUNNING AVERAGE\n    PARTITION BY o_custkey -- PARTITIONED BY customer\n    ORDER BY order_month \n    ROWS BETWEEN 1 PRECEDING AND 1 FOLLOWING -- WINDOW FRAME DEFINED AS 1 ROW PRECEDING to 1 ROW FOLLOWING\n    )\n\n\n\nWindow frame with ROWS\n\n\n\n\n3.1.1.2 Use values of the columns to define window frame using RANGE clause\n\nRANGE: Used to select a set of rows relative to the current row based on the value of the columns specified in the ORDER BY clause.\n\nRange definition format RANGE BETWEEN start_point AND end_point.\nThe start_point and end_point can be any of the following:\n\nCURRENT ROW: The current row.\nn PRECEDING: All rows with values within the specified range that are less than or equal to n units preceding the value of the current row.\nn FOLLOWING: All rows with values within the specified range that are greater than or equal to n units following the value of the current row.\nUNBOUNDED PRECEDING: All rows before the current row within the partition.\nUNBOUNDED FOLLOWING: All rows after the current row within the partition.\n\nRANGE is particularly useful when dealing with numeric or date/time ranges, allowing for calculations like running totals, moving averages, or cumulative distributions.\n\n\nLet’s see how RANGE works with AVG(total price) OVER (PARTITION BY customer id ORDER BY date RANGE BETWEEN INTERVAL '1' DAY PRECEDING AND '1' DAY FOLLOWING) using the below visualization:\n\n\n\nRANGE",
    "crumbs": [
      "Use SQL to transform data",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Use window function when you need to use values from other rows to compute a value for the current row</span>"
    ]
  },
  {
    "objectID": "windows.html#ranking-functions-enable-you-to-rank-your-rows-based-on-order-by",
    "href": "windows.html#ranking-functions-enable-you-to-rank-your-rows-based-on-order-by",
    "title": "3  Use window function when you need to use values from other rows to compute a value for the current row",
    "section": "3.2 Ranking functions enable you to rank your rows based on order by",
    "text": "3.2 Ranking functions enable you to rank your rows based on order by\nIf you are working on a problem to get the top/bottom n rows (as defined by some value) then use the row functions.\nLet’s look at an example of how to use a row function:\nFrom the orders table get the top 3 spending customers per day. The orders table schema is shown below:\n\n\n\nOrders table\n\n\n\n3.2.1 [Example]\nSELECT\n  *\nFROM\n  (\n    SELECT\n      o_orderdate,\n      o_totalprice,\n      o_custkey,\n      RANK() -- RANKING FUNCTION \n      OVER (\n        PARTITION BY\n          o_orderdate -- PARTITION BY order date\n        ORDER BY\n          o_totalprice DESC -- ORDER rows withing partition by totalprice\n      ) AS rnk\n    FROM\n      orders\n  )\nWHERE\n  rnk &lt;= 3\nORDER BY\n  o_orderdate\nLIMIT\n  5;\n\n\n3.2.2 [Exercise] From the orders table get the 3 lowest spending customers per day\nTime limit during live workshop: 5 min\nHint 1. Figure out the PARTITION BY column first, then the ORDER BY column and finally the FUNCTION to use to compute running average.\nThe orders table schema is shown below:\n\n\n\nOrders table\n\n\n-- your code here\n\n\n3.2.3 Standard RANKING functions:\n\nRANK: Ranks the rows starting from 1 to n within the window frame. Ranks the rows with the same value (defined by the “ORDER BY” clause) as the same and skips the ranking numbers that would have been present if the values were different.\nDENSE_RANK: Ranks the rows starting from 1 to n within the window frame. Ranks the rows with the same value (defined by the “ORDER BY” clause) as the same and does not skip any ranking numbers.\nROW_NUMBER: Adds a row number that starts from 1 to n within the window frame and does not create any repeating values.\n\n\n\n3.2.4 [Example]\n-- Let's look at an example showing the difference between RANK, DENSE_RANK and ROW_NUMBER\nSELECT \n    order_date,\n    order_id,\n    total_price,\n    ROW_NUMBER() OVER (PARTITION BY order_date ORDER BY total_price) AS row_number,\n    RANK() OVER (PARTITION BY order_date ORDER BY total_price) AS rank,\n    DENSE_RANK() OVER (PARTITION BY order_date ORDER BY total_price) AS dense_rank\nFROM (\n    SELECT \n        '2024-07-08' AS order_date, 'order_1' AS order_id, 100 AS total_price UNION ALL\n    SELECT \n        '2024-07-08', 'order_2', 200 UNION ALL\n    SELECT \n        '2024-07-08', 'order_3', 150 UNION ALL\n    SELECT \n        '2024-07-08', 'order_4', 90 UNION ALL\n    SELECT \n        '2024-07-08', 'order_5', 100 UNION ALL\n    SELECT \n        '2024-07-08', 'order_6', 90 UNION ALL\n    SELECT \n        '2024-07-08', 'order_7', 100 UNION ALL\n    SELECT \n        '2024-07-10', 'order_8', 100 UNION ALL\n    SELECT \n        '2024-07-10', 'order_9', 100 UNION ALL\n    SELECT \n        '2024-07-10', 'order_10', 100 UNION ALL\n    SELECT \n        '2024-07-11', 'order_11', 100\n) AS orders\nORDER BY order_date, row_number;\nNow that we have see how to define a window function and how to use ranking and aggregation functions, let’s take it a step further by practicing value functions.\nRemember that value functions are used to access other row’s values while operating on the current row\nLet’s take a look at LEAD and LAG functions:\n\n\n\nLAG AND LEAD\n\n\n\n\n3.2.5 [Exercise] Write a SQL query using the orders table that calculates the following columns:\n1. o_orderdate: From orders table\n2. o_custkey: From orders table\n3. o_totalprice: From orders table\n4. totalprice_diff: The customers current day's o_totalprice - that same customers most recent previous purchase's o_totalprice\n\nTime limit during live workshop: 5 min\nHint:\n\nStart by figuring out what the PARTITION BY column should be, then what the ORDER BY column should be, and then finally the function to use.\nUse the LAG(column_name) ranking function to identify the prior day’s revenue.\n\nThe orders table schema is shown below:\n\n\n\n\nOrders table\n\n\n-- write your query here",
    "crumbs": [
      "Use SQL to transform data",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Use window function when you need to use values from other rows to compute a value for the current row</span>"
    ]
  },
  {
    "objectID": "windows.html#ranking-functions-enable-you-to-rank-your-rows-based-on-order-by-clause",
    "href": "windows.html#ranking-functions-enable-you-to-rank-your-rows-based-on-order-by-clause",
    "title": "3  Use window function when you need to use values from other rows to compute a value for the current row",
    "section": "3.2 Ranking functions enable you to rank your rows based on order by clause",
    "text": "3.2 Ranking functions enable you to rank your rows based on order by clause\nIf you are working on a problem to get the top/bottom n rows (as defined by some value) then use the row functions.\nLet’s look at an example of how to use a row function:\nFrom the orders table get the top 3 spending customers per day. The orders table schema is shown below:\n\n\n\nOrders table\n\n\nSELECT\n  *\nFROM\n  (\n    SELECT\n      o_orderdate,\n      o_totalprice,\n      o_custkey,\n      RANK() -- RANKING FUNCTION \n      OVER (\n        PARTITION BY\n          o_orderdate -- PARTITION BY order date\n        ORDER BY\n          o_totalprice DESC -- ORDER rows withing partition by totalprice\n      ) AS rnk\n    FROM\n      orders\n  )\nWHERE\n  rnk &lt;= 3\nORDER BY\n  o_orderdate\nLIMIT\n  5;\nStandard RANKING functions:\n\nRANK: Ranks the rows starting from 1 to n within the window frame. Ranks the rows with the same value (defined by the “ORDER BY” clause) as the same and skips the ranking numbers that would have been present if the values were different.\nDENSE_RANK: Ranks the rows starting from 1 to n within the window frame. Ranks the rows with the same value (defined by the “ORDER BY” clause) as the same and does not skip any ranking numbers.\nROW_NUMBER: Adds a row number that starts from 1 to n within the window frame and does not create any repeating values.\n\n[Example]\n-- Let's look at an example showing the difference between RANK, DENSE_RANK and ROW_NUMBER\nSELECT \n    order_date,\n    order_id,\n    total_price,\n    ROW_NUMBER() OVER (PARTITION BY order_date ORDER BY total_price) AS row_number,\n    RANK() OVER (PARTITION BY order_date ORDER BY total_price) AS rank,\n    DENSE_RANK() OVER (PARTITION BY order_date ORDER BY total_price) AS dense_rank\nFROM (\n    SELECT \n        '2024-07-08' AS order_date, 'order_1' AS order_id, 100 AS total_price UNION ALL\n    SELECT \n        '2024-07-08', 'order_2', 200 UNION ALL\n    SELECT \n        '2024-07-08', 'order_3', 150 UNION ALL\n    SELECT \n        '2024-07-08', 'order_4', 90 UNION ALL\n    SELECT \n        '2024-07-08', 'order_5', 100 UNION ALL\n    SELECT \n        '2024-07-08', 'order_6', 90 UNION ALL\n    SELECT \n        '2024-07-08', 'order_7', 100 UNION ALL\n    SELECT \n        '2024-07-10', 'order_8', 100 UNION ALL\n    SELECT \n        '2024-07-10', 'order_9', 100 UNION ALL\n    SELECT \n        '2024-07-10', 'order_10', 100 UNION ALL\n    SELECT \n        '2024-07-11', 'order_11', 100\n) AS orders\nORDER BY order_date, row_number;\nNow that we have see how to define a window function and how to use ranking and aggregation functions, let’s take it a step further by practicing value functions.\nRemember that value functions are used to access other row’s values while operating on the current row\nLet’s take a look at LEAD and LAG functions:\n\n\n\nLAG AND LEAD",
    "crumbs": [
      "Use SQL to transform data",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Use window function when you need to use values from other rows to compute a value for the current row</span>"
    ]
  },
  {
    "objectID": "data_model.html",
    "href": "data_model.html",
    "title": "Data modeling is the process of getting data ready for analytics",
    "section": "",
    "text": "As a data engineer your key objective is to enable stakeholders to be able to use data effectively to answer questions about business performance and predict how a business may perform in the future.\nMost companies production system store data in denormalized tables across multiple microservices, which make analytics hard as the data user will now be required to join across multiple tables. If your company uses a microservice architetcure this becomes impossible.\nAnalytical querying often require processing large amounts of data which can have a significant impact on the database performance which is usually unacceptable for production systems.\nProduction system ususaly only stores current state and does not store a log of changes, which is typically necessary for historical analysis\nMost companies produce event(click tracking, e-commerce ordering, server logs monitoring, etc ) which are usually too large to be stored and queried efficiently in a production database\nThese are some of the reasons you need a warehouse system to be able to analyze historical information.\nadd: image moving from denormalized tables to star schema to normalized tables",
    "crumbs": [
      "Data modeling is the process of getting data ready for analytics"
    ]
  },
  {
    "objectID": "index.html#data",
    "href": "index.html#data",
    "title": "Data Engineering For Beginners",
    "section": "Data",
    "text": "Data\nWe will use the TPCH dataset for exercises and example throughout this book. he TPC-H data represents a car parts seller’s data warehouse, where we record orders, items that make up that order (lineitem), supplier, customer, part (parts sold), region, nation, and partsupp (parts supplier).\nNote: Have a copy of the data model as you follow along; this will help in understanding the examples provided and in answering exercise questions.",
    "crumbs": [
      "Start here"
    ]
  },
  {
    "objectID": "airflow.html#user-interface-to-see-the-how-your-pipelines-are-running-and-their-history",
    "href": "airflow.html#user-interface-to-see-the-how-your-pipelines-are-running-and-their-history",
    "title": "11  Airflow is both a scheduler and an orchestrator",
    "section": "11.3 User interface to see the how your pipelines are running and their history",
    "text": "11.3 User interface to see the how your pipelines are running and their history\nWhen a data pipeline runs, all information is stored in a metadata db and as logs. The historical information allows us to observe your data pipelines’ current state and historical state.\nHere is a list of the tables used to store metadata about our pipelines:\n\n\n\nMetadata DB\n\n\nApache Airflow enables us to store logs in multiple locations (ref docs). In our project we store them locally in the file system, which can be accessed by clicking on a specific task -&gt; Logs as shown below.\n\n\n\nSpark logs\n\n\n\n11.3.1 See progress & historical information on UI\nWhen we run data pipelines, we can use a nice web UI to see the progress, failures, and other details. Powered by the metadata db and the logs, we can also see individual task logs and the inputs to a specific task, among others.\nThe web UI provides good visibility into our pipelines’ current and historical state.\n\n\n\nDAG logs\n\n\n\n\n\nTask inputs\n\n\n\n\n11.3.2 Analyze data pipeline performance with Web UI\nWe can see how pipelines have performed over time, inspect task run time, and see how long a task had to wait to get started. The performance metrics provide us with the necessary insights to optimize our systems.\n\n\n\nPerformance\n\n\n\n\n11.3.3 Re-run data pipelines via UI\nIn addition to seeing how our pipelines are running, we can manually trigger DAGs with custom inputs as necessary. The ability to trigger/re-run DAGs helps us quickly resolve one-off issues. See this link for information on triggering dags with UI and CLI.\n\n\n\nTriggers\n\n\n\n\n11.3.4 Reuse variables and connections across your pipelines\nApache Airflow also allows us to create and store variables and connection settings that can be reused across our data pipelines. In our code, we create variables using Airflow CLI here.\nVariables and connections are especially important when you want to use a variable across data pipelines and to connect to external systems respectively.\nOnce the connection/variables are set, we can see them in our UI:\n\n\n\nConnection\n\n\nAnd use them in your DAG code as such\nadd: code",
    "crumbs": [
      "Scheduler defines when & Orchestrator defines how to, run your data pipelines",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Airflow is both a scheduler and an orchestrator</span>"
    ]
  },
  {
    "objectID": "dbt.html#document-test-tables-with-yml-files",
    "href": "dbt.html#document-test-tables-with-yml-files",
    "title": "12  dbt-core is an orchestrator that makes managing pipelines simpler",
    "section": "12.2 Document & test tables with yml files",
    "text": "12.2 Document & test tables with yml files\nYou can also document what the table and the columns of your tables mean in yml files. These yml files have to be within the same folder and also reference the data model’s name and the column names.\nIn addition to descriptions you can also specify any tests to be run on the columns as needed.\nThe documentation will be rendered when you run the dbt render command and HTML files will be created which we will view with a dbt serve command in a later section.\nThe tests can be run with the dbt test command, note that the tests can only be run after the data is available, so it is not entirely suitable for the WAP pattern.\nadd: core.yml\nRun the tests with dbt test command.",
    "crumbs": [
      "Scheduler defines when & Orchestrator defines how to, run your data pipelines",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>dbt-core is an orchestrator that makes managing pipelines simpler</span>"
    ]
  },
  {
    "objectID": "dbt.html#define-db-connections-in-profiles.yml",
    "href": "dbt.html#define-db-connections-in-profiles.yml",
    "title": "12  dbt-core is an orchestrator that makes managing pipelines simpler",
    "section": "12.3 Define db connections in profiles.yml",
    "text": "12.3 Define db connections in profiles.yml\ndbt uses a yml file to define how it connects to your db engine. Let’s look at our example\nadd: profiles.yml\nWe tell dbt to connect to Apache Spark. The target variable defines the environment. The default is dev, but you can specify which environment to run on with --target flag in the dbt run command.\nBy default dbt will look for a profiles.yml in your HOME directory. We can tell dbt to look for the profiles.yml file in a specific folder using the --profiles-dir flag as shown below.\ndbt run --profiles-dir .",
    "crumbs": [
      "Scheduler defines when & Orchestrator defines how to, run your data pipelines",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>dbt-core is an orchestrator that makes managing pipelines simpler</span>"
    ]
  },
  {
    "objectID": "dbt.html#create-tables-with-select-sql-files",
    "href": "dbt.html#create-tables-with-select-sql-files",
    "title": "12  dbt-core is an orchestrator that makes managing pipelines simpler",
    "section": "",
    "text": "add: silver table code",
    "crumbs": [
      "Scheduler defines when & Orchestrator defines how to, run your data pipelines",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>dbt-core is an orchestrator that makes managing pipelines simpler</span>"
    ]
  },
  {
    "objectID": "capstone_project.html",
    "href": "capstone_project.html",
    "title": "13  Capstone Project: Spark, dbt, Airflow with Docker",
    "section": "",
    "text": "13.1 Presentation matters\nWhen a hiring manager looks at your project, assume that they will not read over the code. Typically when people look at projects they browse high level sections, these includes\nWe will see how you can address these",
    "crumbs": [
      "Scheduler defines when & Orchestrator defines how to, run your data pipelines",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Capstone Project: Spark, dbt, Airflow with Docker</span>"
    ]
  },
  {
    "objectID": "capstone_project.html#presentation-matters",
    "href": "capstone_project.html#presentation-matters",
    "title": "13  Capstone Project: Spark, dbt, Airflow with Docker",
    "section": "",
    "text": "Outcome of your project\nHigh level architecture\nProject structure to understand how your code works\nBrowse code for clarity and code-cleanliness",
    "crumbs": [
      "Scheduler defines when & Orchestrator defines how to, run your data pipelines",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Capstone Project: Spark, dbt, Airflow with Docker</span>"
    ]
  },
  {
    "objectID": "capstone_project.html#start-with-the-outcome",
    "href": "capstone_project.html#start-with-the-outcome",
    "title": "13  Capstone Project: Spark, dbt, Airflow with Docker",
    "section": "13.2 Start with the outcome",
    "text": "13.2 Start with the outcome\nWe are creating data for the sales team to enable customer outreach and for this we need to present customers who will most likely convert. While this is a complex data science question, a simple approach could be to target customers who have the highest average order value (assuming high/low order values are outliers).\nCreate a dashboard to show the top 10 customers by average order values as a descending bar chart.\nadd: gif\nadd: image of bar chart",
    "crumbs": [
      "Scheduler defines when & Orchestrator defines how to, run your data pipelines",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Capstone Project: Spark, dbt, Airflow with Docker</span>"
    ]
  },
  {
    "objectID": "capstone_project.html#high-level-architecture",
    "href": "capstone_project.html#high-level-architecture",
    "title": "13  Capstone Project: Spark, dbt, Airflow with Docker",
    "section": "13.3 High level architecture",
    "text": "13.3 High level architecture\nThe objective of this is to show your expertise in\n\nDesigning data pipelines, by following industry standard 3-hop architecture\nIndustry standard tools like dbt, Airflow and Spark\nWriting clean code using auto formatters and linters\n\nOur base repo comes with all of these setup and installed for you to copy over and use.\nadd: 3-hop dbt folder structure add: auto formatter and linter for python and sql",
    "crumbs": [
      "Scheduler defines when & Orchestrator defines how to, run your data pipelines",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Capstone Project: Spark, dbt, Airflow with Docker</span>"
    ]
  },
  {
    "objectID": "capstone_project.html#code-explanation",
    "href": "capstone_project.html#code-explanation",
    "title": "13  Capstone Project: Spark, dbt, Airflow with Docker",
    "section": "13.4 Code explanation",
    "text": "13.4 Code explanation\nWe have code that does the following\n\nUse dbt to define Spark SQL pipelines following the 3-hop architecture that we saw in the dbt chapter. Add: folder structure screenshot\nUse Airflow to load data into the warehouse and run dbt pipeline on a schedule and be able to monitor it add; dag screenshot\nFollow Kimball dimensional modeling for our dbt pipeline, add: 3-hop arch",
    "crumbs": [
      "Scheduler defines when & Orchestrator defines how to, run your data pipelines",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Capstone Project: Spark, dbt, Airflow with Docker</span>"
    ]
  },
  {
    "objectID": "sql_basics.html#exercises",
    "href": "sql_basics.html#exercises",
    "title": "2  Read data, Combine tables, & aggregate numbers to understand business performance",
    "section": "2.11 Exercises",
    "text": "2.11 Exercises\n\nCreate a report that shows the number of returns for each region name\nTop 10 most selling parts\nSellers who sell atleast one of the top 10 selling parts\nNumber of returns per order price bucket\n\nAssume the price bucket logic is\n CASE\n        WHEN totalprice &gt; 100000 THEN 'high'\n        WHEN totalprice BETWEEN 25000\n        AND 100000 THEN 'medium'\n        ELSE 'low'\n    END AS order_price_bucket\n\nAverage time (in days) between receiptdate and shipdate for each nation",
    "crumbs": [
      "Use SQL to transform data",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Read data, Combine tables, & aggregate numbers to understand business performance</span>"
    ]
  },
  {
    "objectID": "sql_basics.html#recommended-reading",
    "href": "sql_basics.html#recommended-reading",
    "title": "2  Read data, Combine tables, & aggregate numbers to understand business performance",
    "section": "2.12 Recommended reading",
    "text": "2.12 Recommended reading\n\nhttps://www.startdataengineering.com/post/improve-sql-skills-de/\nhttps://www.startdataengineering.com/post/n-sql-tips-de/\nhttps://www.startdataengineering.com/post/advanced-sql/",
    "crumbs": [
      "Use SQL to transform data",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Read data, Combine tables, & aggregate numbers to understand business performance</span>"
    ]
  },
  {
    "objectID": "cte.html#exercises",
    "href": "cte.html#exercises",
    "title": "2  CTE (Common Table Expression) improves code readability and reduces repetition",
    "section": "2.4 Exercises",
    "text": "2.4 Exercises\n\nSellers who sell atleast one of the top 10 selling parts.",
    "crumbs": [
      "Use SQL to transform data",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>CTE (Common Table Expression) improves code readability and reduces repetition</span>"
    ]
  },
  {
    "objectID": "cte.html#recommended-reading",
    "href": "cte.html#recommended-reading",
    "title": "2  CTE (Common Table Expression) improves code readability and reduces repetition",
    "section": "2.5 Recommended reading",
    "text": "2.5 Recommended reading\n\nhttps://www.startdataengineering.com/post/using-common-table-expression-in-redshift/",
    "crumbs": [
      "Use SQL to transform data",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>CTE (Common Table Expression) improves code readability and reduces repetition</span>"
    ]
  },
  {
    "objectID": "windows.html#exercises",
    "href": "windows.html#exercises",
    "title": "3  Use window function when you need to use values from other rows to compute a value for the current row",
    "section": "3.5 Exercises",
    "text": "3.5 Exercises\n\nWrite a query on the orders table that has the following output:\n\no_custkey\norder_month: In YYYY-MM format, use strftime(o_orderdate, ‘%Y-%m’) AS order_month\ntotal_price: Sum of o_totalprice for that month\nthree_mo_total_price_avg: The 3 month (previous, current & next) average of total_price for that customer\n\n\nSELECT\n  order_month,\n  o_custkey,\n  total_price,\n  ROUND(\n    AVG(total_price) OVER ( -- FUNCTION: RUNNING AVERAGE\n      PARTITION BY\n        o_custkey -- PARTITIONED BY customer\n      ORDER BY\n        order_month ROWS BETWEEN 1 PRECEDING\n        AND 1 FOLLOWING -- WINDOW FRAME DEFINED AS 1 ROW PRECEDING to 1 ROW FOLLOWING\n    ),\n    2\n  ) AS three_mo_total_price_avg\nFROM\n  (\n    SELECT\n      strftime (o_orderdate, '%Y-%m') AS order_month,\n      o_custkey,\n      SUM(o_totalprice) AS total_price\n    FROM\n      orders\n    GROUP BY\n      1,\n      2\n  )\nLIMIT\n  5;\nNow that we have seen how to create a window frame with ROWS, let’ explore how to do this with RANGE.\n\nWrite a query on the orders table that has the following output:\n\norder_month,\no_custkey,\ntotal_price,\nthree_mo_total_price_avg\nconsecutive_three_mo_total_price_avg: The consecutive 3 month average of total_price for that customer. Note that this should only include months that are chronologically next to each other.\n\n\nTime limit during live workshop: 10 min\nHint: Use CAST(strftime(o_orderdate, '%Y-%m-01') AS DATE) to cast order_month to date format.\nHint: Use the INTERVAL format shown above to construct the window function to compute consecutive_three_mo_total_price_avg column.\n\nThe orders table schema is shown below:\n\n\n\n\nOrders table\n\n\n-- write your query here\n\nFrom the orders table get the 3 lowest spending customers per day\n\nTime limit during live workshop: 5 min\nHint 1. Figure out the PARTITION BY column first, then the ORDER BY column and finally the FUNCTION to use to compute running average.\nThe orders table schema is shown below:\n\n\n\nOrders table\n\n\n-- your code here\n\nWrite a SQL query using the orders table that calculates the following columns:\n\no_orderdate: From orders table\no_custkey: From orders table\no_totalprice: From orders table\ntotalprice_diff: The customers current day’s o_totalprice - that same customers most recent previous purchase’s o_totalprice\n\n\n\nTime limit during live workshop: 5 min\nHint:\n\nStart by figuring out what the PARTITION BY column should be, then what the ORDER BY column should be, and then finally the function to use.\nUse the LAG(column_name) ranking function to identify the prior day’s revenue.\n\nThe orders table schema is shown below:\n\n\n\n\nOrders table\n\n\n-- write your query here",
    "crumbs": [
      "Use SQL to transform data",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Use window function when you need to use values from other rows to compute a value for the current row</span>"
    ]
  },
  {
    "objectID": "windows.html#recommended-reading",
    "href": "windows.html#recommended-reading",
    "title": "3  Use window function when you need to use values from other rows to compute a value for the current row",
    "section": "3.6 Recommended reading",
    "text": "3.6 Recommended reading\n\nWindow SQL Youtube workshop",
    "crumbs": [
      "Use SQL to transform data",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Use window function when you need to use values from other rows to compute a value for the current row</span>"
    ]
  },
  {
    "objectID": "py_basics.html#exercises",
    "href": "py_basics.html#exercises",
    "title": "4  Manipulate data with standard libraries and co-locate code with classes and functions",
    "section": "4.1 Exercises",
    "text": "4.1 Exercises\n\nAssume you have to store customer data and quickly retrieve individual customer data using customer id, how would you store this data\n\n# customer data\n\nAssume you have a list of dictionary as customer data, how would you filter it and only output customers who have &gt;=2 orders\nCreate a customer class, with number of items as a variable. Create a method called add_orders that accepts an interger number of orders to be added to that customer and get_orders to print the number of orders for that customer.\n\n# template",
    "crumbs": [
      "Python connects the different part of your data pipeline",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Manipulate data with standard libraries and co-locate code with classes and functions</span>"
    ]
  },
  {
    "objectID": "py_basics.html#recommended-reading",
    "href": "py_basics.html#recommended-reading",
    "title": "4  Manipulate data with standard libraries and co-locate code with classes and functions",
    "section": "4.2 Recommended reading",
    "text": "4.2 Recommended reading\n\nhttps://www.startdataengineering.com/post/how-to-validate-datatypes-in-python/\nhttps://www.startdataengineering.com/post/writing-memory-efficient-dps-in-python/\nhttps://www.startdataengineering.com/post/code-patterns/\nhttps://www.startdataengineering.com/post/python-for-de/\nhttps://www.startdataengineering.com/post/cost-effective-pipelines/\nhttps://www.startdataengineering.com/post/sql-v-python/",
    "crumbs": [
      "Python connects the different part of your data pipeline",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Manipulate data with standard libraries and co-locate code with classes and functions</span>"
    ]
  },
  {
    "objectID": "py_transform.html#recommended-reading",
    "href": "py_transform.html#recommended-reading",
    "title": "6  Python has libraries to tell the data processing engine (Spark, Trino, Duckdb, Polars, etc) what to do",
    "section": "6.2 Recommended reading",
    "text": "6.2 Recommended reading\n\nhttps://www.startdataengineering.com/post/cost-effective-pipelines/\nhttps://www.startdataengineering.com/post/sql-v-python/\nhttps://www.startdataengineering.com/post/test-pyspark/",
    "crumbs": [
      "Python connects the different part of your data pipeline",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Python has libraries to tell the data processing engine (Spark, Trino, Duckdb, Polars, etc) what to do</span>"
    ]
  },
  {
    "objectID": "py_el.html#exercises",
    "href": "py_el.html#exercises",
    "title": "5  Python has libraries to read and write data to (almost) any system",
    "section": "",
    "text": "Read data from pokeapi and write data into a local file\n\ndata_api = \"https://pokeapi.co/api/v2/pokemon/1/\"\nlocal_file = \"tmp_file\"\n\n# pull data from data_api \n# write data to local_file\n\nRead data from local file and print it\n\n# use python standard libraries to open the local_file\n# print contents of the local_file\n\nRead data from local file and insert it into a sqllite3 table\n\n# create a sqlite3 table\n# use python standard libraries to open the local_file\n# print contents of the local_file\n# inbsert id, name and base_experience into the above sqlite3 table",
    "crumbs": [
      "Python connects the different part of your data pipeline",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Python has libraries to read and write data to (almost) any system</span>"
    ]
  },
  {
    "objectID": "py_el.html#recommended-reading",
    "href": "py_el.html#recommended-reading",
    "title": "5  Python has libraries to read and write data to (almost) any system",
    "section": "5.2 Recommended reading",
    "text": "5.2 Recommended reading",
    "crumbs": [
      "Python connects the different part of your data pipeline",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Python has libraries to read and write data to (almost) any system</span>"
    ]
  },
  {
    "objectID": "py_transform.html#exercises",
    "href": "py_transform.html#exercises",
    "title": "6  Python has libraries to tell the data processing engine (Spark, Trino, Duckdb, Polars, etc) what to do",
    "section": "",
    "text": "Find the top spending customer using Pyspark dataframe API and Python standard library",
    "crumbs": [
      "Python connects the different part of your data pipeline",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Python has libraries to tell the data processing engine (Spark, Trino, Duckdb, Polars, etc) what to do</span>"
    ]
  },
  {
    "objectID": "dw_table_types.html#exercises",
    "href": "dw_table_types.html#exercises",
    "title": "8  Data warehouse modeling (Kimball) is based off of 2 types of tables: Fact and dimensions",
    "section": "8.4 Exercises",
    "text": "8.4 Exercises\n\nWhat are the fact tables in our TPCH data model?\nWhat source tables in TPCH data model would you consider to create a customer dimension table?",
    "crumbs": [
      "Data modeling is the process of getting data ready for analytics",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Data warehouse modeling (Kimball) is based off of 2 types of tables: Fact and dimensions</span>"
    ]
  },
  {
    "objectID": "dw_table_types.html#recommended-reading",
    "href": "dw_table_types.html#recommended-reading",
    "title": "8  Data warehouse modeling (Kimball) is based off of 2 types of tables: Fact and dimensions",
    "section": "8.5 Recommended reading",
    "text": "8.5 Recommended reading\n\nhttps://www.startdataengineering.com/post/metrics_sot/\nhttps://www.startdataengineering.com/post/n-steps-avoid-messy-dw/\nhttps://www.startdataengineering.com/post/data-lake-warehouse-diff/\nhttps://www.startdataengineering.com/post/what-is-a-data-warehouse/",
    "crumbs": [
      "Data modeling is the process of getting data ready for analytics",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Data warehouse modeling (Kimball) is based off of 2 types of tables: Fact and dimensions</span>"
    ]
  },
  {
    "objectID": "capstone_project.html#exercise",
    "href": "capstone_project.html#exercise",
    "title": "13  Capstone Project: Spark, dbt, Airflow with Docker",
    "section": "13.5 Exercise",
    "text": "13.5 Exercise\n\nFind a dataset that you are interested in, showing an interesting take for the data. Outcome should be shown with data.\n\nYou can copy paste the airflow folder into a separate folder and make the necessary changes. You can add requirements to the requirements.txt file\nadd: code wit unix commands",
    "crumbs": [
      "Scheduler defines when & Orchestrator defines how to, run your data pipelines",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Capstone Project: Spark, dbt, Airflow with Docker</span>"
    ]
  },
  {
    "objectID": "capstone_project.html#read-this",
    "href": "capstone_project.html#read-this",
    "title": "13  Capstone Project: Spark, dbt, Airflow with Docker",
    "section": "13.6 Read this",
    "text": "13.6 Read this\n\nApproach on identifying problem space and datasets https://www.startdataengineering.com/post/what-data-project-to-build/#23-data\nhttps://www.startdataengineering.com/post/data-engineering-project-to-impress-hiring-managers/",
    "crumbs": [
      "Scheduler defines when & Orchestrator defines how to, run your data pipelines",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Capstone Project: Spark, dbt, Airflow with Docker</span>"
    ]
  },
  {
    "objectID": "SQL.html",
    "href": "SQL.html",
    "title": "1  SQL",
    "section": "",
    "text": "2 Read data, Combine tables, & aggregate numbers to understand business performance\nIn this chapter, we will go over SQL basics. As you follow along with the jupyter notebook, make sure to add the %%sql in the cell block to let jupyter notebook run this as spark sql.",
    "crumbs": [
      "Use SQL to transform data",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>SQL</span>"
    ]
  },
  {
    "objectID": "SQL.html#a-spark-catalog-can-have-multiple-schemas-schemas-can-have-multiple-tables",
    "href": "SQL.html#a-spark-catalog-can-have-multiple-schemas-schemas-can-have-multiple-tables",
    "title": "1  SQL",
    "section": "2.1 A Spark catalog can have multiple schemas, & schemas can have multiple tables",
    "text": "2.1 A Spark catalog can have multiple schemas, & schemas can have multiple tables\nTypically database servers can have multiple databases; each database can have multiple schemas. Each schema can have multiple tables, and each table can have multiple columns.\nNote: We use Trino, which has catalogs that allow it to connect with the different underlying systems. (e.g., Postgres, Redis, Hive, etc.)\nIn our lab, we use Trino, and we can check the available catalogs, their schemas, the tables in a schema, & the columns in a table, as shown below.",
    "crumbs": [
      "Use SQL to transform data",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>SQL</span>"
    ]
  }
]